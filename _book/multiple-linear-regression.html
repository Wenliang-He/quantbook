<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Quantitative Research Methods for Education</title>
  <meta name="description" content="This is the first trial version!">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="Quantitative Research Methods for Education" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is the first trial version!" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Quantitative Research Methods for Education" />
  
  <meta name="twitter:description" content="This is the first trial version!" />
  

<meta name="author" content="Wenliang He">


<meta name="date" content="2018-08-29">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="linear-regression.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="libs/htmlwidgets-1.2/htmlwidgets.js"></script>
<script src="libs/d3-3.3.8/d3.min.js"></script>
<script src="libs/dagre-0.4.0/dagre-d3.min.js"></script>
<link href="libs/mermaid-0.3.0/dist/mermaid.css" rel="stylesheet" />
<script src="libs/mermaid-0.3.0/dist/mermaid.slim.min.js"></script>
<link href="libs/DiagrammeR-styles-0.2/styles.css" rel="stylesheet" />
<script src="libs/chromatography-0.1/chromatography.js"></script>
<script src="libs/DiagrammeR-binding-1.0.0/DiagrammeR.js"></script>
<link href="libs/datatables-css-0.0.0/datatables-crosstalk.css" rel="stylesheet" />
<script src="libs/datatables-binding-0.4/datatables.js"></script>
<link href="libs/dt-core-1.10.16/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="libs/dt-core-1.10.16/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="libs/dt-core-1.10.16/js/jquery.dataTables.min.js"></script>
<link href="libs/crosstalk-1.0.0/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk-1.0.0/js/crosstalk.min.js"></script>


<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#intro-to-rmarkdown"><i class="fa fa-check"></i><b>1.1</b> Intro to Rmarkdown</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#intro-to-bookdown"><i class="fa fa-check"></i><b>1.2</b> Intro to Bookdown</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#resources-to-rmarkdown"><i class="fa fa-check"></i><b>1.3</b> Resources to Rmarkdown</a></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#resources-to-bookdown"><i class="fa fa-check"></i><b>1.4</b> Resources to Bookdown</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="the-fundamentals-of-statistics.html"><a href="the-fundamentals-of-statistics.html"><i class="fa fa-check"></i><b>2</b> The Fundamentals of Statistics</a><ul>
<li class="chapter" data-level="2.1" data-path="the-fundamentals-of-statistics.html"><a href="the-fundamentals-of-statistics.html#why-studying-statistics"><i class="fa fa-check"></i><b>2.1</b> Why Studying Statistics</a><ul>
<li class="chapter" data-level="2.1.1" data-path="the-fundamentals-of-statistics.html"><a href="the-fundamentals-of-statistics.html#understanding-the-world"><i class="fa fa-check"></i><b>2.1.1</b> Understanding the World</a></li>
<li class="chapter" data-level="2.1.2" data-path="the-fundamentals-of-statistics.html"><a href="the-fundamentals-of-statistics.html#science-vs.philosophy"><i class="fa fa-check"></i><b>2.1.2</b> Science vs. Philosophy</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="the-fundamentals-of-statistics.html"><a href="the-fundamentals-of-statistics.html#the-big-picture"><i class="fa fa-check"></i><b>2.2</b> The Big Picture</a><ul>
<li class="chapter" data-level="2.2.1" data-path="the-fundamentals-of-statistics.html"><a href="the-fundamentals-of-statistics.html#central-themes-in-statistics"><i class="fa fa-check"></i><b>2.2.1</b> Central Themes in Statistics</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="the-fundamentals-of-statistics.html"><a href="the-fundamentals-of-statistics.html#the-fundamentals-of-statistics-1"><i class="fa fa-check"></i><b>2.3</b> The Fundamentals of Statistics</a><ul>
<li class="chapter" data-level="2.3.1" data-path="the-fundamentals-of-statistics.html"><a href="the-fundamentals-of-statistics.html#storing-data-in-tables"><i class="fa fa-check"></i><b>2.3.1</b> Storing Data in Tables</a></li>
<li class="chapter" data-level="2.3.2" data-path="the-fundamentals-of-statistics.html"><a href="the-fundamentals-of-statistics.html#variable-types"><i class="fa fa-check"></i><b>2.3.2</b> Variable Types</a></li>
<li class="chapter" data-level="2.3.3" data-path="the-fundamentals-of-statistics.html"><a href="the-fundamentals-of-statistics.html#exploratory-data-analysis-checking-distributions"><i class="fa fa-check"></i><b>2.3.3</b> Exploratory Data Analysis: Checking Distributions</a></li>
<li class="chapter" data-level="2.3.4" data-path="the-fundamentals-of-statistics.html"><a href="the-fundamentals-of-statistics.html#normal-distributions"><i class="fa fa-check"></i><b>2.3.4</b> Normal Distributions</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="statistical-inference.html"><a href="statistical-inference.html"><i class="fa fa-check"></i><b>3</b> Statistical Inference</a><ul>
<li class="chapter" data-level="3.1" data-path="statistical-inference.html"><a href="statistical-inference.html#random-variable"><i class="fa fa-check"></i><b>3.1</b> Random Variable</a><ul>
<li class="chapter" data-level="3.1.1" data-path="statistical-inference.html"><a href="statistical-inference.html#iid---independently-and-identically-distributed"><i class="fa fa-check"></i><b>3.1.1</b> IID - Independently and Identically Distributed</a></li>
<li class="chapter" data-level="3.1.2" data-path="statistical-inference.html"><a href="statistical-inference.html#random-variable-vs.case"><i class="fa fa-check"></i><b>3.1.2</b> Random Variable vs. Case</a></li>
<li class="chapter" data-level="3.1.3" data-path="statistical-inference.html"><a href="statistical-inference.html#properties-of-a-random-variable"><i class="fa fa-check"></i><b>3.1.3</b> Properties of a Random Variable</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="statistical-inference.html"><a href="statistical-inference.html#central-limit-theorem"><i class="fa fa-check"></i><b>3.2</b> Central Limit Theorem</a><ul>
<li class="chapter" data-level="3.2.1" data-path="statistical-inference.html"><a href="statistical-inference.html#presenting-central-limit-theorem"><i class="fa fa-check"></i><b>3.2.1</b> Presenting Central Limit Theorem</a></li>
<li class="chapter" data-level="3.2.2" data-path="statistical-inference.html"><a href="statistical-inference.html#understanding-central-limit-theorem"><i class="fa fa-check"></i><b>3.2.2</b> Understanding Central Limit Theorem</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="statistical-inference.html"><a href="statistical-inference.html#statistical-inference-1"><i class="fa fa-check"></i><b>3.3</b> Statistical Inference</a><ul>
<li class="chapter" data-level="3.3.1" data-path="statistical-inference.html"><a href="statistical-inference.html#the-diamond-expert"><i class="fa fa-check"></i><b>3.3.1</b> The Diamond Expert</a></li>
<li class="chapter" data-level="3.3.2" data-path="statistical-inference.html"><a href="statistical-inference.html#hypothesis-testing"><i class="fa fa-check"></i><b>3.3.2</b> Hypothesis Testing</a></li>
<li class="chapter" data-level="3.3.3" data-path="statistical-inference.html"><a href="statistical-inference.html#hypothesis-testing-with-unknown-variance"><i class="fa fa-check"></i><b>3.3.3</b> Hypothesis Testing with Unknown Variance</a></li>
<li class="chapter" data-level="3.3.4" data-path="statistical-inference.html"><a href="statistical-inference.html#examples-of-hypothesis-testing"><i class="fa fa-check"></i><b>3.3.4</b> Examples of Hypothesis Testing</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="common-statistical-tests.html"><a href="common-statistical-tests.html"><i class="fa fa-check"></i><b>4</b> Common Statistical Tests</a><ul>
<li class="chapter" data-level="4.1" data-path="common-statistical-tests.html"><a href="common-statistical-tests.html#dummy-x---continuous-y"><i class="fa fa-check"></i><b>4.1</b> Dummy (X) - Continuous (Y)</a><ul>
<li class="chapter" data-level="4.1.1" data-path="common-statistical-tests.html"><a href="common-statistical-tests.html#two-sample-t-test"><i class="fa fa-check"></i><b>4.1.1</b> Two-sample T-Test</a></li>
<li class="chapter" data-level="4.1.2" data-path="common-statistical-tests.html"><a href="common-statistical-tests.html#statistical-power"><i class="fa fa-check"></i><b>4.1.2</b> Statistical Power</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="common-statistical-tests.html"><a href="common-statistical-tests.html#dummy-x---dummy-y"><i class="fa fa-check"></i><b>4.2</b> Dummy (X) - Dummy (Y)</a><ul>
<li class="chapter" data-level="4.2.1" data-path="common-statistical-tests.html"><a href="common-statistical-tests.html#basics"><i class="fa fa-check"></i><b>4.2.1</b> Basics</a></li>
<li class="chapter" data-level="4.2.2" data-path="common-statistical-tests.html"><a href="common-statistical-tests.html#difference-in-proportions"><i class="fa fa-check"></i><b>4.2.2</b> Difference in Proportions</a></li>
<li class="chapter" data-level="4.2.3" data-path="common-statistical-tests.html"><a href="common-statistical-tests.html#relative-risks"><i class="fa fa-check"></i><b>4.2.3</b> Relative Risks</a></li>
<li class="chapter" data-level="4.2.4" data-path="common-statistical-tests.html"><a href="common-statistical-tests.html#odds-ratio"><i class="fa fa-check"></i><b>4.2.4</b> Odds Ratio</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="common-statistical-tests.html"><a href="common-statistical-tests.html#categorical-x---dummy-y"><i class="fa fa-check"></i><b>4.3</b> Categorical (X) - Dummy (Y)</a><ul>
<li class="chapter" data-level="4.3.1" data-path="common-statistical-tests.html"><a href="common-statistical-tests.html#basics-1"><i class="fa fa-check"></i><b>4.3.1</b> Basics</a></li>
<li class="chapter" data-level="4.3.2" data-path="common-statistical-tests.html"><a href="common-statistical-tests.html#chi-squared-test"><i class="fa fa-check"></i><b>4.3.2</b> Chi-squared Test</a></li>
<li class="chapter" data-level="4.3.3" data-path="common-statistical-tests.html"><a href="common-statistical-tests.html#g-square-test"><i class="fa fa-check"></i><b>4.3.3</b> G-square Test</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="common-statistical-tests.html"><a href="common-statistical-tests.html#categorical-x---continuous-y"><i class="fa fa-check"></i><b>4.4</b> Categorical (X) - Continuous (Y)</a><ul>
<li class="chapter" data-level="4.4.1" data-path="common-statistical-tests.html"><a href="common-statistical-tests.html#anova"><i class="fa fa-check"></i><b>4.4.1</b> ANOVA</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="linear-regression.html"><a href="linear-regression.html"><i class="fa fa-check"></i><b>5</b> Linear Regression</a><ul>
<li class="chapter" data-level="5.1" data-path="linear-regression.html"><a href="linear-regression.html#table-of-contents"><i class="fa fa-check"></i><b>5.1</b> Table of Contents</a></li>
<li class="chapter" data-level="5.2" data-path="linear-regression.html"><a href="linear-regression.html#pearson-product-moment-correlation-coefficient"><i class="fa fa-check"></i><b>5.2</b> Pearson Product-Moment Correlation Coefficient</a><ul>
<li class="chapter" data-level="5.2.1" data-path="linear-regression.html"><a href="linear-regression.html#definition"><i class="fa fa-check"></i><b>5.2.1</b> Definition</a></li>
<li class="chapter" data-level="5.2.2" data-path="linear-regression.html"><a href="linear-regression.html#properties-of-pearson-correlation"><i class="fa fa-check"></i><b>5.2.2</b> Properties of Pearson Correlation</a></li>
<li class="chapter" data-level="5.2.3" data-path="linear-regression.html"><a href="linear-regression.html#limitations-of-pearson-correlation"><i class="fa fa-check"></i><b>5.2.3</b> Limitations of Pearson Correlation</a></li>
<li class="chapter" data-level="5.2.4" data-path="linear-regression.html"><a href="linear-regression.html#other-issues-on-the-usage-of-pearson-correlation"><i class="fa fa-check"></i><b>5.2.4</b> Other Issues on the Usage of Pearson Correlation</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="linear-regression.html"><a href="linear-regression.html#simple-linear-regression"><i class="fa fa-check"></i><b>5.3</b> Simple Linear Regression</a><ul>
<li class="chapter" data-level="5.3.1" data-path="linear-regression.html"><a href="linear-regression.html#independent-variable-from-categorical-to-continuous"><i class="fa fa-check"></i><b>5.3.1</b> Independent Variable: From Categorical to Continuous</a></li>
<li class="chapter" data-level="5.3.2" data-path="linear-regression.html"><a href="linear-regression.html#fitting-a-straight-line-to-data"><i class="fa fa-check"></i><b>5.3.2</b> Fitting a Straight Line to Data</a></li>
<li class="chapter" data-level="5.3.3" data-path="linear-regression.html"><a href="linear-regression.html#the-least-squares-method"><i class="fa fa-check"></i><b>5.3.3</b> The Least-Squares Method</a></li>
<li class="chapter" data-level="5.3.4" data-path="linear-regression.html"><a href="linear-regression.html#interpretations-of-regression-coefficients"><i class="fa fa-check"></i><b>5.3.4</b> Interpretations of Regression Coefficients</a></li>
<li class="chapter" data-level="5.3.5" data-path="linear-regression.html"><a href="linear-regression.html#understanding-the-estimators"><i class="fa fa-check"></i><b>5.3.5</b> Understanding the Estimators</a></li>
<li class="chapter" data-level="5.3.6" data-path="linear-regression.html"><a href="linear-regression.html#residual-variance"><i class="fa fa-check"></i><b>5.3.6</b> Residual Variance</a></li>
<li class="chapter" data-level="5.3.7" data-path="linear-regression.html"><a href="linear-regression.html#standard-error-of-the-slope"><i class="fa fa-check"></i><b>5.3.7</b> Standard Error of the Slope</a></li>
<li class="chapter" data-level="5.3.8" data-path="linear-regression.html"><a href="linear-regression.html#effect-size-and-standardized-coefficients"><i class="fa fa-check"></i><b>5.3.8</b> Effect Size and Standardized Coefficients</a></li>
<li class="chapter" data-level="5.3.9" data-path="linear-regression.html"><a href="linear-regression.html#anova-for-regression"><i class="fa fa-check"></i><b>5.3.9</b> ANOVA for Regression</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="linear-regression.html"><a href="linear-regression.html#non-linear-relations"><i class="fa fa-check"></i><b>5.4</b> Non-linear Relations</a><ul>
<li class="chapter" data-level="5.4.1" data-path="linear-regression.html"><a href="linear-regression.html#log-transformations"><i class="fa fa-check"></i><b>5.4.1</b> Log-transformations</a></li>
<li class="chapter" data-level="5.4.2" data-path="linear-regression.html"><a href="linear-regression.html#quadratic-relations"><i class="fa fa-check"></i><b>5.4.2</b> Quadratic Relations</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="linear-regression.html"><a href="linear-regression.html#the-assumptions-of-simple-linear-regression"><i class="fa fa-check"></i><b>5.5</b> The Assumptions of Simple Linear Regression</a></li>
<li class="chapter" data-level="5.6" data-path="linear-regression.html"><a href="linear-regression.html#assumption-diagnostics"><i class="fa fa-check"></i><b>5.6</b> Assumption Diagnostics</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html"><i class="fa fa-check"></i><b>6</b> Multiple Linear Regression</a><ul>
<li class="chapter" data-level="6.1" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#regression-models---the-base-form"><i class="fa fa-check"></i><b>6.1</b> Regression Models - The Base Form</a><ul>
<li class="chapter" data-level="6.1.1" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#model-specification-with-population-parameters"><i class="fa fa-check"></i><b>6.1.1</b> Model Specification with Population Parameters</a></li>
<li class="chapter" data-level="6.1.2" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#model-specification-with-sample-parameters"><i class="fa fa-check"></i><b>6.1.2</b> Model Specification with Sample Parameters</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#the-ordinary-least-squres-method"><i class="fa fa-check"></i><b>6.2</b> The Ordinary Least-Squres Method</a><ul>
<li class="chapter" data-level="6.2.1" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#interpretations-of-regression-coefficients-1"><i class="fa fa-check"></i><b>6.2.1</b> Interpretations of Regression Coefficients</a></li>
<li class="chapter" data-level="6.2.2" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#residual-variance-1"><i class="fa fa-check"></i><b>6.2.2</b> Residual Variance</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#standard-errors-of-regression-coefficients"><i class="fa fa-check"></i><b>6.3</b> Standard Errors of Regression Coefficients</a><ul>
<li class="chapter" data-level="6.3.1" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#variance-inflation-factor"><i class="fa fa-check"></i><b>6.3.1</b> Variance Inflation Factor</a></li>
<li class="chapter" data-level="6.3.2" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#significance-of-vif"><i class="fa fa-check"></i><b>6.3.2</b> Significance of VIF</a></li>
<li class="chapter" data-level="6.3.3" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#multicollinearity"><i class="fa fa-check"></i><b>6.3.3</b> Multicollinearity</a></li>
<li class="chapter" data-level="6.3.4" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#influencing-factors-on-standard-errors"><i class="fa fa-check"></i><b>6.3.4</b> Influencing Factors on Standard Errors</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#r2-and-anova-for-multiple-regression"><i class="fa fa-check"></i><b>6.4</b> <span class="math inline">\(R^2\)</span> and ANOVA for Multiple Regression</a></li>
<li class="chapter" data-level="6.5" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#regression-models-extended---categorical-features"><i class="fa fa-check"></i><b>6.5</b> Regression Models Extended - Categorical Features</a><ul>
<li class="chapter" data-level="6.5.1" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#dummy-variables"><i class="fa fa-check"></i><b>6.5.1</b> Dummy Variables</a></li>
<li class="chapter" data-level="6.5.2" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#nominal-variables"><i class="fa fa-check"></i><b>6.5.2</b> Nominal Variables</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#regression-models-extended---interaction-effects"><i class="fa fa-check"></i><b>6.6</b> Regression Models Extended - Interaction Effects</a><ul>
<li class="chapter" data-level="6.6.1" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#dummy-dummy-interactions"><i class="fa fa-check"></i><b>6.6.1</b> Dummy-Dummy Interactions</a></li>
<li class="chapter" data-level="6.6.2" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#dummy-continuous-interactions"><i class="fa fa-check"></i><b>6.6.2</b> Dummy-Continuous Interactions</a></li>
<li class="chapter" data-level="6.6.3" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#continuous-continuous-interactions"><i class="fa fa-check"></i><b>6.6.3</b> Continuous-Continuous Interactions</a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#regression-models-extended---nonlinear-relations"><i class="fa fa-check"></i><b>6.7</b> Regression Models Extended - Nonlinear Relations</a></li>
<li class="chapter" data-level="6.8" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#model-comparision-evaluation"><i class="fa fa-check"></i><b>6.8</b> Model Comparision &amp; Evaluation</a><ul>
<li class="chapter" data-level="6.8.1" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#f-test-for-nested-models"><i class="fa fa-check"></i><b>6.8.1</b> F-test for Nested Models</a></li>
<li class="chapter" data-level="6.8.2" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#adjusted-r-squared"><i class="fa fa-check"></i><b>6.8.2</b> Adjusted R-Squared</a></li>
<li class="chapter" data-level="6.8.3" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#information-criteria"><i class="fa fa-check"></i><b>6.8.3</b> Information Criteria</a></li>
</ul></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Quantitative Research Methods for Education</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="multiple-linear-regression" class="section level1">
<h1><span class="header-section-number">6</span> Multiple Linear Regression</h1>
<p>Multiple linear regression, or simply <strong>multiple regression</strong>, is a powerful statistical modeling technique that can handle</p>
<ul>
<li>independent variables of assorted data types, i.e., ditchotomous, nominal, continuous<br />
</li>
<li>complex relations among the IVs, e.g., nonlinear relations and interaction effects</li>
</ul>
<div id="regression-models---the-base-form" class="section level2">
<h2><span class="header-section-number">6.1</span> Regression Models - The Base Form</h2>
<div id="model-specification-with-population-parameters" class="section level3">
<h3><span class="header-section-number">6.1.1</span> Model Specification with Population Parameters</h3>
<p>In a <em>simple linear regression model</em>, we use only one independent variable X to predict a continuous dependent variable Y and the <em>prediction model</em> has two parameters, i.e. the intercept and the slope. In this chapter, instead of usign one IV, we will use <em>p</em> IVs, i.e. <span class="math inline">\(X_1, X_2, \dots, X_p\)</span>. As a result, with the <em>population parameters</em> in place, the <strong>multiple linear regression model</strong> is</p>
<p><span class="math display">\[ Y_i = \mu_i + \varepsilon_i = \beta_0 + \beta_1X_{1i} + \beta_2X_{2i} + \cdots + \beta_pX_{pi} + \varepsilon_i  \]</span> where <span class="math inline">\(i = 1,2,\dots,n\)</span> and <span class="math inline">\(\mu_i \equiv E(Y_i|\boldsymbol{X}_i)\)</span> is commonly referred to as the <strong>mean response model</strong> or the <strong>prediction model</strong></p>
<p><span class="math display">\[ E(Y_i|\boldsymbol{X}_i) \equiv \mu_i = \beta_0 + \beta_1X_{1i} + \beta_2X_{2i} + \cdots + \beta_pX_{pi} \]</span> and <span class="math inline">\(\varepsilon_i\)</span> is called the <strong>error</strong> and</p>
<p><span class="math display">\[\varepsilon_i \overset{iid}{\text{~}} N(0,\sigma^2)\]</span></p>
<p>To note, the population parameters of this linear model are <span class="math inline">\(\beta_0, \beta_1, \dots, \beta_p\)</span> and <span class="math inline">\(\sigma\)</span>.</p>
</div>
<div id="model-specification-with-sample-parameters" class="section level3">
<h3><span class="header-section-number">6.1.2</span> Model Specification with Sample Parameters</h3>
<p>Accordingly, using the <em>sample parameters</em>, the <strong>multiple linear regression model</strong> is</p>
<p><span class="math display">\[ Y_i = \hat{Y}_i + e_i = b_0 + b_1X_{1i} + b_2X_{2i} + \cdots + b_pX_{pi} + e_i  \]</span> where <span class="math inline">\(i = 1,2,\dots,n\)</span> and the <strong>prediction model</strong> is</p>
<p><span class="math display">\[ \hat{Y}_i = b_0 + b_1X_{1i} + b_2X_{2i} + \cdots + b_pX_{pi}  \]</span> and <span class="math inline">\(e_i\)</span> is called the <strong>residual</strong> and</p>
<p><span class="math display">\[ e_i \overset{iid}{\sim} N(0,s^2)\]</span></p>
<p>To note, <span class="math inline">\(b_0, b_1, \dots, b_p\)</span> and <span class="math inline">\(s^2\)</span> are parameters estimated from a given sample to approximate the unknown and constant population parameters <span class="math inline">\(\beta_0, \beta_1, \dots, \beta_p\)</span> and <span class="math inline">\(\sigma\)</span>.</p>
</div>
</div>
<div id="the-ordinary-least-squres-method" class="section level2">
<h2><span class="header-section-number">6.2</span> The Ordinary Least-Squres Method</h2>
<p>Similar to the case of simple linear regression, the <strong>ordinary least squares (OLS)</strong> method can also be applied to estimate sample parameters for multiple linear regression.</p>
<p><span class="math display">\[ SSR = \sum_{i=1}^n R_i^2 = \sum_{i=1}^n (Y_i-\hat{Y}_i)^2 = \sum_{i=1}^n (Y_i - (b_0 + b_1X_{1i} + b_2X_{2i} + \cdots + b_pX_{pi}))^2 \]</span></p>
<p>The solutions for <span class="math inline">\(b_j\)</span>, <span class="math inline">\(j = 1,2,\dots,p\)</span>, should minimize <span class="math inline">\(SSR\)</span>, hence the name <strong>least-squares method</strong>.</p>
<div id="interpretations-of-regression-coefficients-1" class="section level3">
<h3><span class="header-section-number">6.2.1</span> Interpretations of Regression Coefficients</h3>
<p>Given the following <em>mean response model</em>,</p>
<p><span class="math display">\[ \hat{Y}_i = b_0 + b_1X_{1i} + b_2X_{2i} + \cdots + b_pX_{pi}  \]</span></p>
<ul>
<li>intercept <span class="math inline">\(b_0\)</span>: The expected value (i.e., mean) of Y when all IVs are zero.</li>
<li>slope <span class="math inline">\(b_i\)</span>: Holding other IVs constant, increase in <span class="math inline">\(X_i\)</span> by 1 unit corresponds to an average of <span class="math inline">\(b_i\)</span> units of increase in Y.</li>
</ul>
</div>
<div id="residual-variance-1" class="section level3">
<h3><span class="header-section-number">6.2.2</span> Residual Variance</h3>
<p>The <strong>residual variance</strong> (aka <strong>conditional variance</strong>) is given as follows.</p>
<p><span class="math display">\[ s^2 = MSR = \frac{\sum R_i^2}{df} 
= \frac{\sum_{i=1}^n (Y_i-\hat{Y_i})^2}{df} = \frac{\sum_{i=1}^n (Y_i - (b_0 + b_1X_{1i} + b_2X_{2i} + \cdots + b_pX_{pi}))^2}{n-(1+p)} \]</span></p>
<p>It should be clear that <span class="math inline">\(1+p\)</span> is the number of parameters required to estimate <span class="math inline">\(\hat{Y}_i\)</span> (i.e., the degrees of freedom we have to spend or give up for fitting the model), which equals <span class="math inline">\(p\)</span> slope parameters plus 1 for the intercept. The denominator <span class="math inline">\(n-(1+p)\)</span> is the degrees of freedom that are left for us after fitting the model.</p>
<p>As a result, <span class="math inline">\(s\)</span> is called the <strong>residual standard deviation</strong> or <strong>conditional standard deviation</strong> or <strong>RMSE</strong>.</p>
</div>
</div>
<div id="standard-errors-of-regression-coefficients" class="section level2">
<h2><span class="header-section-number">6.3</span> Standard Errors of Regression Coefficients</h2>
<p>As explained previously, we are usually only interested in the standard error of a slope. Let <span class="math inline">\(b_j\)</span> denote the slope parameter for the <em>j</em>th feature <span class="math inline">\(X_j\)</span> where <span class="math inline">\(j = 1,2,\dots,p\)</span>. We have</p>
<p><span class="math display">\[ s_{b_j}^2 = \frac{s^2}{\text{SSX}_j} \cdot \text{VIF}_j 
= \frac{s^2}{(n-1) Var(X_j)} \cdot \text{VIF}_j \]</span> where <span class="math inline">\(s^2\)</span> is the residual variance, <span class="math inline">\(\text{SSX}_j = \sum_{i=1}^m (X_{ij}-\bar{X}_j)^2\)</span> is the sum of squares for <span class="math inline">\(X_j\)</span>, and <span class="math inline">\(\text{VIF}_j\)</span> is an important concept called <strong>variance inflation factor</strong>.</p>
<div id="variance-inflation-factor" class="section level3">
<h3><span class="header-section-number">6.3.1</span> Variance Inflation Factor</h3>
<p>To compute VIF for <span class="math inline">\(X_j\)</span>, use the following equation.</p>
<p><span class="math display">\[ \text{VIF}_j = \frac{1}{1-R_j^2} \]</span> where <span class="math inline">\(R_j^2\)</span> is the R-squared of regressing <span class="math inline">\(X_j\)</span> on all the other IVs except for <span class="math inline">\(X_j\)</span>. To be clear, the VIF of <span class="math inline">\(X_j\)</span> can be computed in three steps.</p>
<ul>
<li>Step 1: For <span class="math inline">\(X_1,X_2,\dots,X_p\)</span>, regress <span class="math inline">\(Y_j\)</span> on the rest of the IVs: There are only <span class="math inline">\(p-1\)</span> features and <span class="math inline">\(Y\)</span> is completely ignored.</li>
</ul>
<p><span class="math display">\[ X_j = a_0 + a_1X_1 + a_2X_2 + \cdots + a_pX_p \]</span></p>
<ul>
<li><p>Step 2: Compute the <span class="math inline">\(R^2\)</span> for this regression model, which results in <span class="math inline">\(R_j^2\)</span>.</p></li>
<li><p>Step 3: Compute VIF based on the formula given above.</p></li>
</ul>
</div>
<div id="significance-of-vif" class="section level3">
<h3><span class="header-section-number">6.3.2</span> Significance of VIF</h3>
<p>Remember that <span class="math inline">\(R^2\)</span> is interpreted as the proportion of variance explained by the IVs and <span class="math inline">\(1-R^2\)</span> is the proportion of unexplained variance. It should be clear that when <span class="math inline">\(X_j\)</span> highly correlates with other features, <span class="math inline">\(R_j^2\)</span> would be high and <span class="math inline">\(1-R_j^2\)</span> would be low and hence <span class="math inline">\(VIF_j\)</span> tends to be high. In fact, when <span class="math inline">\(X_j\)</span> does not correlate with any other features, i.e., <span class="math inline">\(R_j^2 = 0\)</span>, we will have <span class="math inline">\(VIF_j = 1\)</span>, which would give the smallest possible standard error for <span class="math inline">\(b_j\)</span>, as shown in the following equation. To note, this is the same formula for the standard error of <span class="math inline">\(b_j\)</span> in simple linear regression.</p>
<p><span class="math display">\[ s_{b_j}^2 = \frac{s^2}{\text{SSX}_j} \cdot \text{VIF}_j = \frac{s^2}{\text{SSX}_j} \]</span> In contrast, if <span class="math inline">\(X_j\)</span> highly correlates with some other features such that the <span class="math inline">\(R_j^2 = 0.90\)</span>, i.e., 90% of the variance in <span class="math inline">\(X_j\)</span> can be explained by the other features, then <span class="math inline">\(VIF_j = 10\)</span>. In other words, the standard error of <span class="math inline">\(b_j\)</span> derived from multiple linear regression would be inflated 10 times as compared to the standard error of <span class="math inline">\(b_j\)</span> that we would have got from a simple linear regression model where <span class="math inline">\(X_j\)</span> is the sole predictor.</p>
</div>
<div id="multicollinearity" class="section level3">
<h3><span class="header-section-number">6.3.3</span> Multicollinearity</h3>
<p>This previous example illustrates how the term VIF gets its name <em>variance inflation factor</em> and the phenonemon of variance inflation due to presence of highly correlated featuers is called <strong>multicollinearity</strong>. As a rule of thumb, multicollinearity is usually not regarded as an issue unless VIF is greater than 10.</p>
<p>Another point worth noticing is that, since in most situations, a given feature will always correlate with some other features, IVs or features in the context of multiple linear regression are also called <strong>covariates</strong> (since features co-vary), which is particularly true for continuous features.</p>
</div>
<div id="influencing-factors-on-standard-errors" class="section level3">
<h3><span class="header-section-number">6.3.4</span> Influencing Factors on Standard Errors</h3>
<p>With the meaning of VIF explained, we can now discuss the factors affecting the standard error of the slope parameter <span class="math inline">\(b_j\)</span> for the covariate <span class="math inline">\(X_j\)</span>.</p>
<p><span class="math display">\[ s_{b_j}^2 = \frac{s^2}{\text{SSX}_j} \cdot \text{VIF}_j 
= \frac{s^2}{(n-1) Var(X_j)} \cdot \frac{1}{1-R_j^2} \]</span></p>
<ul>
<li><span class="math inline">\(s^2\)</span>: Better model fit leads to smaller standard errors, or greater scatter in data around the regression line leads to more uncertainties in slope parameter estimations.</li>
<li><span class="math inline">\(n\)</span>: Larger sample size results in smaller standard errors.</li>
<li><span class="math inline">\(Var(X_j)\)</span>: Greater instrinsic variability in a covariate yields smaller standard errors.</li>
<li><span class="math inline">\(R_j^2\)</span>: Features that highly correlate with other features might associate with large standard errors.</li>
</ul>
</div>
</div>
<div id="r2-and-anova-for-multiple-regression" class="section level2">
<h2><span class="header-section-number">6.4</span> <span class="math inline">\(R^2\)</span> and ANOVA for Multiple Regression</h2>
<p>When dealing with simple linear regression, we have already studied how to derive the ANOVA table, compute the <span class="math inline">\(R^2\)</span> and perform the <span class="math inline">\(F\)</span> test. The same results also hold here, which will not be repeated except for one point. The formula for computing <span class="math inline">\(R^2\)</span> is</p>
<p><span class="math display">\[ R^2 = \frac{\text{SSM}}{\text{SST}} = \frac{ \sum(\hat{Y}_i-\bar{Y})^2 }{ \sum(Y_i-\bar{Y})^2 } \]</span></p>
<p>In the context of multiple regression, the statistic <span class="math inline">\(R^2\)</span> is called the <strong>coefficient of determination</strong> of the linear model, and the square root of <span class="math inline">\(R^2\)</span> is called the <strong>multiple correlation coefficient</strong>, which interestingly enough is the correlation between observed values <span class="math inline">\(Y_i\)</span> and predicted values <span class="math inline">\(\hat{Y}_i\)</span>.</p>
<p><strong>R_Proof: Multiple Correlation Coefficient</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">tdat =<span class="st"> </span><span class="kw">na.omit</span>(xdat[<span class="kw">c</span>(<span class="st">&quot;mid3&quot;</span>,<span class="st">&quot;satmath&quot;</span>,<span class="st">&quot;satreading&quot;</span>,<span class="st">&quot;gpa&quot;</span>)])
mod1 =<span class="st"> </span><span class="kw">lm</span>(mid3 ~<span class="st"> </span>satmath +<span class="st"> </span>satreading +<span class="st"> </span>gpa, <span class="dt">data=</span>tdat)
yhat =<span class="st"> </span><span class="kw">predict</span>(mod1)
ybar =<span class="st"> </span><span class="kw">mean</span>(tdat$mid3, <span class="dt">na.rm=</span>T)
ssm =<span class="st"> </span><span class="kw">sum</span>((yhat-ybar)^<span class="dv">2</span>, <span class="dt">na.rm=</span>T)
sst =<span class="st"> </span><span class="kw">sum</span>((tdat$mid3-ybar)^<span class="dv">2</span>, <span class="dt">na.rm=</span>T)
r2 =<span class="st"> </span>ssm/sst
r =<span class="st"> </span><span class="kw">cor</span>(tdat$mid3, yhat)
<span class="kw">c</span>(<span class="kw">sqrt</span>(r2), r)</code></pre></div>
<pre><code>## [1] 0.07316646 0.07316646</code></pre>
</div>
<div id="regression-models-extended---categorical-features" class="section level2">
<h2><span class="header-section-number">6.5</span> Regression Models Extended - Categorical Features</h2>
<div id="dummy-variables" class="section level3">
<h3><span class="header-section-number">6.5.1</span> Dummy Variables</h3>
<p>A <strong>dummy variable</strong> has only two unique levels. For comparison purposes, we would designate one level as the <strong>reference level</strong> and code it as 0, whereas the other level is coded as 1. Since 1 is usually used to indicate the presence of something of interest, a dummy variable is also called a <strong>indicator variable</strong>.</p>
<p>When a dummy variable is the only feature in a linear regression model, it gives identical results to a <strong><em>two-sample t-test</em></strong>. To see how this is true, let’s use a concrete example, where we regress final exam scores on gender to see if males outperformed females in a Chemistry class. Let us use <em>M</em> to indicate a dummy variable, where <em>M=0</em> for females and <em>M=1</em> for males. The prediction model would be as follows.</p>
<p><span class="math display">\[ \hat{Y}_i = b_0 + b_1M_i \]</span> Three points can be inferred from this equation:</p>
<ul>
<li>When <span class="math inline">\(M_i = 0\)</span>, the group average for females is <span class="math inline">\(\hat{Y}_i=b_0\)</span>.<br />
</li>
<li>When <span class="math inline">\(M_i = 1\)</span>, the group average for males is <span class="math inline">\(\hat{Y}_i=b_0+b_1\)</span>.<br />
</li>
<li>The interpretation of <span class="math inline">\(b_1\)</span> is that 1 unit increase in <span class="math inline">\(M_i\)</span>, i.e., changing from female to male, <span class="math inline">\(\hat{Y}_i\)</span> on average would increase by <span class="math inline">\(b_1\)</span>.</li>
</ul>
<p>Putting together, it should be clear that (a) <span class="math inline">\(b_0\)</span> is the group mean for females, (b) <span class="math inline">\(b_0+b_1\)</span> is the group mean for males, and (c) <span class="math inline">\(b_1\)</span> is the difference in group means.</p>
<p><strong>R_Proof: Regression vs Two-sample T-test</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># two-sample t-test with equal variance</span>
tt =<span class="st"> </span><span class="kw">t.test</span>(mid3 ~<span class="st"> </span>gender, <span class="dt">data=</span>xdat, <span class="dt">var.equal=</span><span class="ot">TRUE</span>)
<span class="co"># simple linear regression with a dummy feature</span>
mod =<span class="st"> </span><span class="kw">lm</span>(mid3 ~<span class="st"> </span>gender, <span class="dt">data=</span>xdat)
modsum =<span class="st"> </span><span class="kw">summary</span>(mod)

modsum$coefficients</code></pre></div>
<pre><code>##              Estimate Std. Error   t value     Pr(&gt;|t|)
## (Intercept) 98.426230   3.720447 26.455485 1.282478e-65
## genderMale   8.830913   6.161646  1.433207 1.534421e-01</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">c</span>(<span class="kw">diff</span>(tt$estimate), tt$statistic, tt$p.value)</code></pre></div>
<pre><code>## mean in group Male                  t                    
##          8.8309133         -1.4332070          0.1534421</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">r2 =<span class="st"> </span>modsum$r.squared
r =<span class="st"> </span><span class="kw">cor</span>(xdat$mid3, <span class="kw">as.numeric</span>(xdat$gender)-<span class="dv">1</span>, <span class="dt">use=</span><span class="st">&quot;pair&quot;</span>)
<span class="kw">c</span>(r2, r^<span class="dv">2</span>)</code></pre></div>
<pre><code>## [1] 0.01069533 0.01069533</code></pre>
</div>
<div id="nominal-variables" class="section level3">
<h3><span class="header-section-number">6.5.2</span> Nominal Variables</h3>
<p>In regression analysis, a nominal variable is routinely converted into a set of dummy features, designate one level/feature as the <strong>reference level</strong>, and put the rest of the levels/features into the regression model. To illustrate this, let’s regress final exam scores on year to examine if students from different years performed differently. There are three sessions (i.e., years) in the dataset. Let’s use <em>S1</em>, <em>S2</em>, and <em>S3</em> to denote the first, second, and third sessions, and designate <em>S1</em> as the <strong>reference level</strong>. The prediction model would be as follows.</p>
<p><span class="math display">\[ \hat{Y}_i = b_0 + b_1S_{2i} + b_2S_{3i} \]</span> To note, since the dummy feature <em>S1</em> is used as the reference level, it is absent as a feature from the equation. Moreover, the three dummy variables are mutually exlucsive, e.g., when <em>S1=1</em>, <em>S2</em> and <em>S3</em> have to be zero. Three points can be inferred from this equation:</p>
<ul>
<li>When <span class="math inline">\(S_{1i} = 1\)</span>, the group average for students from the first session is <span class="math inline">\(\hat{Y}_i=b_0\)</span>.<br />
</li>
<li>When <span class="math inline">\(S_{2i} = 1\)</span>, the group average for students from the second session is <span class="math inline">\(\hat{Y}_i=b_0+b_1\)</span>.<br />
</li>
<li>When <span class="math inline">\(S_{3i} = 1\)</span>, the group average for students from the third session is <span class="math inline">\(\hat{Y}_i=b_0+b_2\)</span>.</li>
</ul>
<p>Putting together, it should be clear that (a) <span class="math inline">\(b_1\)</span> is the difference in group means between the second and first sessions, and (b) <span class="math inline">\(b_2\)</span> is the difference in group means between the third and first sessions. If there are more sessions up to <em>p</em> levels, then <span class="math inline">\(b_3,b_4,\dots,b_p\)</span> would be difference in group means between each of these levels and the first session. Everything is compared to the first session, which is why it is termed as the <strong>reference level</strong> or <strong>base level</strong> or <strong>reference group</strong>.</p>
<p>When a nominal variable is converted into a set of dummies and used as the only feature in a linear regression model, it gives identical results to the <strong><em>one-way ANOVA</em></strong>.</p>
<p><strong>R_Proof: Regression vs ANOVA</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># one-way ANOVA</span>
aovmod =<span class="st"> </span><span class="kw">aov</span>(mid3 ~<span class="st"> </span>session, <span class="dt">data=</span>xdat)
aovsum =<span class="st"> </span><span class="kw">summary</span>(aovmod)
aovsum</code></pre></div>
<pre><code>##              Df Sum Sq Mean Sq F value Pr(&gt;F)
## session       2    114    56.8   0.033  0.967
## Residuals   189 324206  1715.4               
## 8 observations deleted due to missingness</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># simple linear regression with a nominal feature</span>
mod =<span class="st"> </span><span class="kw">lm</span>(mid3 ~<span class="st"> </span>session, <span class="dt">data=</span>xdat)
<span class="kw">anova</span>(mod)</code></pre></div>
<pre><code>## Analysis of Variance Table
## 
## Response: mid3
##            Df Sum Sq Mean Sq F value Pr(&gt;F)
## session     2    114   56.81  0.0331 0.9674
## Residuals 189 324206 1715.38</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">modsum =<span class="st"> </span><span class="kw">summary</span>(mod)
modsum</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = mid3 ~ session, data = xdat)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -86.620 -32.910  -0.779  32.221  89.380 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  100.571      5.535  18.171   &lt;2e-16 ***
## sessionSS10    2.049      8.058   0.254    0.800    
## sessionSS11    1.208      7.112   0.170    0.865    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 41.42 on 189 degrees of freedom
##   (8 observations deleted due to missingness)
## Multiple R-squared:  0.0003503,  Adjusted R-squared:  -0.01023 
## F-statistic: 0.03312 on 2 and 189 DF,  p-value: 0.9674</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">modsum$fstatistic</code></pre></div>
<pre><code>##       value       numdf       dendf 
##   0.0331181   2.0000000 189.0000000</code></pre>
<p>Let’s create a nominal feature with three levels (i.e., ‘a’,‘b’,‘c’) and use the <code>model.matrix()</code> function in R to generate a set of dummy features.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">xvec =<span class="st"> </span><span class="kw">gl</span>(<span class="dv">3</span>, <span class="dv">2</span>, <span class="dt">labels=</span><span class="kw">c</span>(<span class="st">&quot;a&quot;</span>,<span class="st">&quot;b&quot;</span>,<span class="st">&quot;c&quot;</span>))
tdat =<span class="st"> </span><span class="kw">data.frame</span>(xvec)
tdat</code></pre></div>
<pre><code>##   xvec
## 1    a
## 2    a
## 3    b
## 4    b
## 5    c
## 6    c</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">model.matrix</span>(~xvec, <span class="dt">data=</span>tdat)</code></pre></div>
<pre><code>##   (Intercept) xvecb xvecc
## 1           1     0     0
## 2           1     0     0
## 3           1     1     0
## 4           1     1     0
## 5           1     0     1
## 6           1     0     1
## attr(,&quot;assign&quot;)
## [1] 0 1 1
## attr(,&quot;contrasts&quot;)
## attr(,&quot;contrasts&quot;)$xvec
## [1] &quot;contr.treatment&quot;</code></pre>
<p>Notice that (a) a column of 1s is automatically created, and (b) the first level ‘a’ is missing from the results.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">model.matrix</span>(~xvec<span class="dv">-1</span>, <span class="dt">data=</span>tdat)</code></pre></div>
<pre><code>##   xveca xvecb xvecc
## 1     1     0     0
## 2     1     0     0
## 3     0     1     0
## 4     0     1     0
## 5     0     0     1
## 6     0     0     1
## attr(,&quot;assign&quot;)
## [1] 1 1 1
## attr(,&quot;contrasts&quot;)
## attr(,&quot;contrasts&quot;)$xvec
## [1] &quot;contr.treatment&quot;</code></pre>
</div>
</div>
<div id="regression-models-extended---interaction-effects" class="section level2">
<h2><span class="header-section-number">6.6</span> Regression Models Extended - Interaction Effects</h2>
<p>The <strong>interaction effects</strong> are also called <strong>moderation effects</strong>, which are critically important in regression analysis. It is the idea that the effect of X on Y is <strong><em>moderated</em></strong> by Z. For example, we might be interested in the effect of <em>study time</em> X on <em>course performance</em> Y. It is conceivable that the way study time affects exam results might depend on <em>whether the time is spent willingly</em> Z.</p>
<ul>
<li>If students are forced to spend a lot of time studying, then the more time spent may or may not translate to stronger performance.<br />
</li>
<li>If students are willingly investing time in a course, more time spent should have a much stronger effect on exam performance.</li>
</ul>
<div id="dummy-dummy-interactions" class="section level3">
<h3><span class="header-section-number">6.6.1</span> Dummy-Dummy Interactions</h3>
<p>In our dataset, let’s consider the effect of repeater status R (i.e., repeater=1) on final exam scores Y moderated by gender Male (i.e., male=1).</p>
<p><span class="math display">\[ \hat{Y}_i = b_0 + b_1\cdot\text{R} + b_2\cdot\text{Male} + b_3\cdot\text{R}\cdot\text{Male} \]</span></p>
<p>For <span class="math inline">\(Male=0\)</span> (i.e., females), we have</p>
<p><span class="math display">\[ \hat{Y}_i = b_0 + b_1\cdot\text{R} \]</span></p>
<p>which shows (a) that the average score for female non-repeaters is <span class="math inline">\(b_0\)</span> and (b) that females enjoyed an average of <span class="math inline">\(b_1\)</span> points increase when switching from non-repeater to repeater status.</p>
<p>For <span class="math inline">\(Male=1\)</span> (i.e., males), we have</p>
<p><span class="math display">\[ \hat{Y}_i = b_0 + b_1\cdot\text{R} + b_2 + b_3\cdot\text{R} 
= (b_0+b_2) + (b_1+b_3)\cdot\text{R}\]</span></p>
<p>which shows (a) that the average score for male non-repeaters was <span class="math inline">\(b_2\)</span> points higher than that for female non-repeaters and (b) that for the same switch from non-repeater to repeater status, males enjoyed an additional <span class="math inline">\(b_3\)</span> points increase as compared to the score gain for females making the same status switch.</p>
<p>Now let’s fit a linear regression model and examine the coefficients.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mod =<span class="st"> </span><span class="kw">lm</span>(mid3 ~<span class="st"> </span>repeater*gender, <span class="dt">data=</span>xdat)
modsum =<span class="st"> </span><span class="kw">summary</span>(mod)
modsum</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = mid3 ~ repeater * gender, data = xdat)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -91.109 -26.337   2.514  24.455  80.976 
## 
## Coefficients:
##                             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)                  111.024      3.861  28.752  &lt; 2e-16 ***
## repeaterRepeater             -41.537      7.012  -5.924 1.47e-08 ***
## genderMale                    13.085      6.516   2.008   0.0461 *  
## repeaterRepeater:genderMale   -7.613     11.381  -0.669   0.5043    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 35.6 on 188 degrees of freedom
##   (8 observations deleted due to missingness)
## Multiple R-squared:  0.2653, Adjusted R-squared:  0.2536 
## F-statistic: 22.63 on 3 and 188 DF,  p-value: 1.488e-12</code></pre>
<p>The visualize a dummy-continuous interaction effect, fit a simple linear regression model between X and Y conditioned on Z.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">newdat =<span class="st"> </span><span class="kw">expand.grid</span>(mod$xlevels)
newdat$yhat =<span class="st"> </span><span class="kw">predict</span>(mod, newdat)

yrange =<span class="st"> </span><span class="kw">range</span>(newdat$yhat)
yrange =<span class="st"> </span>yrange +<span class="st"> </span><span class="kw">c</span>(-<span class="dv">1</span>,<span class="dv">1</span>)*<span class="kw">diff</span>(yrange)*<span class="fl">0.1</span>
<span class="kw">plot</span>(<span class="dv">1</span>:<span class="dv">2</span>, newdat$yhat[<span class="dv">1</span>:<span class="dv">2</span>], <span class="dt">ylim=</span>yrange, <span class="dt">type=</span><span class="st">&quot;o&quot;</span>,
     <span class="dt">xaxt=</span><span class="st">&quot;n&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;Final Exam Scores&quot;</span>, <span class="dt">xlab=</span><span class="ot">NA</span>)
<span class="kw">axis</span>(<span class="dv">1</span>, <span class="dt">at=</span><span class="dv">1</span>:<span class="dv">2</span>, <span class="dt">labels=</span><span class="kw">levels</span>(newdat[,<span class="dv">1</span>]))
<span class="kw">points</span>(<span class="dv">1</span>:<span class="dv">2</span>, newdat$yhat[<span class="dv">3</span>:<span class="dv">4</span>], <span class="dt">type=</span><span class="st">&quot;o&quot;</span>, <span class="dt">pty=</span><span class="dv">2</span>, <span class="dt">lty=</span><span class="dv">2</span>)</code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-28-1.png" width="672" /></p>
</div>
<div id="dummy-continuous-interactions" class="section level3">
<h3><span class="header-section-number">6.6.2</span> Dummy-Continuous Interactions</h3>
<p>In our dataset, let’s consider the effect of high school GPA X on final exam scores Y moderated by repeater status R.</p>
<p><span class="math display">\[ \hat{Y}_i = b_0 + b_1\cdot\text{GPA} + b_2\cdot\text{R} + b_3\cdot\text{GPA}\cdot\text{R} \]</span></p>
<p>For <span class="math inline">\(R=0\)</span> (i.e., non-repeaters), we have</p>
<p><span class="math display">\[ \hat{Y}_i = b_0 + b_1\cdot\text{GPA} \]</span></p>
<p>which shows that (a) a non-repeater with 0 highschool GPA got an average of <span class="math inline">\(b_0\)</span> points in the final exam, and (b) that the average effect of 1 point increase in GPA on exam performance is <span class="math inline">\(b_1\)</span> points for non-repeaters.</p>
<p>For <span class="math inline">\(R=1\)</span> (i.e., repeaters), we have</p>
<p><span class="math display">\[ \hat{Y}_i = b_0 + b_1\cdot\text{GPA} + b_2 + b_3\cdot\text{GPA} 
= (b_0+b_2) + (b_1+b_3)\cdot\text{GPA}\]</span></p>
<p>which shows that (a) repeaters with 0 highschool GPA would on average get <span class="math inline">\(b_2\)</span> points higher on the final exam, and (b) that the average effect of 1 point increase in GPA on exam performance is <span class="math inline">\(b_1+b_3\)</span> for repeaters. Hence, for the same 1 point increase in GPA, repeaters enjoyed an extra <span class="math inline">\(b_3\)</span> points increase in the final exam as compared to the <span class="math inline">\(b_1\)</span> points gain in final scores for non-repeaters.</p>
<p>The visualize a dummy-continuous interaction effect, fit a simple linear regression model between X and Y conditioned on Z.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">car::<span class="kw">scatterplot</span>(mid3 ~<span class="st"> </span>gpa*repeater, <span class="dt">data=</span>xdat)</code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-29-1.png" width="672" /></p>
<p>In R, interaction effect between X and Z is represented by <code>X*Z</code>. To note, whenever an interaction effect (e.g., <code>X*Z</code>) is added to a linear model, the component variables involved (e.g., <code>X</code> and <code>Z</code>) will be automatically added to the linear model as well. In other words, the following two linear models give identical results.</p>
<p><code>lm(mid3 ~ repeater*gpa, data=xdat)</code></p>
<p><code>lm(mid3 ~ repeater + gpa + repeater*gpa, data=xdat)</code></p>
<p>Now let’s fit a linear regression model and examine the coefficients.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mod =<span class="st"> </span><span class="kw">lm</span>(mid3 ~<span class="st"> </span>repeater*gpa, <span class="dt">data=</span>xdat)
modsum =<span class="st"> </span><span class="kw">summary</span>(mod)
modsum</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = mid3 ~ repeater * gpa, data = xdat)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -83.477 -26.153   4.577  24.992  77.077 
## 
## Coefficients:
##                      Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)           129.451     15.230   8.499 5.79e-15 ***
## repeaterRepeater      -99.104     29.478  -3.362 0.000938 ***
## gpa                    -4.738      5.107  -0.928 0.354678    
## repeaterRepeater:gpa   18.394      9.668   1.903 0.058618 .  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 35.67 on 188 degrees of freedom
##   (8 observations deleted due to missingness)
## Multiple R-squared:  0.2624, Adjusted R-squared:  0.2507 
## F-statistic:  22.3 on 3 and 188 DF,  p-value: 2.135e-12</code></pre>
</div>
<div id="continuous-continuous-interactions" class="section level3">
<h3><span class="header-section-number">6.6.3</span> Continuous-Continuous Interactions</h3>
<p>Let’s consider the effect of highschool GPA on final exam scores Y moderated by perceived usefulness of the course U.</p>
<p><span class="math display">\[ \hat{Y}_i = b_0 + b_1\cdot\text{GPA} + b_2 \cdot U + b_3\cdot\text{GPA} \cdot U \]</span></p>
<p>When <span class="math inline">\(U=0\)</span>, we have</p>
<p><span class="math display">\[ \hat{Y}_i = b_0 + b_1\cdot\text{GPA} \]</span></p>
<p>When <span class="math inline">\(U=1\)</span>, we have</p>
<p><span class="math display">\[ \hat{Y}_i = (b_0 +b_2) + (b_1+b_3)\cdot\text{GPA} \]</span></p>
<p>At this point, the interpretations of the coefficients are very similar to those we have encountered in the case of dummy-continuous interactions. To be noted, if we use <span class="math inline">\(U\)</span> and <span class="math inline">\(U+1\)</span> instead of using <span class="math inline">\(U=0\)</span> and <span class="math inline">\(U=1\)</span>, we would end up with very similar interpretations: (a) 1 unit increase in perceived course usefulness would result in <span class="math inline">\(b_0\)</span> points increase in final exam scores for students with 0 GPA, and (b) 1 unit increase in perceived course usefulness would enhance the effect of GPA by <span class="math inline">\(b_3\)</span> points per 1 unit increase in GPA.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mod =<span class="st"> </span><span class="kw">lm</span>(mid3 ~<span class="st"> </span>gpa*utcourse, <span class="dt">data=</span>xdat)
modsum =<span class="st"> </span><span class="kw">summary</span>(mod)
modsum</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = mid3 ~ gpa * utcourse, data = xdat)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -88.790 -31.017   1.257  30.518  91.027 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)    48.479     57.140   0.848    0.397
## gpa            13.523     19.300   0.701    0.484
## utcourse       13.042     12.221   1.067    0.287
## gpa:utcourse   -3.354      4.090  -0.820    0.413
## 
## Residual standard error: 41.6 on 171 degrees of freedom
##   (25 observations deleted due to missingness)
## Multiple R-squared:  0.01214,    Adjusted R-squared:  -0.005192 
## F-statistic: 0.7004 on 3 and 171 DF,  p-value: 0.553</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">xs =<span class="st"> </span><span class="kw">quantile</span>(xdat$gpa, <span class="dt">na.rm=</span><span class="ot">TRUE</span>)
ys =<span class="st"> </span><span class="kw">quantile</span>(xdat$utcourse, <span class="dt">na.rm=</span><span class="ot">TRUE</span>, <span class="dt">probs=</span><span class="kw">c</span>(<span class="fl">0.25</span>, <span class="fl">0.75</span>))
newdat =<span class="st"> </span><span class="kw">expand.grid</span>(xs,ys)
<span class="kw">colnames</span>(newdat) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;gpa&quot;</span>,<span class="st">&quot;utcourse&quot;</span>)
newdat$yhat =<span class="st"> </span><span class="kw">predict</span>(mod, newdat)

yrange =<span class="st"> </span><span class="kw">range</span>(newdat$yhat)
yrange =<span class="st"> </span>yrange +<span class="st"> </span><span class="kw">c</span>(-<span class="dv">1</span>,<span class="dv">1</span>)*<span class="kw">diff</span>(yrange)*<span class="fl">0.1</span>
<span class="kw">plot</span>(xdat$gpa, xdat$mid3,<span class="dt">xlab=</span><span class="st">&quot;GPA&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;Final Exam Scores&quot;</span>)
<span class="kw">points</span>(newdat$gpa[<span class="dv">1</span>:<span class="dv">5</span>], newdat$yhat[<span class="dv">1</span>:<span class="dv">5</span>], <span class="dt">type=</span><span class="st">&quot;o&quot;</span>, <span class="dt">pch=</span><span class="dv">2</span>)
<span class="kw">points</span>(newdat$gpa[<span class="dv">6</span>:<span class="dv">10</span>], newdat$yhat[<span class="dv">6</span>:<span class="dv">10</span>], <span class="dt">type=</span><span class="st">&quot;o&quot;</span>, <span class="dt">pch=</span><span class="dv">5</span>, <span class="dt">lty=</span><span class="dv">2</span>)
<span class="kw">legend</span>(<span class="st">&quot;topleft&quot;</span>, <span class="dt">legend=</span><span class="kw">c</span>(<span class="st">&quot;25% Quantile&quot;</span>,<span class="st">&quot;75% Quantile&quot;</span>), <span class="dt">bty=</span><span class="st">&quot;n&quot;</span>, <span class="dt">pch=</span><span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">5</span>), <span class="dt">lty=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))</code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-32-1.png" width="672" /></p>
<p>To note, do not plot only the fitted lines without the actual data points.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(newdat$gpa[<span class="dv">1</span>:<span class="dv">5</span>], newdat$yhat[<span class="dv">1</span>:<span class="dv">5</span>], <span class="dt">ylim=</span>yrange, <span class="dt">type=</span><span class="st">&quot;o&quot;</span>,
     <span class="dt">xlab=</span><span class="st">&quot;GPA&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;predicted&quot;</span>, <span class="dt">pch=</span><span class="dv">2</span>)
<span class="kw">points</span>(newdat$gpa[<span class="dv">6</span>:<span class="dv">10</span>], newdat$yhat[<span class="dv">6</span>:<span class="dv">10</span>], <span class="dt">type=</span><span class="st">&quot;o&quot;</span>, <span class="dt">pch=</span><span class="dv">5</span>, <span class="dt">lty=</span><span class="dv">2</span>)</code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-33-1.png" width="672" /></p>
</div>
</div>
<div id="regression-models-extended---nonlinear-relations" class="section level2">
<h2><span class="header-section-number">6.7</span> Regression Models Extended - Nonlinear Relations</h2>
</div>
<div id="model-comparision-evaluation" class="section level2">
<h2><span class="header-section-number">6.8</span> Model Comparision &amp; Evaluation</h2>
<div id="f-test-for-nested-models" class="section level3">
<h3><span class="header-section-number">6.8.1</span> F-test for Nested Models</h3>
</div>
<div id="adjusted-r-squared" class="section level3">
<h3><span class="header-section-number">6.8.2</span> Adjusted R-Squared</h3>
</div>
<div id="information-criteria" class="section level3">
<h3><span class="header-section-number">6.8.3</span> Information Criteria</h3>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="linear-regression.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
