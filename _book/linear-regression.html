<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Quantitative Research Methods for Education</title>
  <meta name="description" content="This is the first trial version!">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="Quantitative Research Methods for Education" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is the first trial version!" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Quantitative Research Methods for Education" />
  
  <meta name="twitter:description" content="This is the first trial version!" />
  

<meta name="author" content="Wenliang He">


<meta name="date" content="2018-08-29">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="common-statistical-tests.html">
<link rel="next" href="multiple-linear-regression.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="libs/htmlwidgets-1.2/htmlwidgets.js"></script>
<script src="libs/d3-3.3.8/d3.min.js"></script>
<script src="libs/dagre-0.4.0/dagre-d3.min.js"></script>
<link href="libs/mermaid-0.3.0/dist/mermaid.css" rel="stylesheet" />
<script src="libs/mermaid-0.3.0/dist/mermaid.slim.min.js"></script>
<link href="libs/DiagrammeR-styles-0.2/styles.css" rel="stylesheet" />
<script src="libs/chromatography-0.1/chromatography.js"></script>
<script src="libs/DiagrammeR-binding-1.0.0/DiagrammeR.js"></script>
<link href="libs/datatables-css-0.0.0/datatables-crosstalk.css" rel="stylesheet" />
<script src="libs/datatables-binding-0.4/datatables.js"></script>
<link href="libs/dt-core-1.10.16/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="libs/dt-core-1.10.16/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="libs/dt-core-1.10.16/js/jquery.dataTables.min.js"></script>
<link href="libs/crosstalk-1.0.0/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk-1.0.0/js/crosstalk.min.js"></script>


<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#intro-to-rmarkdown"><i class="fa fa-check"></i><b>1.1</b> Intro to Rmarkdown</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#intro-to-bookdown"><i class="fa fa-check"></i><b>1.2</b> Intro to Bookdown</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#resources-to-rmarkdown"><i class="fa fa-check"></i><b>1.3</b> Resources to Rmarkdown</a></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#resources-to-bookdown"><i class="fa fa-check"></i><b>1.4</b> Resources to Bookdown</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="the-fundamentals-of-statistics.html"><a href="the-fundamentals-of-statistics.html"><i class="fa fa-check"></i><b>2</b> The Fundamentals of Statistics</a><ul>
<li class="chapter" data-level="2.1" data-path="the-fundamentals-of-statistics.html"><a href="the-fundamentals-of-statistics.html#why-studying-statistics"><i class="fa fa-check"></i><b>2.1</b> Why Studying Statistics</a><ul>
<li class="chapter" data-level="2.1.1" data-path="the-fundamentals-of-statistics.html"><a href="the-fundamentals-of-statistics.html#understanding-the-world"><i class="fa fa-check"></i><b>2.1.1</b> Understanding the World</a></li>
<li class="chapter" data-level="2.1.2" data-path="the-fundamentals-of-statistics.html"><a href="the-fundamentals-of-statistics.html#science-vs.philosophy"><i class="fa fa-check"></i><b>2.1.2</b> Science vs. Philosophy</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="the-fundamentals-of-statistics.html"><a href="the-fundamentals-of-statistics.html#the-big-picture"><i class="fa fa-check"></i><b>2.2</b> The Big Picture</a><ul>
<li class="chapter" data-level="2.2.1" data-path="the-fundamentals-of-statistics.html"><a href="the-fundamentals-of-statistics.html#central-themes-in-statistics"><i class="fa fa-check"></i><b>2.2.1</b> Central Themes in Statistics</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="the-fundamentals-of-statistics.html"><a href="the-fundamentals-of-statistics.html#the-fundamentals-of-statistics-1"><i class="fa fa-check"></i><b>2.3</b> The Fundamentals of Statistics</a><ul>
<li class="chapter" data-level="2.3.1" data-path="the-fundamentals-of-statistics.html"><a href="the-fundamentals-of-statistics.html#storing-data-in-tables"><i class="fa fa-check"></i><b>2.3.1</b> Storing Data in Tables</a></li>
<li class="chapter" data-level="2.3.2" data-path="the-fundamentals-of-statistics.html"><a href="the-fundamentals-of-statistics.html#variable-types"><i class="fa fa-check"></i><b>2.3.2</b> Variable Types</a></li>
<li class="chapter" data-level="2.3.3" data-path="the-fundamentals-of-statistics.html"><a href="the-fundamentals-of-statistics.html#exploratory-data-analysis-checking-distributions"><i class="fa fa-check"></i><b>2.3.3</b> Exploratory Data Analysis: Checking Distributions</a></li>
<li class="chapter" data-level="2.3.4" data-path="the-fundamentals-of-statistics.html"><a href="the-fundamentals-of-statistics.html#normal-distributions"><i class="fa fa-check"></i><b>2.3.4</b> Normal Distributions</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="statistical-inference.html"><a href="statistical-inference.html"><i class="fa fa-check"></i><b>3</b> Statistical Inference</a><ul>
<li class="chapter" data-level="3.1" data-path="statistical-inference.html"><a href="statistical-inference.html#random-variable"><i class="fa fa-check"></i><b>3.1</b> Random Variable</a><ul>
<li class="chapter" data-level="3.1.1" data-path="statistical-inference.html"><a href="statistical-inference.html#iid---independently-and-identically-distributed"><i class="fa fa-check"></i><b>3.1.1</b> IID - Independently and Identically Distributed</a></li>
<li class="chapter" data-level="3.1.2" data-path="statistical-inference.html"><a href="statistical-inference.html#random-variable-vs.case"><i class="fa fa-check"></i><b>3.1.2</b> Random Variable vs. Case</a></li>
<li class="chapter" data-level="3.1.3" data-path="statistical-inference.html"><a href="statistical-inference.html#properties-of-a-random-variable"><i class="fa fa-check"></i><b>3.1.3</b> Properties of a Random Variable</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="statistical-inference.html"><a href="statistical-inference.html#central-limit-theorem"><i class="fa fa-check"></i><b>3.2</b> Central Limit Theorem</a><ul>
<li class="chapter" data-level="3.2.1" data-path="statistical-inference.html"><a href="statistical-inference.html#presenting-central-limit-theorem"><i class="fa fa-check"></i><b>3.2.1</b> Presenting Central Limit Theorem</a></li>
<li class="chapter" data-level="3.2.2" data-path="statistical-inference.html"><a href="statistical-inference.html#understanding-central-limit-theorem"><i class="fa fa-check"></i><b>3.2.2</b> Understanding Central Limit Theorem</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="statistical-inference.html"><a href="statistical-inference.html#statistical-inference-1"><i class="fa fa-check"></i><b>3.3</b> Statistical Inference</a><ul>
<li class="chapter" data-level="3.3.1" data-path="statistical-inference.html"><a href="statistical-inference.html#the-diamond-expert"><i class="fa fa-check"></i><b>3.3.1</b> The Diamond Expert</a></li>
<li class="chapter" data-level="3.3.2" data-path="statistical-inference.html"><a href="statistical-inference.html#hypothesis-testing"><i class="fa fa-check"></i><b>3.3.2</b> Hypothesis Testing</a></li>
<li class="chapter" data-level="3.3.3" data-path="statistical-inference.html"><a href="statistical-inference.html#hypothesis-testing-with-unknown-variance"><i class="fa fa-check"></i><b>3.3.3</b> Hypothesis Testing with Unknown Variance</a></li>
<li class="chapter" data-level="3.3.4" data-path="statistical-inference.html"><a href="statistical-inference.html#examples-of-hypothesis-testing"><i class="fa fa-check"></i><b>3.3.4</b> Examples of Hypothesis Testing</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="common-statistical-tests.html"><a href="common-statistical-tests.html"><i class="fa fa-check"></i><b>4</b> Common Statistical Tests</a><ul>
<li class="chapter" data-level="4.1" data-path="common-statistical-tests.html"><a href="common-statistical-tests.html#dummy-x---continuous-y"><i class="fa fa-check"></i><b>4.1</b> Dummy (X) - Continuous (Y)</a><ul>
<li class="chapter" data-level="4.1.1" data-path="common-statistical-tests.html"><a href="common-statistical-tests.html#two-sample-t-test"><i class="fa fa-check"></i><b>4.1.1</b> Two-sample T-Test</a></li>
<li class="chapter" data-level="4.1.2" data-path="common-statistical-tests.html"><a href="common-statistical-tests.html#statistical-power"><i class="fa fa-check"></i><b>4.1.2</b> Statistical Power</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="common-statistical-tests.html"><a href="common-statistical-tests.html#dummy-x---dummy-y"><i class="fa fa-check"></i><b>4.2</b> Dummy (X) - Dummy (Y)</a><ul>
<li class="chapter" data-level="4.2.1" data-path="common-statistical-tests.html"><a href="common-statistical-tests.html#basics"><i class="fa fa-check"></i><b>4.2.1</b> Basics</a></li>
<li class="chapter" data-level="4.2.2" data-path="common-statistical-tests.html"><a href="common-statistical-tests.html#difference-in-proportions"><i class="fa fa-check"></i><b>4.2.2</b> Difference in Proportions</a></li>
<li class="chapter" data-level="4.2.3" data-path="common-statistical-tests.html"><a href="common-statistical-tests.html#relative-risks"><i class="fa fa-check"></i><b>4.2.3</b> Relative Risks</a></li>
<li class="chapter" data-level="4.2.4" data-path="common-statistical-tests.html"><a href="common-statistical-tests.html#odds-ratio"><i class="fa fa-check"></i><b>4.2.4</b> Odds Ratio</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="common-statistical-tests.html"><a href="common-statistical-tests.html#categorical-x---dummy-y"><i class="fa fa-check"></i><b>4.3</b> Categorical (X) - Dummy (Y)</a><ul>
<li class="chapter" data-level="4.3.1" data-path="common-statistical-tests.html"><a href="common-statistical-tests.html#basics-1"><i class="fa fa-check"></i><b>4.3.1</b> Basics</a></li>
<li class="chapter" data-level="4.3.2" data-path="common-statistical-tests.html"><a href="common-statistical-tests.html#chi-squared-test"><i class="fa fa-check"></i><b>4.3.2</b> Chi-squared Test</a></li>
<li class="chapter" data-level="4.3.3" data-path="common-statistical-tests.html"><a href="common-statistical-tests.html#g-square-test"><i class="fa fa-check"></i><b>4.3.3</b> G-square Test</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="common-statistical-tests.html"><a href="common-statistical-tests.html#categorical-x---continuous-y"><i class="fa fa-check"></i><b>4.4</b> Categorical (X) - Continuous (Y)</a><ul>
<li class="chapter" data-level="4.4.1" data-path="common-statistical-tests.html"><a href="common-statistical-tests.html#anova"><i class="fa fa-check"></i><b>4.4.1</b> ANOVA</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="linear-regression.html"><a href="linear-regression.html"><i class="fa fa-check"></i><b>5</b> Linear Regression</a><ul>
<li class="chapter" data-level="5.1" data-path="linear-regression.html"><a href="linear-regression.html#table-of-contents"><i class="fa fa-check"></i><b>5.1</b> Table of Contents</a></li>
<li class="chapter" data-level="5.2" data-path="linear-regression.html"><a href="linear-regression.html#pearson-product-moment-correlation-coefficient"><i class="fa fa-check"></i><b>5.2</b> Pearson Product-Moment Correlation Coefficient</a><ul>
<li class="chapter" data-level="5.2.1" data-path="linear-regression.html"><a href="linear-regression.html#definition"><i class="fa fa-check"></i><b>5.2.1</b> Definition</a></li>
<li class="chapter" data-level="5.2.2" data-path="linear-regression.html"><a href="linear-regression.html#properties-of-pearson-correlation"><i class="fa fa-check"></i><b>5.2.2</b> Properties of Pearson Correlation</a></li>
<li class="chapter" data-level="5.2.3" data-path="linear-regression.html"><a href="linear-regression.html#limitations-of-pearson-correlation"><i class="fa fa-check"></i><b>5.2.3</b> Limitations of Pearson Correlation</a></li>
<li class="chapter" data-level="5.2.4" data-path="linear-regression.html"><a href="linear-regression.html#other-issues-on-the-usage-of-pearson-correlation"><i class="fa fa-check"></i><b>5.2.4</b> Other Issues on the Usage of Pearson Correlation</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="linear-regression.html"><a href="linear-regression.html#simple-linear-regression"><i class="fa fa-check"></i><b>5.3</b> Simple Linear Regression</a><ul>
<li class="chapter" data-level="5.3.1" data-path="linear-regression.html"><a href="linear-regression.html#independent-variable-from-categorical-to-continuous"><i class="fa fa-check"></i><b>5.3.1</b> Independent Variable: From Categorical to Continuous</a></li>
<li class="chapter" data-level="5.3.2" data-path="linear-regression.html"><a href="linear-regression.html#fitting-a-straight-line-to-data"><i class="fa fa-check"></i><b>5.3.2</b> Fitting a Straight Line to Data</a></li>
<li class="chapter" data-level="5.3.3" data-path="linear-regression.html"><a href="linear-regression.html#the-least-squares-method"><i class="fa fa-check"></i><b>5.3.3</b> The Least-Squares Method</a></li>
<li class="chapter" data-level="5.3.4" data-path="linear-regression.html"><a href="linear-regression.html#interpretations-of-regression-coefficients"><i class="fa fa-check"></i><b>5.3.4</b> Interpretations of Regression Coefficients</a></li>
<li class="chapter" data-level="5.3.5" data-path="linear-regression.html"><a href="linear-regression.html#understanding-the-estimators"><i class="fa fa-check"></i><b>5.3.5</b> Understanding the Estimators</a></li>
<li class="chapter" data-level="5.3.6" data-path="linear-regression.html"><a href="linear-regression.html#residual-variance"><i class="fa fa-check"></i><b>5.3.6</b> Residual Variance</a></li>
<li class="chapter" data-level="5.3.7" data-path="linear-regression.html"><a href="linear-regression.html#standard-error-of-the-slope"><i class="fa fa-check"></i><b>5.3.7</b> Standard Error of the Slope</a></li>
<li class="chapter" data-level="5.3.8" data-path="linear-regression.html"><a href="linear-regression.html#effect-size-and-standardized-coefficients"><i class="fa fa-check"></i><b>5.3.8</b> Effect Size and Standardized Coefficients</a></li>
<li class="chapter" data-level="5.3.9" data-path="linear-regression.html"><a href="linear-regression.html#anova-for-regression"><i class="fa fa-check"></i><b>5.3.9</b> ANOVA for Regression</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="linear-regression.html"><a href="linear-regression.html#non-linear-relations"><i class="fa fa-check"></i><b>5.4</b> Non-linear Relations</a><ul>
<li class="chapter" data-level="5.4.1" data-path="linear-regression.html"><a href="linear-regression.html#log-transformations"><i class="fa fa-check"></i><b>5.4.1</b> Log-transformations</a></li>
<li class="chapter" data-level="5.4.2" data-path="linear-regression.html"><a href="linear-regression.html#quadratic-relations"><i class="fa fa-check"></i><b>5.4.2</b> Quadratic Relations</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="linear-regression.html"><a href="linear-regression.html#the-assumptions-of-simple-linear-regression"><i class="fa fa-check"></i><b>5.5</b> The Assumptions of Simple Linear Regression</a></li>
<li class="chapter" data-level="5.6" data-path="linear-regression.html"><a href="linear-regression.html#assumption-diagnostics"><i class="fa fa-check"></i><b>5.6</b> Assumption Diagnostics</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html"><i class="fa fa-check"></i><b>6</b> Multiple Linear Regression</a><ul>
<li class="chapter" data-level="6.1" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#regression-models---the-base-form"><i class="fa fa-check"></i><b>6.1</b> Regression Models - The Base Form</a><ul>
<li class="chapter" data-level="6.1.1" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#model-specification-with-population-parameters"><i class="fa fa-check"></i><b>6.1.1</b> Model Specification with Population Parameters</a></li>
<li class="chapter" data-level="6.1.2" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#model-specification-with-sample-parameters"><i class="fa fa-check"></i><b>6.1.2</b> Model Specification with Sample Parameters</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#the-ordinary-least-squres-method"><i class="fa fa-check"></i><b>6.2</b> The Ordinary Least-Squres Method</a><ul>
<li class="chapter" data-level="6.2.1" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#interpretations-of-regression-coefficients-1"><i class="fa fa-check"></i><b>6.2.1</b> Interpretations of Regression Coefficients</a></li>
<li class="chapter" data-level="6.2.2" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#residual-variance-1"><i class="fa fa-check"></i><b>6.2.2</b> Residual Variance</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#standard-errors-of-regression-coefficients"><i class="fa fa-check"></i><b>6.3</b> Standard Errors of Regression Coefficients</a><ul>
<li class="chapter" data-level="6.3.1" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#variance-inflation-factor"><i class="fa fa-check"></i><b>6.3.1</b> Variance Inflation Factor</a></li>
<li class="chapter" data-level="6.3.2" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#significance-of-vif"><i class="fa fa-check"></i><b>6.3.2</b> Significance of VIF</a></li>
<li class="chapter" data-level="6.3.3" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#multicollinearity"><i class="fa fa-check"></i><b>6.3.3</b> Multicollinearity</a></li>
<li class="chapter" data-level="6.3.4" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#influencing-factors-on-standard-errors"><i class="fa fa-check"></i><b>6.3.4</b> Influencing Factors on Standard Errors</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#r2-and-anova-for-multiple-regression"><i class="fa fa-check"></i><b>6.4</b> <span class="math inline">\(R^2\)</span> and ANOVA for Multiple Regression</a></li>
<li class="chapter" data-level="6.5" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#regression-models-extended---categorical-features"><i class="fa fa-check"></i><b>6.5</b> Regression Models Extended - Categorical Features</a><ul>
<li class="chapter" data-level="6.5.1" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#dummy-variables"><i class="fa fa-check"></i><b>6.5.1</b> Dummy Variables</a></li>
<li class="chapter" data-level="6.5.2" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#nominal-variables"><i class="fa fa-check"></i><b>6.5.2</b> Nominal Variables</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#regression-models-extended---interaction-effects"><i class="fa fa-check"></i><b>6.6</b> Regression Models Extended - Interaction Effects</a><ul>
<li class="chapter" data-level="6.6.1" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#dummy-dummy-interactions"><i class="fa fa-check"></i><b>6.6.1</b> Dummy-Dummy Interactions</a></li>
<li class="chapter" data-level="6.6.2" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#dummy-continuous-interactions"><i class="fa fa-check"></i><b>6.6.2</b> Dummy-Continuous Interactions</a></li>
<li class="chapter" data-level="6.6.3" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#continuous-continuous-interactions"><i class="fa fa-check"></i><b>6.6.3</b> Continuous-Continuous Interactions</a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#regression-models-extended---nonlinear-relations"><i class="fa fa-check"></i><b>6.7</b> Regression Models Extended - Nonlinear Relations</a></li>
<li class="chapter" data-level="6.8" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#model-comparision-evaluation"><i class="fa fa-check"></i><b>6.8</b> Model Comparision &amp; Evaluation</a><ul>
<li class="chapter" data-level="6.8.1" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#f-test-for-nested-models"><i class="fa fa-check"></i><b>6.8.1</b> F-test for Nested Models</a></li>
<li class="chapter" data-level="6.8.2" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#adjusted-r-squared"><i class="fa fa-check"></i><b>6.8.2</b> Adjusted R-Squared</a></li>
<li class="chapter" data-level="6.8.3" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#information-criteria"><i class="fa fa-check"></i><b>6.8.3</b> Information Criteria</a></li>
</ul></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Quantitative Research Methods for Education</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="linear-regression" class="section level1">
<h1><span class="header-section-number">5</span> Linear Regression</h1>
<div id="table-of-contents" class="section level2">
<h2><span class="header-section-number">5.1</span> Table of Contents</h2>
<ol style="list-style-type: decimal">
<li>Pearson Correlation</li>
<li>Simple Linear Regression</li>
<li>Scatterplots</li>
<li>Transformations</li>
</ol>
</div>
<div id="pearson-product-moment-correlation-coefficient" class="section level2">
<h2><span class="header-section-number">5.2</span> Pearson Product-Moment Correlation Coefficient</h2>
<div id="definition" class="section level3">
<h3><span class="header-section-number">5.2.1</span> Definition</h3>
<p>Commonly referred to as <strong>Pearson correlation</strong> or simply <strong>correlation</strong>, <strong>Pearson product-moment correlation coefficient</strong> is a standardized form of the covariance. Given the formula for variance,</p>
<p><span class="math display">\[ Var(Y) = \frac{\sum_{i=1}^n(Y_i - \bar{Y})^2}{n-1}  \]</span> one should not be too surprised to see the formula for covariance.</p>
<p><span class="math display">\[ Cov(X,Y) = \frac{\sum_{i=1}^n(X_i - \bar{X})(Y_i-\bar{Y})}{n-1}  \]</span></p>
<p>It should thus be clear that variance of Y is the covariance of Y with Y itself, i.e., variance is a special case of covariance.</p>
<p><span class="math display">\[ Cov(Y,Y) = \frac{\sum_{i=1}^n(Y_i - \bar{Y})(Y_i-\bar{Y})}{n-1} 
= \frac{\sum_{i=1}^n(Y_i - \bar{Y})^2}{n-1} = Var(Y) \]</span></p>
<p>If we divide <span class="math inline">\(Cov(X,Y)\)</span> by the corresponding standard deviations involved with regard to X and Y, we get <em>Pearson correlation</em>. In other words, Pearson correlation is the covariance of z-scores.</p>
<p><span class="math display">\[ r = \frac {\color{red}{Cov(X,Y)}} {s_Xs_Y} 
= \frac {\color{red}{\sum_{i=1}^n(X_i - \bar{X})(Y_i-\bar{Y})}} {s_X s_Y \color{red}{(n-1)}} 
= \frac{1}{n-1} \sum_{i=1}^n \color{gold}{\frac{(X_i-\bar{X})}{s_X}} \color{blue}{\frac{(Y_i-\bar{Y})}{s_Y}} 
= \frac{1}{n-1} \sum_{i=1}^n \color{gold}{z_X} \color{blue}{z_Y} \]</span></p>
</div>
<div id="properties-of-pearson-correlation" class="section level3">
<h3><span class="header-section-number">5.2.2</span> Properties of Pearson Correlation</h3>
<ol style="list-style-type: decimal">
<li>The correlation <span class="math inline">\(r\)</span> is always <strong>between -1 and 1</strong>. Values of <span class="math inline">\(r\)</span> close to 0 indicate a weak linear relationship. Values of <span class="math inline">\(r\)</span> close to 1 imply a strong positive linear relationship, whereas values close to -1 imply a negative linear relationship. The extreme cases of <span class="math inline">\(r=-1\)</span> or <span class="math inline">\(r=1\)</span> occur only when the points in a scatterplot lie exactly along a straight line. In other words, signs of <span class="math inline">\(r\)</span> show the <strong><em>direction</em></strong> and absolute values of <span class="math inline">\(r\)</span> indicate the <strong><em>strength</em></strong> of the relationship.<br />
</li>
<li>The correlation <span class="math inline">\(r\)</span> does not change when we change the <strong>units of measurement</strong> of either X or Y or both. This should be clear since correlation <span class="math inline">\(r\)</span> uses the unitless z-scores.<br />
</li>
<li>The correlation <span class="math inline">\(r\)</span> <strong>does not distinguish</strong> between independent and dependent variables. Reversing X and Y gives identical results. It has to be emphasized again that statistics knows no causal directions. Statistics on its own only reveals associations, not causations.</li>
</ol>
</div>
<div id="limitations-of-pearson-correlation" class="section level3">
<h3><span class="header-section-number">5.2.3</span> Limitations of Pearson Correlation</h3>
<ol style="list-style-type: decimal">
<li>Pearson correlation measures only <strong>linear association</strong>. Always plot your data before calculating <span class="math inline">\(r\)</span>. <code>TODO: need a graph here!</code><br />
</li>
<li>Calculation of Pearson correlation invovles means and standard deviations, and hence <span class="math inline">\(r\)</span> is <strong>susceptible to outliers</strong>. Always plot your data and look for potentially <strong><em>influential data points</em></strong> (which is defined later).</li>
</ol>
</div>
<div id="other-issues-on-the-usage-of-pearson-correlation" class="section level3">
<h3><span class="header-section-number">5.2.4</span> Other Issues on the Usage of Pearson Correlation</h3>
<ol style="list-style-type: decimal">
<li>Pearson correlation based on <strong>averaged data</strong> is usually higher than the correlation between the same variables using data for individuals. For example, the average weight of infants against their age in months would give a very strong positive correlation near one. A plot of weight against age for individual infants will show much more scatter and lower correlation. On the contrary, using a composite score averaged from multiple items measuring motivation usually gives much more consistent, and hence reliable, result.<br />
</li>
<li>When the data we use do not contain information on the full range of independent and dependent variables, we have the <strong>restricted-range problem</strong>. When data suffer from restricted range, <span class="math inline">\(r\)</span> is usually lower than it would be if the full range could be observed. <code>TODO: need an example here!</code><br />
</li>
<li><strong>Lurking variables</strong> can make correlation results misleading. This problem is not specific to Pearson correlation. All measures of two-way relationships suffer from this problem, which is known as the <strong>interaction effect</strong>. <code>TODO: need a graph here!</code><br />
</li>
<li><strong>Extrapolation</strong> (using a model far beyond the range of data used to fit it) often produces unreliable predictions. This problem is not specific to Pearson correlation. All models suffer from some generalizability concern, which might raise the concern of omitted-variable bias (e.g., individuals possessing values beyond the observed range of values might systematically differ from individuals currently in the sample).</li>
</ol>
</div>
</div>
<div id="simple-linear-regression" class="section level2">
<h2><span class="header-section-number">5.3</span> Simple Linear Regression</h2>
<div id="independent-variable-from-categorical-to-continuous" class="section level3">
<h3><span class="header-section-number">5.3.1</span> Independent Variable: From Categorical to Continuous</h3>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mid1cat =<span class="st"> </span><span class="kw">cut</span>(xdat$mid1, <span class="dt">breaks=</span><span class="kw">quantile</span>(xdat$mid1), <span class="dt">include.lowest=</span>T)
xdat =<span class="st"> </span><span class="kw">data.frame</span>(xdat, mid1cat)
<span class="kw">boxplot</span>(mid3 ~<span class="st"> </span>mid1cat, <span class="dt">data=</span>xdat, 
        <span class="dt">main=</span><span class="st">&quot;Boxplot of Exam Results&quot;</span>, 
        <span class="dt">xlab=</span><span class="st">&quot;First Midterm Score Category&quot;</span>,
        <span class="dt">ylab=</span><span class="st">&quot;Final Exam Score&quot;</span>)
<span class="kw">abline</span>(<span class="dt">h=</span><span class="kw">mean</span>(xdat$mid3,<span class="dt">na.rm=</span>T), <span class="dt">lty=</span><span class="dv">2</span>)</code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-20-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(mid3 ~<span class="st"> </span>mid1, <span class="dt">data=</span>xdat, 
     <span class="dt">main=</span><span class="st">&quot;Scatterplot of Exam Results&quot;</span>, 
     <span class="dt">xlab=</span><span class="st">&quot;First Midterm Score&quot;</span>, 
     <span class="dt">ylab=</span><span class="st">&quot;Final Exam Score&quot;</span>)
mod =<span class="st"> </span><span class="kw">lm</span>(mid3 ~<span class="st"> </span>mid1, <span class="dt">data=</span>xdat)
<span class="kw">abline</span>(<span class="dt">h=</span><span class="kw">mean</span>(xdat$mid3,<span class="dt">na.rm=</span>T), <span class="dt">lty=</span><span class="dv">2</span>)
<span class="kw">abline</span>(<span class="kw">coef</span>(mod)[<span class="dv">1</span>], <span class="kw">coef</span>(mod)[<span class="dv">2</span>], <span class="dt">col=</span><span class="st">&quot;red&quot;</span>)</code></pre></div>
<p><img src="_main_files/figure-html/simple_linear_regression-1.png" width="672" /></p>
</div>
<div id="fitting-a-straight-line-to-data" class="section level3">
<h3><span class="header-section-number">5.3.2</span> Fitting a Straight Line to Data</h3>
<p>Let us fit a straight line with an intercept and a slope. As a result, the conditional expectation of Y is now a function of X and the prediction model with unknown <strong><em>population parameters</em></strong> can be expressed as follows. To note, “the conditional expectation of Y” is just a fancy way of saying “the predicted values of Y” or “the prediction model of Y”. They are all different ways of saying the same thing.</p>
<p><span class="math display">\[ E(Y|X) = \beta_0 + \beta_1X \]</span></p>
<p>Accordingly, the predition model of Y with <strong><em>sample parameters</em></strong> is:</p>
<p><span class="math display">\[ \hat{E}(Y_i|X_i) = \hat{\beta}_0 + \hat{\beta}_1X_i \]</span></p>
<p>In practice, we often use simplified notations to represent the preceding formula.</p>
<p><span class="math display">\[ \hat{Y}_i = b_0 + b_1X_i \]</span></p>
<p>where <span class="math inline">\(\hat{Y}_i \equiv \hat{E}(Y_i|X_i)\)</span>, <span class="math inline">\(b_0 \equiv \hat{\beta}_0\)</span> and <span class="math inline">\(b_1 \equiv \hat{\beta}_1\)</span>. This is the formula for the deterministic part of the linear regression. By adding the stochastic part, we have</p>
<p><span class="math display">\[ Y_i = \hat{Y}_i + \hat{R}_i = b_0 + b_1X_i + e_i \]</span></p>
<p>We can describe the previous simple linear model as the <strong><em>regression of Y on X</em></strong>. At this moment, this expression might be exceedingly clear to you, since the use of Y and X makes it abundently clear which one is the DV and which one is the IV. In a real case, however, one might be confused over which is which. For example, with the expression <em>“regressing income on education”</em>, one must know that income is Y and education is X.</p>
</div>
<div id="the-least-squares-method" class="section level3">
<h3><span class="header-section-number">5.3.3</span> The Least-Squares Method</h3>
<p>The least-squares method is one of the ways to solve the above equation for the unknown parameters <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span>. The official name for this method is <strong>ordinary least squares (OLS)</strong> method.</p>
<p><span class="math display">\[ SSR = \sum_{i=1}^n R_i^2 = \sum_{i=1}^n (Y_i-\hat{Y}_i)^2 = \sum_{i=1}^n (Y_i - (b_0 + b_1X_i))^2 \]</span></p>
<p>The idea is that we would find <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span> such that <span class="math inline">\(SSR\)</span> would reach its minimum. Remember, the last time we use this idea, we end up finding the mean.</p>
<p><span class="math display">\[ SSR = \sum_{i=1}^n (Y_i-c)^2 \]</span> Solving for <span class="math inline">\(c\)</span> that minimizes <span class="math inline">\(SSR\)</span> gives <span class="math inline">\(c = \bar{Y}\)</span>. Applying the idea above, we would eventually find the following solutions.</p>
<p><span class="math display">\[ b_1 = \frac{\sum(X_i-\bar{X})(Y_i-\bar{Y})} {\sum(X_i-\bar{X})^2} \]</span> and</p>
<p><span class="math display">\[ b_0 = \bar{Y} - b_1 \bar{X} \]</span></p>
</div>
<div id="interpretations-of-regression-coefficients" class="section level3">
<h3><span class="header-section-number">5.3.4</span> Interpretations of Regression Coefficients</h3>
<p><img src="_main_files/figure-html/unnamed-chunk-21-1.png" width="672" /></p>
<ul>
<li>It should be clear that <strong><span class="math inline">\(b_0\)</span></strong> is the intercept, which is interpreted as <strong>the value of Y when X = 0</strong>. To note, values of X oftentimes cannot be exactly 0, e.g., weight or IQ. In such a case, the interpretation of <span class="math inline">\(b_0\)</span> would not make intuitive sense (since no one would have 0 weight or 0 intelligence); the intercept is there only to make the math work.</li>
<li>To make the interpretation of <strong><span class="math inline">\(b_0\)</span></strong> meaningful, we can choose to center X with respect to its mean, i.e. <span class="math inline">\(X^* = X - \bar{X}\)</span>. Now <span class="math inline">\(b_0\)</span> is <strong>the value of Y when <span class="math inline">\(X = \bar{X}\)</span></strong>.</li>
<li>In contrast, <strong><span class="math inline">\(b_1\)</span></strong> is the slope, which is interpreted as <strong>the amount of change in Y when X changes by 1 unit of measurement</strong>. As a result, the value of <span class="math inline">\(b_1\)</span> would change, if the unit of measurement for Y or X or both changes.</li>
</ul>
</div>
<div id="understanding-the-estimators" class="section level3">
<h3><span class="header-section-number">5.3.5</span> Understanding the Estimators</h3>
<p>There are several ways to look at <span class="math inline">\(b_1\)</span>, each provides some insight from different perspectives.</p>
<p><span class="math display">\[ b_1 
\overset{(1)}{=} \frac { \color{blue}{ \sum(X_i-\bar{X})(Y_i-\bar{Y}) } } { \color{red}{\sum(X_i-\bar{X})^2} } 
\overset{(2)}{=} r \frac{s_Y}{s_X}
\overset{(3)}{=} \color{blue}{\sum} \left( \frac{ \color{blue}{ (X_i-\bar{X}) } }{ \color{red}{SSX} } \cdot \color{blue}{(Y_i-\bar{Y})} \right)
\overset{(4)}{=} \frac { \color{blue}{Cov(X,Y)} } { \color{red}{Var(X)} } \]</span> <em>Note</em>: Similar to <span class="math inline">\(\bar{X}\)</span>, <span class="math inline">\(SSX = (n-1)Var(X)\)</span> is treated as a constant and can therefore move in and out of the summation sign <span class="math inline">\(\sum\)</span> freely.</p>
<ol style="list-style-type: decimal">
<li>When we change the scale of <span class="math inline">\(X\)</span> to <span class="math inline">\(cX\)</span> (without loss of generality, let’s assume <span class="math inline">\(c\ge1\)</span>), the term <span class="math inline">\((X_i-\bar{X})\)</span> would change by <span class="math inline">\(c\)</span> and <span class="math inline">\((X_i-\bar{X})^2\)</span> would change by <span class="math inline">\(c^2\)</span>, and hence we would expect <span class="math inline">\(b_1\)</span> to become <span class="math inline">\(1/c \cdot b_1\)</span>. This result makes sense, since increase in the <em>scale</em> of X is canceled out by decrease in the <em>coefficient</em> of X such that their product remains the same. Similarly, when we change the scale of <span class="math inline">\(Y\)</span> to <span class="math inline">\(cY\)</span>, we would expect <span class="math inline">\(b_1\)</span> to become <span class="math inline">\(c b_1\)</span>.</li>
</ol>
<p><strong>R_Proof: Linear Transformations</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">x =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">10000</span>, <span class="dt">mean=</span><span class="dv">1</span>, <span class="dt">sd=</span><span class="dv">5</span>)
c =<span class="st"> </span><span class="dv">10</span>
cx =<span class="st"> </span>c *<span class="st"> </span>x
y =<span class="st"> </span><span class="dv">2</span>*x +<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">10000</span>, <span class="dv">0</span>, <span class="dv">5</span>)
cy =<span class="st"> </span>c *<span class="st"> </span>y
dat =<span class="st"> </span><span class="kw">data.frame</span>(x, cx, y, cy)
b1 =<span class="st"> </span><span class="kw">lm</span>( y ~<span class="st">  </span>x, <span class="dt">data=</span>dat)$coefficients[<span class="dv">2</span>]
b2 =<span class="st"> </span><span class="kw">lm</span>( y ~<span class="st"> </span>cx, <span class="dt">data=</span>dat)$coefficients[<span class="dv">2</span>]
b3 =<span class="st"> </span><span class="kw">lm</span>(cy ~<span class="st">  </span>x, <span class="dt">data=</span>dat)$coefficients[<span class="dv">2</span>]
bs =<span class="st"> </span><span class="kw">c</span>(c,b1,b2,b3)
<span class="kw">names</span>(bs) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;c&quot;</span>,<span class="st">&quot;x&quot;</span>,<span class="st">&quot;cx&quot;</span>,<span class="st">&quot;cy&quot;</span>)
bs</code></pre></div>
<pre><code>##          c          x         cx         cy 
## 10.0000000  2.0090753  0.2009075 20.0907534</code></pre>
<ol start="2" style="list-style-type: decimal">
<li>When X and Y are standardized z-scores, which entails that <span class="math inline">\(s_X=1\)</span> and <span class="math inline">\(s_Y=1\)</span>, <span class="math inline">\(b_1\)</span> becomes the Pearson correlation <span class="math inline">\(r\)</span>. <span class="math display">\[ b_1 = \color{red}{r} \cdot \frac{s_Y}{s_X} 
= \color{red}{ \frac {\sum_{i=1}^n(X_i - \bar{X})(Y_i-\bar{Y})} {s_X s_Y (n-1)} } \cdot \frac{s_Y}{s_X} 
= \frac { \sum_{i=1}^n(X_i - \bar{X})(Y_i-\bar{Y})} {(n-1)~ \color{gold}{s_X^2} }
= \frac { \sum(X_i-\bar{X})(Y_i-\bar{Y}) } { \color{blue}{\sum(X_i-\bar{X})^2} } = b_1 \]</span></li>
</ol>
<p>where</p>
<p><span class="math display">\[ \color{gold}{s_X^2} = \frac{ \color{blue}{\sum (X_i-\bar{X})^2} }{n-1}  \]</span></p>
<ol start="3" style="list-style-type: decimal">
<li>If we regard <span class="math inline">\((X_i-\bar{X})/SSX\)</span> as a weight, it is thus clear that <span class="math inline">\(X_i\)</span> that is farther away from <span class="math inline">\(\bar{X}\)</span> has stronger <em>influence</em> on <span class="math inline">\(\hat{\beta}_1\)</span>. In fact, if a singular data point has undue (i.e., overly large) influence on regression coefficients, it is regarded as an <strong>influential data point</strong>. It should be clear that outliers are mostly likely to become influential data points.</li>
</ol>
<p><span class="math display">\[ b_1 = \sum \left( \color{red}{ \frac{(X_i-\bar{X})}{SSX} } \cdot (Y_i-\bar{Y}) \right) \overset{*}{=} \sum \left( \color{red}{ \frac{(X_i-\bar{X})}{SSX} } \cdot Y_i \right)
= \sum \left( \color{red}{w_i} \cdot Y_i \right) \]</span></p>
<p><em>Note</em>: To see how <span class="math inline">\(\overset{*}{=}\)</span> is true, let <span class="math inline">\(c\)</span> be a constant and we have</p>
<p><span class="math display">\[ \sum (X_i - \bar{X})(Y_i \pm c) 
= \sum (X_i - \bar{X})Y_i \pm \sum (X_i - \bar{X})c 
= \sum (X_i - \bar{X})Y_i \pm c\sum (X_i - \bar{X}) 
= \sum (X_i - \bar{X})Y_i \pm c \cdot 0 = \sum (X_i - \bar{X})Y_i \]</span></p>
<ol start="4" style="list-style-type: decimal">
<li>Measurement error in <span class="math inline">\(X\)</span> always shrinks the magnitude of <span class="math inline">\(b_1\)</span> towards 0, whereas measurement error in <span class="math inline">\(Y\)</span> does not affect <span class="math inline">\(b_1\)</span>. To note, measurement error is regarded as random noise. To see this, let <span class="math inline">\(X^* = X+E\)</span>, where <span class="math inline">\(E\)</span> denotes random error.</li>
</ol>
<p><span class="math display">\[ Cov(X^*,Y) = Cov(X+E,~Y) = Cov(X,Y) + Cov(E,Y) = Cov(X,Y) \]</span> <span class="math display">\[ Var(X^*) = Var(X+E) = Var(X) + Var(E) + 2Cov(X,E) = Var(X) + Var(E) \]</span> <em>Note</em>: Both <span class="math inline">\(Cov(Y,E)=0\)</span> and <span class="math inline">\(Cov(X,E)=0\)</span> are true, because random noise by definition does not co-vary with anything. As a result, we have</p>
<p><span class="math display">\[ b_1^* = \frac{Cov(X^*,Y)}{Var(X^*)} = \frac{Cov(X+E,~Y)}{Var(X+E)} 
= \frac{Cov(X,Y)}{Var(X)+Var(E)} &lt; \frac{Cov(X,Y)}{Var(X)} = b_1\]</span></p>
<p><strong>R_Proof: Measurement Error</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">noise =<span class="st"> </span><span class="kw">rnorm</span>(<span class="kw">length</span>(x), <span class="dt">mean=</span><span class="dv">0</span>, <span class="dt">sd=</span><span class="kw">sd</span>(x))
nx =<span class="st"> </span>x +<span class="st"> </span>noise
noise =<span class="st"> </span><span class="kw">rnorm</span>(<span class="kw">length</span>(y), <span class="dt">mean=</span><span class="dv">0</span>, <span class="dt">sd=</span><span class="kw">sd</span>(y))
ny =<span class="st"> </span>y +<span class="st"> </span>noise
dat =<span class="st"> </span><span class="kw">data.frame</span>(dat, nx, ny)
b4 =<span class="st"> </span><span class="kw">lm</span>( y ~<span class="st"> </span>nx, <span class="dt">data=</span>dat)$coefficients[<span class="dv">2</span>]
b5 =<span class="st"> </span><span class="kw">lm</span>(ny ~<span class="st">  </span>x, <span class="dt">data=</span>dat)$coefficients[<span class="dv">2</span>]
names =<span class="st"> </span><span class="kw">names</span>(bs)
bs =<span class="st"> </span><span class="kw">c</span>(bs, b4, b5)
<span class="kw">names</span>(bs) =<span class="st"> </span><span class="kw">c</span>(names,<span class="st">&quot;noise_x&quot;</span>,<span class="st">&quot;noise_y&quot;</span>)
bs</code></pre></div>
<pre><code>##          c          x         cx         cy    noise_x    noise_y 
## 10.0000000  2.0090753  0.2009075 20.0907534  1.0048716  2.0124322</code></pre>
<p><em>Note</em>: The result from adding noise to Y changes slightly, because the <code>cor(Y,E)</code> in our sample is 0.0065184, which is not strictly 0 due to sampling randomness.</p>
</div>
<div id="residual-variance" class="section level3">
<h3><span class="header-section-number">5.3.6</span> Residual Variance</h3>
<p><strong>Residual variance</strong> (aka <strong>conditional variance</strong>) is given as follows.</p>
<p><span class="math display">\[ \hat{\sigma}^2 = MSR = \frac{SSR}{DFR} = \frac{\sum R_i^2}{df} 
= \frac{\sum_{i=1}^n (Y_i-\hat{Y_i})^2}{df} = \frac{\sum_{i=1}^n (Y_i - (b_0 + b_1X_i))^2}{n-2} \]</span></p>
<p>It is thus clear that <strong>RMSE</strong> (i.e., root of mean squares residual) is the <strong>residual standard deviation</strong>, aka <strong>conditional standard deviation</strong>.</p>
<p>Notation-wise, for the sake of simplicity, we will use <span class="math inline">\(s\)</span> to denote the residual standard deviation <span class="math inline">\(\hat{\sigma}\)</span> and hence <span class="math inline">\(s^2\)</span> to represent the residual variance <span class="math inline">\(\hat{\sigma}^2\)</span>.</p>
</div>
<div id="standard-error-of-the-slope" class="section level3">
<h3><span class="header-section-number">5.3.7</span> Standard Error of the Slope</h3>
<p>Given the residual standard deviation,</p>
<p><span class="math display">\[ \hat{\sigma} = \sqrt{\frac{\sum R_i^2}{df}}
= \sqrt{ \frac{\sum_{i=1}^n (Y_i-\hat{Y_i})^2}{n-2} } 
= \sqrt{ \frac{\sum_{i=1}^n (Y_i - (b_0 + b_1X_i))^2}{n-2} } \]</span> The standard errors of <span class="math inline">\(b_1\)</span> and <span class="math inline">\(b_0\)</span> are given as follows.</p>
<p><span class="math display">\[ \hat{\sigma}_{b_1}^2
= \frac{\hat{\sigma}^2}{\sum (X_i-\bar{X})^2}
= \frac{\hat{\sigma}^2}{\text{SSX}}
= \frac{\hat{\sigma}^2}{(n-1) Var(X)}\]</span> and <span class="math display">\[ \hat{\sigma}_{b_0}^2
= \frac{\hat{\sigma}^2}{n} \cdot \frac{\sum (X_i)^2}{\sum (X_i-\bar{X})^2}  \]</span></p>
<p>It must be emphasized that we usually do not care about inference regarding <span class="math inline">\(b_0\)</span>. The standard error of <span class="math inline">\(b_0\)</span> is provided here primarily for the sake of completeness rather than for any praticial value.</p>
<ul>
<li>It is clear that the standard error of <span class="math inline">\(b_1\)</span> is directly proportional to the <strong>residual standard deviation</strong>. In other words, if the overall fit of the line is good, the RMSE (i.e., <span class="math inline">\(\hat{\sigma}\)</span>) will be small and hence <span class="math inline">\(\hat{\sigma}_{b_1}\)</span>.</li>
<li>When <strong>n is large</strong>, <span class="math inline">\(\hat{\sigma}\)</span> would be small and <span class="math inline">\(SSX\)</span> would be large, which drives <span class="math inline">\(\hat{\sigma}_{b_1}\)</span> to become even smaller.</li>
<li>All else being equal, greater <strong>intrinsic variation in X</strong> would result in a larger <span class="math inline">\(SSX\)</span>, which in turn produces a small <span class="math inline">\(\hat{\sigma}_{b_0}\)</span>. Results from surveys using a 1-5 Likert scale is a good example. Sometimes, the wording of a question is phrased in such a way that the responses to the question would only span a restricted range (e.g., registering exclusively positive ratings). This clearly leads to reduced intrinsic variation in X.</li>
</ul>
</div>
<div id="effect-size-and-standardized-coefficients" class="section level3">
<h3><span class="header-section-number">5.3.8</span> Effect Size and Standardized Coefficients</h3>
<p>Since the magnitude of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> is subject to change with respect to the units of measurement adopted in Y and X. Therefore, it would be helpful to derive some kind of <em>standardized</em> coefficients such that sizes of the coefficients are invariant to the choice of measurement units. After standardizing both X and Y, the resultant coefficients are <strong>standardized coefficients</strong>, known as the <strong>beta coefficients</strong>. As shown previously, in simple OLS regression, standardized <span class="math inline">\(b_1\)</span> is identical to the Pearson correlation <span class="math inline">\(r\)</span>. Beta coefficients are therefore used as an <strong>effect size</strong> measure in communicating the substantive practical significance of a linear relationship.</p>
<p>Let <span class="math inline">\(s_X\)</span> and <span class="math inline">\(s_Y\)</span> be the standard deviations of X and Y, we have <span class="math display">\[ r = b_{\text{standardized}} = \frac{s_X}{s_Y} \cdot b_{\text{original}} \]</span></p>
<p>For certain problems, sometimes we would only want to standardize either X or Y, but not both of them. For example, when X is an ordinal variable measured on a 1-5 Likert scale. We might want to standardize only Y and claim that 1 point increase on the Likert scale would result in <span class="math inline">\(b_1\)</span> standard deviations of increase in Y.</p>
</div>
<div id="anova-for-regression" class="section level3">
<h3><span class="header-section-number">5.3.9</span> ANOVA for Regression</h3>
<p><code>HOWTO: how to add lines above and below the table?</code></p>
<p><code>HOWTO: how to &quot;center&quot;&quot; content in a cell</code></p>
<table style="width:31%;">
<colgroup>
<col width="5%" />
<col width="8%" />
<col width="8%" />
<col width="8%" />
</colgroup>
<thead>
<tr class="header">
<th></th>
<th align="center">SS</th>
<th align="center">DF</th>
<th align="center">MS</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math display">\[\mathbf{Total}~~\]</span></td>
<td align="center"><span class="math display">\[~~\sum(Y_i-\bar{Y})^2~~\]</span></td>
<td align="center"><span class="math display">\[~~n-1~~\]</span></td>
<td align="center"><span class="math display">\[~~\frac{1}{n-1} \sum (Y_i-\bar{Y})^2\]</span></td>
</tr>
<tr class="even">
<td><span class="math display">\[\mathbf{Model}~~\]</span></td>
<td align="center"><span class="math display">\[~~\sum(\hat{Y}_i-\bar{Y})^2~~\]</span></td>
<td align="center"><span class="math display">\[~~(1+p)-1~~\]</span></td>
<td align="center"><span class="math display">\[~~\frac{1}{p}\sum (\hat{Y}_i-\bar{Y})^2\]</span></td>
</tr>
<tr class="odd">
<td><span class="math display">\[\mathbf{Residual}~~\]</span></td>
<td align="center"><span class="math display">\[~~\sum(Y_i - \hat{Y}_i)^2~~\]</span></td>
<td align="center"><span class="math display">\[~~n-(1+p)~~\]</span></td>
<td align="center"><span class="math display">\[\frac{1}{n-1-p}\sum (Y_i-\hat{Y}_i)^2\]</span></td>
</tr>
</tbody>
</table>
<p>where <span class="math inline">\(\hat{Y}_i = b_0 + b_1X_i\)</span> and <span class="math inline">\(p\)</span> is the number of features, i.e., the number of parameters minus 1 to exclude the intercept.</p>
<p><span class="math display">\[ R^2 = \frac{\text{SSM}}{\text{SST}} = \frac{ \sum(\hat{Y}_i-\bar{Y})^2 }{ \sum(Y_i-\bar{Y})^2 } \]</span> and</p>
<p><span class="math display">\[ F = \frac{MSM}{MSE} = \frac{ \frac{1}{p}\sum (\hat{Y}_i-\bar{Y})^2 }{ \frac{1}{n-1-p}\sum (Y_i-\hat{Y}_i)^2 } \]</span> where <span class="math inline">\(F(p, n-1-p)\)</span> and <span class="math inline">\(p=1\)</span> in the case of simple linear regression.</p>
<p><code>TODO: check out p594 in IPS</code></p>
<p><strong>R_Proof: R-square and F-test</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mod =<span class="st"> </span><span class="kw">lm</span>(mid3 ~<span class="st"> </span>mid1, <span class="dt">data=</span>xdat)
yhat =<span class="st"> </span><span class="kw">predict</span>(mod)
ybar =<span class="st"> </span><span class="kw">mean</span>(xdat$mid3, <span class="dt">na.rm=</span>T)
ssm =<span class="st"> </span><span class="kw">sum</span>((yhat-ybar)^<span class="dv">2</span>, <span class="dt">na.rm=</span>T)
sst =<span class="st"> </span><span class="kw">sum</span>((xdat$mid3-ybar)^<span class="dv">2</span>, <span class="dt">na.rm=</span>T)
r2 =<span class="st"> </span>ssm/sst
r =<span class="st"> </span><span class="kw">cor</span>(xdat$mid3, xdat$mid1, <span class="dt">use=</span><span class="st">&quot;pairwise&quot;</span>)
<span class="co"># cor(na.omit(xdat$mid3), yhat)</span>
<span class="kw">c</span>(r2, r^<span class="dv">2</span>)</code></pre></div>
<pre><code>## [1] 0.5494976 0.5494976</code></pre>
</div>
</div>
<div id="non-linear-relations" class="section level2">
<h2><span class="header-section-number">5.4</span> Non-linear Relations</h2>
<div id="log-transformations" class="section level3">
<h3><span class="header-section-number">5.4.1</span> Log-transformations</h3>
<ul>
<li>log-transformation is one way for handling non-linearity if the relation between IV and DV is indeed connected by a log-based relationship.</li>
<li>log-transformation is commonly used for handling highly skewed variable by taknig advantage of the <em>suppressive</em> property of logarithm (e.g., <span class="math inline">\(\log_{10}(10)=1\)</span> and <span class="math inline">\(\log_{10}(100)=2\)</span>). As a result, many highly skewed variables, after log-transformations, would look much like a normal distribution.</li>
</ul>
<p>For simple linear regression, the linear model is represented as follows.</p>
<p><span class="math display">\[ Y = b_0 + b_1X + e \]</span></p>
<p>Take the derivative with respect to <span class="math inline">\(X\)</span> on both sides, we have</p>
<p><span class="math display">\[ \frac{dY}{dX} \approx \frac{\Delta Y}{\Delta X} = \frac{d(b_0+b_1X)}{dX} = b_1 \]</span> and hence</p>
<p><span class="math display">\[ \Delta Y = b_1 \Delta X \]</span> In this case, the interpretation derived from a <strong>Calculus</strong> perspective readily agrees with that from an <strong>arithmetic</strong> perspective, i.e., when X increases by 1 unit (i.e., <span class="math inline">\(\Delta X=1\)</span>), Y would increase by an average of <span class="math inline">\(b_1\)</span> (i.e., <span class="math inline">\(\Delta Y = b_1\)</span>). This situation would not be true under log-transformations.</p>
<div id="linear-log-model-log-transformation-on-x" class="section level4">
<h4><span class="header-section-number">5.4.1.1</span> Linear-log Model: Log-transformation on X</h4>
<p>Now, let’s consider log-transforming X. The linear model would become <span class="math display">\[ Y = b_0 + b_1 \log(X) + e \]</span> To note, the <code>log</code> notation in this textbook would always denote the natural logarithm with base <span class="math inline">\(e \approx 2.718\)</span>, which in some textbooks is represented by <code>ln</code>.</p>
<p>Simple rules of <strong>arithmetic</strong> dictate that <span class="math inline">\(b_1\)</span> is the expected units of change in Y given a one-unit increase in log(X), which means</p>
<p><span class="math display">\[ \log(X) + 1 = \log(X) + \log(e) = \log(eX)  \]</span></p>
<p>This result makes it clear that adding 1 to log(X) is equivalent of multipling X by <span class="math inline">\(e \approx 2.718\)</span>. In other words, Y is expected to change by <span class="math inline">\(b_1\)</span> units when X increases by approximately 1.72 times or 172%.</p>
<p>Let’s look at this from a <strong>Calculus</strong> perspective by taking the derivative with respect to X on the both sides.</p>
<p><span class="math display">\[ \frac{dY}{dX} \approx \frac{\Delta Y}{\Delta X} = \frac{d(b_0+b_1 \log(X))}{dX} = b_1 \cdot \frac{1}{X} \]</span> and hence</p>
<p><span class="math display">\[ \Delta Y = b_1 \cdot \frac{\Delta X}{X} 
= \frac{b_1}{100} \cdot \left( 100 \cdot \frac{\Delta X}{X} \right) 
= \frac{b_1}{100} \cdot p \]</span></p>
<p>where <span class="math inline">\(\Delta X/X\)</span> represents the <strong><em>proportion of change</em></strong> in X and hence <span class="math inline">\(100 \cdot \Delta X/X\)</span> denotes the <strong><em>percentage of change</em></strong> in X. In other words, 1% increase in X, namely <span class="math inline">\((100 \cdot \Delta X / X) = 1\)</span>, would translate into <span class="math inline">\(b_1/100\)</span> units of increase in <span class="math inline">\(Y\)</span> or one hundredth of Y. To note, this Calculus-inspired percentage interpretation is only true (i.e., the <span class="math inline">\(\approx\)</span> sign would hold), if <span class="math inline">\(\Delta X\)</span> is small (e.g., 1%).</p>
<p>Now let’s re-examine this interpretation from the <strong>arithmetic</strong> perspective, where a <em>p</em> percent increase in X would be <span class="math inline">\(X \cdot (100+p)/100 = X \cdot (1+p/100)\)</span>. Let <span class="math inline">\(Y_0\)</span> and <span class="math inline">\(Y_p\)</span> be the values of Y corresponding to X and X with <em>p</em> percent increase.</p>
<p><span class="math display">\[ Y_0 = b_0 + b_1 \log(X) + e \]</span> and <span class="math display">\[ Y_p = b_0 + b_1 \log \left( X \cdot \frac{100+p}{100} \right) + e \]</span> and</p>
<p><span class="math display">\[ \frac{\Delta X}{X} 
= \frac{X \cdot (1+p/100) - X}{X} 
= \frac{p}{100} \]</span> or</p>
<p><span class="math display">\[ 100 \cdot \frac{\Delta X}{X} = p \]</span></p>
<p>Hence, a <em>p</em> percent increase in X would result in</p>
<p><span class="math display">\[ \Delta Y = Y_p - Y_0 
= b_1 \left( \log \left( (1+\frac{p}{100}) \cdot X \right) - \log(X) \right) 
= b_1 \log \left( 1 + \frac{p}{100} \right) \]</span></p>
<p>This result indicates that <em>p</em> = 1 or 1% increase in X would result in <span class="math inline">\(b_1 \log(1.01) \approx b_1 \cdot 0.01\)</span>. This interpretation is scalable up to about 10% (i.e., the exact <strong>arithmetic</strong> and the approximate <strong>Calculus</strong> interpretations agree only up to about 10%), since <em>p</em> = 10 or 10% increase in X would result in <span class="math inline">\(b_1 \log(1.10) \approx b_1 \cdot 0.10\)</span>. The following table shows the degree of deviations under and beyond 10% change in X.</p>
<table>
<thead>
<tr class="header">
<th align="center">proportion increase in X</th>
<th align="center">percentage increase in X</th>
<th align="center">increase in Y</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><span class="math inline">\(\Delta X/X = p/100\)</span></td>
<td align="center"><span class="math inline">\(100 \cdot \Delta X/X = p\)</span></td>
<td align="center"><span class="math inline">\(b_1 \log(1+p/100)\)</span></td>
</tr>
<tr class="even">
<td align="center">0.01</td>
<td align="center">1%</td>
<td align="center"><span class="math inline">\(b_1 \cdot 0.01\)</span></td>
</tr>
<tr class="odd">
<td align="center">0.05</td>
<td align="center">5%</td>
<td align="center"><span class="math inline">\(b_1 \cdot 0.05\)</span></td>
</tr>
<tr class="even">
<td align="center">0.10</td>
<td align="center">10%</td>
<td align="center"><span class="math inline">\(b_1 \cdot 0.10\)</span></td>
</tr>
<tr class="odd">
<td align="center">0.20</td>
<td align="center">20%</td>
<td align="center"><span class="math inline">\(b_1 \cdot 0.18\)</span></td>
</tr>
<tr class="even">
<td align="center">0.50</td>
<td align="center">50%</td>
<td align="center"><span class="math inline">\(b_1 \cdot 0.41\)</span></td>
</tr>
<tr class="odd">
<td align="center">1.00</td>
<td align="center">100%</td>
<td align="center"><span class="math inline">\(b_1 \cdot 0.69\)</span></td>
</tr>
<tr class="even">
<td align="center">1.72</td>
<td align="center">172%</td>
<td align="center"><span class="math inline">\(b_1 \cdot 1.00\)</span></td>
</tr>
</tbody>
</table>
<p><code>TODO: add a shiny app to supplement the table!</code></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">curve</span>(log, <span class="dt">from=</span><span class="fl">0.01</span>, <span class="dt">to=</span><span class="dv">4</span>, <span class="dt">ylim=</span><span class="kw">c</span>(-<span class="dv">4</span>,<span class="dv">4</span>))
<span class="kw">curve</span>(x<span class="dv">-1</span>, <span class="dt">add=</span>T, <span class="dt">col=</span><span class="dv">2</span>)
<span class="kw">abline</span>(<span class="dt">v=</span><span class="dv">0</span>, <span class="dt">h=</span><span class="dv">0</span>, <span class="dt">lty=</span><span class="dv">2</span>)</code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-22-1.png" width="672" /></p>
<p>In summary, given the discussions above, the following statements are equivalent (or almost equivalent).</p>
<ul>
<li>When log(X) increases by 1 unit, Y would change by an average of <span class="math inline">\(b_1\)</span>.</li>
<li>When X is multiplied by 2.72, Y would change by an average of <span class="math inline">\(b_1\)</span>.</li>
<li>When X increases by 172%, Y would change by an average of <span class="math inline">\(b_1\)</span>.</li>
<li>When X increases by p%, Y would change by an average of <span class="math inline">\(b_1 \log(1+p/100)\)</span>.</li>
<li>When X increases by 1%, Y would change by an average of <span class="math inline">\(b_1/100\)</span>.</li>
<li>When X increases by 10%, Y would change by an average of <span class="math inline">\(b_1/10\)</span>.</li>
</ul>
</div>
<div id="log-linear-model-log-transformation-on-y" class="section level4">
<h4><span class="header-section-number">5.4.1.2</span> Log-linear Model: Log-transformation on Y</h4>
<p><span class="math display">\[ \log(Y) = b_0 + b_1X + e \]</span></p>
<p>Take the derivative with respect to X on both sides, we have</p>
<p><span class="math display">\[ \frac{1}{Y} \cdot \frac{dY}{dX} \approx \frac{1}{Y} \cdot \frac{\Delta Y}{\Delta X} = b_1 \]</span> and hence</p>
<p><span class="math display">\[ \frac{\Delta Y}{Y} = b_1 \cdot \Delta X \]</span></p>
<p>Replacing <strong><em>proportion of change</em></strong> by <strong><em>percentage of change</em></strong> gives</p>
<p><span class="math display">\[ 100 \cdot \frac{\Delta Y}{Y} = 100 \cdot b_1 \cdot \Delta X \]</span> which leads to the interpretation that 1 unit increase in X would result in <span class="math inline">\(100 \cdot b_1\)</span> percent increase in Y. Similar to the previous case, this <strong>Calculus</strong> interpretation is only true when <span class="math inline">\(b_1\)</span> is less than 0.1 or 10%.</p>
<p>Let’s examine the precise <strong>arithmetic</strong> interpretation. From <span class="math inline">\(\log(Y) = b_0 + b_1X + e\)</span>, we have</p>
<p><span class="math display">\[ Y = e^{(b_0 + b_1X + e)} \]</span></p>
<p>Let <span class="math inline">\(Y_0\)</span>, <span class="math inline">\(Y_1\)</span>, and <span class="math inline">\(Y_p\)</span> be the values of Y corresponding to X, X+1, and X+p. We have</p>
<p><span class="math display">\[ Y_0 = e^{(b_0 + b_1X + e)} \]</span> and</p>
<p><span class="math display">\[ Y_1 = e^{(b_0 + b_1(X+1) + e)} = e^{(b_0 + b_1X + e)} \cdot e^{b_1} = Y_0 \cdot e^{b_1} \]</span> which implies that 1 unit increase in X would result in Y multiplied by <span class="math inline">\(e^{b_1}\)</span>.</p>
<p><span class="math display">\[ \frac{\Delta Y}{Y_0} = \frac{Y_1 - Y_0}{Y_0} 
= \frac{Y_0 \cdot e^{b_1} - Y_0}{Y_0} 
= \frac{Y_0 \cdot (e^{b_1} - 1)}{Y_0} 
= (e^{b_1}-1) \]</span></p>
<p>Replacing <strong><em>proportion of change</em></strong> by <strong><em>percentage of change</em></strong> gives</p>
<p><span class="math display">\[ 100 \cdot \frac{\Delta Y}{Y_0} = 100 \cdot (e^{b_1}-1) \]</span></p>
<p>The <strong>Taylor expansion</strong> of <span class="math inline">\(e^b\)</span> gives the following result <span class="math display">\[ e^b = 1 + b + \frac{b^2}{2!} + \frac{b^3}{3!} + \cdots+\frac{b^n}{n!} \]</span> When b is small (e.g. <span class="math inline">\(b \le 0.1\)</span>), <span class="math inline">\(e^b \approx 1 + b\)</span> and hence <span class="math inline">\(e^b - 1 \approx b\)</span>. In other words, when <span class="math inline">\(b_1\)</span> is less than 0.1 (as suggested by the table below), the exact <strong>arithmetic</strong> interpretation agrees with the approximate <strong>Calculus</strong> interpretation.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">b =<span class="st"> </span><span class="kw">c</span>(<span class="kw">c</span>(<span class="fl">0.01</span>,<span class="fl">0.05</span>,<span class="fl">0.1</span>,<span class="fl">0.15</span>), <span class="kw">seq</span>(<span class="fl">0.2</span>, <span class="dv">1</span>, <span class="dt">by=</span><span class="fl">0.1</span>))
p =<span class="st"> </span><span class="kw">round</span>(<span class="kw">exp</span>(b)-<span class="dv">1</span>, <span class="dv">2</span>)
mat =<span class="st"> </span><span class="kw">cbind</span>(b,p)
<span class="kw">colnames</span>(mat) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;b&quot;</span>,<span class="st">&quot;exp(b)-1&quot;</span>)
mat</code></pre></div>
<pre><code>##          b exp(b)-1
##  [1,] 0.01     0.01
##  [2,] 0.05     0.05
##  [3,] 0.10     0.11
##  [4,] 0.15     0.16
##  [5,] 0.20     0.22
##  [6,] 0.30     0.35
##  [7,] 0.40     0.49
##  [8,] 0.50     0.65
##  [9,] 0.60     0.82
## [10,] 0.70     1.01
## [11,] 0.80     1.23
## [12,] 0.90     1.46
## [13,] 1.00     1.72</code></pre>
<p>In summary,</p>
<ul>
<li>When X increases by 1, log(Y) would increase by <span class="math inline">\(b_1\)</span>.</li>
<li>When X increases by 1, Y would be multiplied by <span class="math inline">\(e^{b_1}\)</span>.</li>
<li>When X increases by c, Y would be multiplied by <span class="math inline">\(e^{c b_1}\)</span>.</li>
<li>When X increases by 1, Y would increase by <span class="math inline">\(100 \cdot (e^{b_1}-1)\)</span> percent.</li>
<li>When X increases by 1 and when <span class="math inline">\(b_1 \le 0.1\)</span>, Y would increase by <span class="math inline">\(100 \cdot b_1\)</span> percent.</li>
</ul>
</div>
<div id="log-log-model-log-transformations-on-x-and-y" class="section level4">
<h4><span class="header-section-number">5.4.1.3</span> Log-log Model: log-transformations on X and Y</h4>
<p><span class="math display">\[ \log(Y) = b_0 + b_1 \log(X) + e \]</span></p>
<p>Taking the derivative with respect to X on both sides gives</p>
<p><span class="math display">\[ \frac{1}{Y} \cdot \frac{dY}{dX} 
\approx \frac{1}{Y} \cdot \frac{\Delta Y}{\Delta X} = \frac{b_1}{X} \]</span></p>
<p>and hence</p>
<p><span class="math display">\[ 100 \cdot \frac{\Delta Y}{Y} = b_1 \cdot \left( 100 \cdot \frac{\Delta X}{X} \right) \]</span> which implies that 1 percent increase in X would result in <span class="math inline">\(b_1\)</span> percent increase in Y.</p>
<p>With the log-log model, the coefficient <span class="math inline">\(b_1\)</span> is called <strong>elasticity</strong> in economics.</p>
<p>In summary,</p>
<ul>
<li>When log(X) increase by 1, log(Y) would increase by <span class="math inline">\(b_1\)</span>.</li>
<li>When X is multiplied by <span class="math inline">\(e\)</span>, the expected value of Y would increase by multiplying <span class="math inline">\(e^{b_1}\)</span>.</li>
<li>When X increases by <em>p</em> percent, the expected value of Y would increase by multiplying <span class="math inline">\(e^{c b_1}\)</span> where <span class="math inline">\(c = \log(1+p/100)\)</span>.</li>
</ul>
</div>
</div>
<div id="quadratic-relations" class="section level3">
<h3><span class="header-section-number">5.4.2</span> Quadratic Relations</h3>
</div>
</div>
<div id="the-assumptions-of-simple-linear-regression" class="section level2">
<h2><span class="header-section-number">5.5</span> The Assumptions of Simple Linear Regression</h2>
<p>When the OLS assumptions hold, OLS estimators have been shown to be unbiased and have minimum standard error of among all unbiased linear estimators, a property known as <strong>Best Linear Unbiased Estimators (BLUE)</strong>.</p>
<p><span class="math display">\[ Y_i|X_i \overset{id}{\sim} N(\beta_0 + \beta_1X_i, \sigma^2) \]</span></p>
</div>
<div id="assumption-diagnostics" class="section level2">
<h2><span class="header-section-number">5.6</span> Assumption Diagnostics</h2>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="common-statistical-tests.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="multiple-linear-regression.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
