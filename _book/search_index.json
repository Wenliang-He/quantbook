[
["index.html", "Quantitative Research Methods for Education 1 Introduction 1.1 Software Installation 1.2 Introduction to R 1.3 Introduction to R Markdown 1.4 Introduction to Git &amp; GitHub 1.5 Introduction to ggplot2 1.6 Introduction to Shiny", " Quantitative Research Methods for Education Wenliang He Updated on 2018-09-10 1 Introduction Before diving into Statistics, you need to install some software, notably R and RStudio, and register for some accounts. 1.1 Software Installation 1.1.1 Install R R is a free software environment for statistical computing and graphics. CRAN, Comprehensive R Archive Network, is a network of web servers around the world that store identical, up-to-date versions of various code for R. Choose a CRAN mirror that is close to you. scroll down the list to your country (e.g. China) choose a link hosted by an institution that is close to you Warning: If you are not using a mirror, but visiting the official link (https://cran.r-project.org/) to download R. The downloading speed can be painfully slow. If so, consider using a mirror. Choose the link corresponding to your operating system. Mac Users scroll down to the Lastest release section click to download the lastest version that looks similar to R-3.5.1.pkg Windows Users click on the link install R for the first time download the lastest version of R that looks similar to R-3.5.1 as shown below Note: The R version shown below does not distinguish between 32-bit and 64-bit Windows, since two versions are combined into one file to simplify the download. But you would still need to choose one during the installation process (see details below). Install R Warning: We highly recommend you to use English as the default language for your R. You will search for help on the internet at some point. It is simply easier to communicate when help seekers and help givers are speaking the same language. Warning: We highly recommend you to use the default directory (默认路径) for installation. Third-party packages will look for R. We do not want to make it too hard for them. For Mac users, the installation process should be straightforward. For Windows users, you need to find out whether your Windows is a 32-bit or 64-bit system and install the one identical to your system. To find out which version is your Windows, right-click on 我的电脑 -&gt; 属性 -&gt; 系统类型. This is where you can find the information. 1.1.2 Install RStudio RStudio is an IDE (Integrated Development Environment), a powerful user interface development specifically for R. For individual use, use can download the free open source edition of RStudio. Note: You can choose to install a Preview release of RStudio. The preview release has more features than standard release. However, the downloading speed can be painfully slow. In the following, we will demonstrate installation of the standard release. Go to the official website for download and click on the DOWNLOAD button for RStudio Desktop under the Open Source License, which is completely free. Choose the link corresponding to your operating system. Install RStudio Run the downloaded program and use the default directory for installation. The rest of the installation process should be straightfoward. Open RStudio The first time you open Rstudio, you might encounter the following error message, which suggests that you are not authorized to run the program. To address it, right-click on rstudio.exe -&gt; 属性 -&gt; 兼容性 -&gt; 以管理员身份运行. Change Settings By default, RStudio is using a CRAN mirror hosted in the US. If you are outside the US (e.g. in China), you may have trouble fetching data/code into R. The solution is to use a local CRAN mirror. after openning RStudio, find Tools -&gt; Global Options -&gt; Packages under CRAN mirror, click Change... scroll down the list and select the one mirror that is close to you click OK to commit the change 1.1.3 Install R Packages R is an extensible system and many people share useful code they have developed as a package via CRAN and/or GitHub. 1.1.3.1 Install from CRAN This is the most commonly used method to install R packages directly from CRAN. In the following, we will install the ggplot2 package and use it as a running example. In the R console, you type install.packages(&quot;ggplot2&quot;) Note, the input must be a string as enclosed by double quotes &quot;&quot;. This is usually good enough. But to be extra-careful, the dependencies=TRUE argument is often added to explicitly tell R to install any additional packages that ggplot2 requires to run its functions. install.packages(&quot;ggplot2&quot;, dependencies = TRUE) By default, dependencies = NA means c(&quot;Depends&quot;, &quot;Imports&quot;, &quot;LinkingTo&quot;). Setting dependencies = TRUE adds &quot;Suggests&quot; to the previous list, which tells R to install all the packages needed to run a package, their examples, tests and vignettes. For more on managing R packages, please refer to the R Installation and Administration Manual. To install multiple packages, e.g., pck1 and pck2, use install.packages(c(&quot;pkg1&quot;, &quot;pkg2&quot;), dependencies = TRUE) Practice: Install the following packages that will be required later in the course. ggplot2: A good alternative to R’s base graphing packages. plotly: A tool that enhances ggplot2 by translating static plots into interactive graphs dygraphs: A graphing tool specifically designed for charting time-series data in R DiagrammeR: A tool for creating diagrams and flowcharts using Graphviz and Mermaid rmarkdown: bookdown: A tool for combining multiple R markdown files into a book shiny: An R package to build interactive web apps shinythemes: More themes for shiny apps DT: A tool to create dynamic tables dplyr: A package to perform data manipulation The function defined below would (a) check if required packages exist, (b) install, if not already installed, and (c) load the packages. force_load &lt;- function(pkgs, use_require=FALSE) { # https://gist.github.com/smithdanielle/9913897 # https://cran.r-project.org/web/packages/install.load/README.html new_pkgs &lt;- pkgs[!(pkgs %in% installed.packages()[, &quot;Package&quot;])] if (length(new_pkgs)) install.packages(new_pkgs, dependencies = TRUE) if (use_require) { loaded = sapply(pkgs, require, character.only = TRUE) } else { loaded = sapply(pkgs, library, character.only = TRUE) } return(loaded) } packages = c(&quot;ggplot2&quot;, &quot;ggthemes&quot;, &quot;rmarkdown&quot;, &quot;bookdown&quot;, &quot;shiny&quot;, &quot;Hmisc&quot;) loaded = force_load(packages) loaded 1.1.3.2 Install from Source Code Occasionally, you might encounter an error message telling you that the package to be installed requires an R version greater than the one you currently have. You can, of course, choose to re-install R, which might necessitate re-installing all the other packages that are previously installed. What a hassle! Alternatively, you can choose to download the source code for the package and install from the source code directly. In any search engine (e.g. www.baidu.com), search for CRAN package ggplot2. This should lead you to the ggplot2 package hosted on CRAN. Scroll down to the Downloads section, next to Package source, click on the link to the file that ends with tar.gz. Save it under a local directory where you can easily find it. In R console, type the following install.packages(&quot;download_path/ggplot2_3.0.0.tar.gz&quot;, repos = NULL, type = &quot;source&quot;) where download_path is the directory that stores the downloaded source file. 1.1.3.3 Commonly Encountered Problems While installing packages, one type of error messages has the following pattern. Warning in install.packages : error message The specific error message you might encounter can be very different from time to time. Some examples are as follows. Warning in install.packages : InternetOpenUrl failed: &#39;&#39; Warning in install.packages : package ‘packagename’ is not available (for R version 3.5.0) Warning in install.packages : unable to access index for repository https://cran.rstudio.com/src/contrib: cannot open URL &#39;https://cran.rstudio.com/src/contrib/PACKAGES&#39; Installing package into ‘C:/Users/YourUserName/Documents/R/win-library/3.5’ (as ‘lib’ is unspecified) Warning in install.packages : unable to access index for repository https://cran.rstudio.com/src/contrib: cannot open URL &#39;https://cran.rstudio.com/src/contrib/PACKAGES&#39; In your R console, type getOption(&quot;repos&quot;). If you get the following response, it means that you fail to set a local CRAN mirror. See the Change Settings section under Install RStudio for the solution. CRAN &quot;https://cran.rstudio.com/&quot; attr(,&quot;RStudio&quot;) [1] TRUE 1.1.4 Update R and R Packages Update R The CRAN solution is not particularly good. Update R Packages update.packages(ask = FALSE, checkBuilt = TRUE) Note: This will only update packages directly installed from CRAN. So if you use a package that lives only on GitHub, you will need to update it manually, e.g. via devtools::install_github(). 1.1.5 Register a GitHub account Choose a GitHub Username Choosing a proper GitHub username is extremely important. Given the Username you choose, your GitHub account will look like https://github.com/Username. This “website” will be the one place to host and showcase all of your coursework. In the information age, when online presence is more important than ever, this is your online portfolio, your “voice”, your professional “business card”. You can change your username later, but it’s better to get it right the first time. https://help.github.com/articles/changing-your-github-username/ https://help.github.com/articles/what-happens-when-i-change-my-username/ Register a GitHub Account Go to GitHub and register for an account. By default, a regular GitHub account offers only public repositories that are visible and accessible to the general public. To have private repositories, you needs to upgrade to a paid account. Here is the good news: For students, faculty, and research staff with .edu email accounts, you can upgrade to a GitHub Education account that offers free unlimited private repositories. 1.1.6 1.2 Introduction to R 1.3 Introduction to R Markdown 1.4 Introduction to Git &amp; GitHub 1.5 Introduction to ggplot2 1.6 Introduction to Shiny "],
["the-fundamentals-of-statistics.html", "2 The Fundamentals of Statistics 2.1 Why Studying Statistics 2.2 The Big Picture 2.3 The Fundamentals of Statistics", " 2 The Fundamentals of Statistics 2.1 Why Studying Statistics teaching, research, practicum a prerequisite for more courses a prerequisite for research a prerequisite for working on practical projects summer internships diversified career prospects data analyst statistician machine learning scientist data scientist K-12 content-wise education revolution teach useful things study-load reduction happy learning don’t teach unless you’ve used it preparation for studying abroad open your eyes and see for yourself leave the door open (just in case) understanding and changing the world understanding precedes effecting changes description leads to understanding changing the world starts with getting accurate descriptions of it e.g., regular expression or exorcism 2.1.1 Understanding the World Subject nature human society Methodology the scientific method: observe-hypothesize-verify Tools languages natural languages mathematical languages, e.g., Calculus, Probability &amp; Statistics computer langages concepts &amp; models Equipped with the scientific method, social sciences have eventually chipped off the humanities and have evolved into stand-alone disciplines in their own right. Statistics plays a central role in the process. If Calculus is the mathematical language for describing the natural world, Statistics is the language for describing human societies. In college, you should not only learn the knowledge, but also the methods and tools begetting the knowledge. 2.1.2 Science vs. Philosophy 2.2 The Big Picture 2.2.1 Central Themes in Statistics Deterministric vs. Stochastic goal: understand your fate e.g., Gaokao score on mathematics goal: reduce randomness to increase the deterministric part e.g., use “School Status” or “IQ” to predict your fate Signal vs. Noise question: do I only need to care about fate and disregard luck completely? e.g., results of two experimental setups goal: construct a ratio to consider fate and luck simultaneously 2.3 The Fundamentals of Statistics 2.3.1 Storing Data in Tables There are many ways to store data (e.g., csv files, SQL files, json files, XML files). For data analysis in particular, the predominant method is to store data in a tabular form. rows vs. columns (common sense &amp; linear algebra) cases vs. variables (statistics) examples vs. features (machine learning) a specific data point is called an entry, an element, a field The following is the first rows of the famous iris dataset. 2.3.2 Variable Types Three major data types: nominal variable &lt; ordinal variable &lt; interval variable Variable Type Type Treatment Examples dummy categorical dummy gender nominal categorical dummy city ordinal categorical nominal or continuous education level interval continuous continuous temperature in Celsius ratio continuous continuous temperature in Kelvin For categorical variables, i.e., nominal and ordinal variables, each unique value is called a level. dummy variable: A dummy variable, a.k.a. dichotomous or indicator variable, is a special kind of normal variable with only two levels, e.g., gender (male vs. female). nominal variable: In practical data analysis, a nominal variable is converted into a set of dummy variables. ordinal variable: An ordinal variable is treated either as a nominal or a continuous variable. interval variable: An interval variable is a measurement where the difference between two values is meaningful. ratio variable: A interval variable is a special kind of ordinal variable, where 0 actually means zero or none or nothing. Eventually, the resulting data matrix is composed of dummies and continuous variables only. Categorical variables are also called discrete variables. Continuous variables, i.e., interval and ratio variables, are called quantitative variables. 2.3.3 Exploratory Data Analysis: Checking Distributions Variable Type Summary Statistics Graphs categorical variable frequency table bargraphpie chart categorical variable center mean median spread variance standard deviation range IQR shape modality (unimodal vs. multi-modal) skewedness (symmetric vs. skewed) kurtosis histogramdensity curvebox plot 2.3.3.1 Statistic vs. Statistics In the context of statistics, a statistic is a summary of some aspect of a sample of data. In daily usage, a statistic is an event or person regarded as no more than a piece of data. The plural form of statistic is statistics. So the study of statistics is statistics. Why do we need any statistics? Because we need to summarize a load of data efficiently. 2.3.3.2 Center: Mean vs. Median \\[ m = \\frac{1}{n} \\sum_{i=1}^n x_i\\] By convention, we usually use \\(m\\) to represent sample mean. Don’t mind the word sample for now; focus on the word mean. # Let&#39;s create some data samp1 = c(1, 2, 5, 8, 10) samp2 = c(1, 2, 5, 8, 100) samp3 = c(1, 2, 5, 8, 10, 100) # compute the means c(mean(samp1), mean(samp2), mean(samp3)) ## [1] 5.2 23.2 21.0 # compute the medians c(median(samp1), median(samp2), median(samp3)) ## [1] 5.0 5.0 6.5 mean easy to compute but sensitive to outliers has nice mathematical properties median harder to compute insensitive to outliers has nice mathematical properties of a different kind 2.3.3.3 Spread: Variance &amp; Standard Deviations If the average of \\(x_i\\) is the mean, then what is the aveage of \\((x_i-m)\\) and \\((x_i-m)^2\\)? \\(E(X) = \\bar{X}\\) \\(E(X - \\bar{X}) = E(X) - E(\\bar{X}) = \\bar{X} - \\bar{X} = 0\\) On average, if there is a data point at some distance away from the mean to the right, there is as if another data point of the same distance from the mean lying on the left. Image a seesaw, where data points are of the same weights but lying at different positions specified by their values. Mean is the balancing point. mean_balance Since mean is sensitive to outliers and median is not, logic dictates that mean_sensitive Let \\(E(X-c)^2 = Q1\\) where \\(c\\) is a constant. Q1 will reach its smallest possible value when \\(c\\) equals the mean. In other words, mean is the value that makes the average of the squared distances the smallest. Let \\(E(|X-c|) = Q2\\) where \\(c\\) is a constant. Q2 will reach its smallest possible value when \\(c\\) equals the median In other words, median is the value that makes the average of the absolute distances the smallest. Questions: Is variance sensitive to outliers? 2.3.3.4 Spread: Quartiles and Percentiles Since a quarter is 1/4 or 25%, The first quartiles is also called the 25th percentile The second quartiles is also called the 50th percentile (a.k.a. median) The third quartiles is also called the 75th percentile IQR = interquartile range = 3rd quartile - 1st quartile Five-number Summary is (1) the minimum, (2) 1st quartiles, (3) median, (4) 3rd quartile, and (5) the maximum NOTE: In R, quantile() is the command for getting percentiles. But why does R use the word quantile instead percentile? I would really hate to say that percentile and quantile mean exactly the same thing in HERE. However, they are two completely different concepts when used in the context of probability functions, which we will discuss below with normal distributions. That being said, quantile is a more apt term in that context, and R are using the two terms consistently. But other sources, e.g. textbooks, might use them interchangably. As a result, you will have to rely on context to decide whether they mean the same thing or not. 2.3.3.5 Spread: Inner Fences, Outer Fences, and Outliers inner fences span between 1.5 times the IQR above the 3rd quartile and below the 1st quartile. Data points lying beyond the inner fences are regarded as potential outliers. outer fences span between 3.0 times the IQR above the 3rd quartile and below the 1st quartile. Data points lying beyond the outer fences are regarded as practically outliers. IQR 2.3.3.6 Shape: Skewness and Kurtosis skewness is a statistic that measures the amount of imbalance away from symmetry. A positive number indicates right-skewness (aka skewed to the right) with a long tail on the right and a negative number indicates left-skewness (aka skewed to the left) with a long tail on the left. average of \\(z^3\\) skewness has no units two tails cancel out kurtosis is a statistic that measures whether sample variability is a result of the presence of infrequent extreme deviations (e.g., outliers). The reference standard is a normal distribution, any of which has a kurtosis of 3. As a result, people usually use excess kurtosis, which is kurtosis - 3, and would simply refer to excess kurtosis as kurtosis. Hence, (excess) kurtosis &gt; 0 means that the mass around the shoulders are shifted to the two tails and/or the center. average of \\(z^4\\) kurtosis has no units each tail contributes separately Understand Kurtosis - external link situation name meaning skewness &gt; 0 right-skewed a long tail on the right skewness &lt; 0 left-skewed a long tail on the left excess kurtosis &gt; 0 shoulder mass shits to the tails and/or centers excess kurtosis &lt; 0 the opposite is true Question: What are the averages of z-score and z-score squared? 2.3.3.7 Effects of Linear Transformations Given x, we have y = a + bx. How does this linear transformation affect measures on center, spread, and shape? Change of units of measurement, e.g., from Celsius to Fahrenheit, is a typical example of a linear transformation. Multipling a b greater than 1 incrases disperstion (i.e., spread), while multipling a b less than 1 decreases dispersion. Adding an a to bx shifts bx by a without changing the dispersion. As a result, we have: Given the old mean m, the new mean changes to a + bm. Given the old standard deviation s, the new standard deviation changes to bs. Shape measures, e.g., skewness and kurtosis, would not change. 2.3.3.8 Standardization and Z-scores Any continuous variable can be standardized by subtracting the mean and then divided by the standard deviaiton, i.e. \\(z = (x-m)/sd\\). The resultant variable will always have \\(mean = 0\\) and \\(sd = 1\\). Since standardization is only a linear transformation, the shape of the variable does not change. Given \\(y = a + bx\\), we have \\(m_y = a + b \\cdot m_x\\) and \\(sd_y = b \\cdot sd_x\\), we have \\[z_x = \\frac{x - m_x}{sd_x}\\] and \\[z_y = \\frac{y - m_y}{sd_y} = \\frac{(a + bx) - (a + bm_x)}{b \\cdot sd_x} = \\frac{x - m_x}{sd_x} = z_x\\] We don’t need to worry about units of measurement anymore. Different features are now somewhat comparable. We have some idea about SDs. 2.3.4 Normal Distributions Normal distributions and their close kin student t distributions are the working horses of statistical inference. 2.3.4.1 Normality Density Curves The probability density function for a normal distribution is \\[f(x) = \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{-\\frac{1}{2} \\left(\\frac{x-\\mu}{\\sigma}\\right)^2}\\] where \\(\\mu\\) is the mean and \\(\\sigma\\) is the standard deviation. Two parameters uniquesly define a normal distribution, denoted as \\(N(\\mu, \\sigma^2)\\). \\(\\mu\\) shifts the center of the curve \\(\\sigma\\) adjusts the spread of the curve A probability density function (PDF) would produce a curve called a density curve. A probability (density) function is a very special function in that the total area under the density curve would always sum to 1 and hence the name probability. Given a probability density function (PDF), one needs to know how to do the followings in R. density = dnorm(quantile, mean=0, sd=1) percentile = pnorm(quantile, mean=0, sd=1) quantile = qnorm(percentile, mean=0, sd=1) data = rnorm(sample_size, mean=0, sd=1) 2.3.4.2 The 68-95-99.7 Rule When \\(\\mu = 0\\) and \\(\\sigma = 1\\), we get the standard normal distribution. We know everything about the standard normal distribution. NOTE: There are an infinite number of normal distributions, each with a different \\(\\mu\\) and \\(\\sigma\\). However, there is only one standard normal distribution, called the standard normal distribution. The 68-95-99.7 Rule 2.3.4.3 Normal Quantile Plot Normal Quantile Plot (aka QQ Plot) is used to visually inspect how much a given distribution deviates from a normal distribution by comparing it against the standard normal distribution. Arrange the observed data values from smallest to largest. Record what percentile of the data each value occupies. For example, the smallest observation in a set of 20 is at the 5% point, the second smallest is at the 10% point, and so on. Do normal distribution calculations to find the z-scores corresponding to these same percentiles. For example, z = -1.645 is the 5% point of the standard normal distribution, and z = -1.282 is the 10% point. Plot each data point on the y axis against the corresponding z-score on the x axis. If the data distribution is close to any normal distribution, the plotted points will lie close to a straight line. "],
["statistical-inference.html", "3 Statistical Inference 3.1 Random Variable 3.2 Central Limit Theorem 3.3 Statistical Inference", " 3 Statistical Inference 3.1 Random Variable Var1 Var2 Var3 Var4 1 \\(y_1\\) 2 \\(y_2\\) 3 \\(y_3\\) 4 \\(...\\) 5 \\(y_n\\) What is a random variable or RV? We have rows as cases and columns as variales. How is a variable as we know it connected with the idea of a random varible? 形而上者谓之道 - 在天成象 - 象（无形之概念、蓝图、计划） 形而下者谓之器 - 在地成形 - 现象（有形之物、器、案例） Let’s choose \\(Y_1, Y_2, Y_3, ..., Y_n\\) from the standard normal distribution \\(N(0,1)\\). before we see the value of \\(Y_i\\) \\(Y_i\\) is a random variable, which is practically a probability distribution \\(Y_1, Y_2, Y_3, ..., Y_n\\) is a random sample a statistic (e.g., \\(\\bar{Y} = 1/n \\sum Y_i\\)) computed from a random sample is another random variable called a sampling statistic after we see the value of \\(Y_i\\) \\(y_i\\) is a case, which is a specific number \\(y_1, y_2, y_3, ..., y_n\\) is a sample a statistic (e.g., \\(m=1/n \\sum y_i\\)) computed from a sample is a number called a sample statistic In this context, the word random means unsettled. The value of a random variable will not be settled unless it is actually sampled and observed, i.e., an actual data point is collected. 3.1.1 IID - Independently and Identically Distributed The phrase, let’s choose \\(Y_1, Y_2, Y_3, ..., Y_n\\) from the standard normal distribution \\(N(0,1)\\), can be succinctly summarized as follows: \\[Y_1, Y_2, Y_3, ..., Y_n \\overset{iid}{\\sim} N(0,1) \\] where iid means independently and identically distributed. 3.1.1.1 Identically Distributed Let’s image that we have 3 black (B) balls and 2 red (R) balls. If you would randomly choose a ball, what is the probability that it’s black? Answer: \\(P(B_1) = 3/5\\). Now if we indeed pick a black ball in our first try and we decide to choose a second ball. Given that we do not put the first ball back, what is the probability that the second one is also black? Answer: \\(P(B_2) = 2/4\\). Hence, choosing two balls in this example violates the identically distributed assumption. Two important ideas emerge in the process of choosing (aka sampling) things. In the previous sampling process, after each draw, if we do not put the balls back, this way of choosing things is called sampling without replacement. In contrast, if you do put the balls back after each draw, it is called sampling with replacement. It is hence clear that the word replacement in this context practically means putting things back (or more precisely finding a replacement for the one that is drawn). In practice, we would rarely sample with relacement. However, if the pool is large enough, we can ignore the slight change in probability after each time one case is drawn. 3.1.1.2 Independently Distributed Now let’s consider another case. Imagine that at the beginning of a semester, you as a student is trying to find a course to enroll. There are N courses in total and all courses are equally attractive (i.e., same probabiliy of being chosen by a student). You and your friend from the same dorm are discussing about the teachers and courses. Eventually, the two of you decide to enroll into the same course denoted as C. What are the probabilities of you and your friend enrolling into course C? Answer: \\(P(C_1) = 1/N\\) and \\(P(C_2) = 1/N\\). Since there are N courses in total and all are equally attractive, the probabilities of you and your friend enrolling into course C are the same. Is the probability of you enrolling into the course independent of the probability of your friend enrolling into the same course? Answer: obviously no! Because you and your friend are not making the course enrollment decisions separately, but with much discussion going around. Hence, choosing the same course in this example violates the independently distributed assumption, even though they are identically distributed. 3.1.2 Random Variable vs. Case Random Variable(s) Case(s) is a concept instance is a probabiilty distribution number comprise a population sample with sample size \\(n = \\infty\\) or all data \\(n\\) is finite or some data comprise a probability distribution empirical spread/distribution comprise a random sample sample give rise to a sampling statistic sample statistic 3.1.3 Properties of a Random Variable Previously, we have \\(Y_1, Y_2, Y_3, ..., Y_n \\overset{iid}{\\sim} N(0,1)\\) and \\(\\bar{Y} = \\frac{1}{n} \\sum_{i=1}^{n} Y_i\\), where both \\(Y_i\\)s and \\(\\bar{Y}\\) are random variables. In practice, we would encounter a variety of random variables. As a result, we would constantly ask the questions: What does a RV look like? What is the distribution of a RV? What are the properties of a RV? center of a RV or \\(E(RV)\\) spread of a RV or \\(Var(RV)\\) shape of a RV or \\(shape(RV)\\) The first two questions are identical. The first one is using informal language and the second using formal academic language. The two questions are reduced to the third question, which is basically asking for a quick summary of a distribution in terms of the center, spread, and shape. The properties of a RV can be succintly summarized as \\[Y \\sim \\forall(\\mu, \\sigma^2) \\] where \\(\\forall\\) stands for any. Notice \\(\\forall\\) is just an inverted A. 3.2 Central Limit Theorem Central Limit Theorem or CLT is the singularly most important theorem in statistics. CLT forms the foundation of statistical inference, a subject of perpetual interest in statistics. 3.2.1 Presenting Central Limit Theorem Given a random variable Y of any shape with mean \\(\\mu\\) and variance \\(\\sigma^2\\), \\[Y \\sim \\forall(\\mu,\\sigma^2)\\] We choose a random sample from Y \\[Y_1, Y_2, Y_3, ..., Y_n ~ \\overset{iid}{\\sim} ~ \\forall(\\mu,\\sigma^2)\\] where each \\(Y_i\\) is a random variable with \\(E(Y_i) = \\mu\\) and \\(Var(Y_i) = \\sigma^2\\). We can compute the sampling statistic of the sample mean \\[\\bar{Y} = \\frac{1}{n} \\sum_{i=1}^{n} Y_i\\] where \\(\\bar{Y}\\) becomes another random variable. Hence, we would immediately ask the question: What are the properties of \\(\\bar{Y}\\)? The answer to this question gives birth to the singularly most important theorem in statistics, known as the Central Limit Theorem or CLT. See the proof of part of the theorem below. Answers: \\[\\bar{Y} \\sim N \\left( \\mu, \\frac{\\sigma^2}{n} \\right)\\] or equivalently \\(E(\\bar{Y}) = \\mu\\) \\(Var(\\bar{Y}) = \\sigma^2 / n\\) \\(shape(\\bar{Y}) = N\\) The most surprising part of the CLT is the discovery that given a random variable \\(Y\\) of any distribution, \\(\\bar{Y}\\) is always normally distributed as long as two conditions are satisfied: \\(Y_1, Y_2, Y_3, ..., Y_n ~ \\overset{iid}{\\sim} ~ \\forall(\\mu,\\sigma^2)\\) sample size n is large enough Proof - Prerequisites: \\(E(cX) = c E(X)\\) \\(E(X+Y) = E(X) + E(Y)\\) \\(E(cX) = c^2E(X)\\) \\(Var(X+Y) = Var(X) + Var(Y)\\) if \\(X \\perp Y\\), i.e., X is independent of Y Proof: \\[E(\\bar{Y}) = E( \\frac{1}{n} \\sum_{i=1}^{n} Y_i ) = \\frac{1}{n} E( \\sum_{i=1}^{n} Y_i ) = \\frac{1}{n} \\sum_{i=1}^{n} E(Y_i) = \\frac{1}{n} \\sum_{i=1}^{n} \\mu = \\frac{1}{n} n\\mu = \\mu\\] and \\[Var(\\bar{Y}) = Var( \\frac{1}{n} \\sum_{i=1}^{n} Y_i ) = \\frac{1}{n^2} Var( \\sum_{i=1}^{n} Y_i ) = \\frac{1}{n^2} \\sum_{i=1}^{n} Var(Y_i) = \\frac{1}{n^2} \\sum_{i=1}^{n} \\sigma^2 = \\frac{1}{n^2} n\\sigma^2 = \\frac{\\sigma^2}{n}\\] Note that the proof of \\(E(\\bar{Y}) = \\mu\\) and \\(Var(\\bar{Y}) = \\sigma^2 / n\\) are very straitforward. However, showing that the \\(shape(\\bar{Y}) = N\\) is much more difficult and is beyond the scope of this book. The third point is in fact the most important contribution of the Central Limit Theorem. 3.2.2 Understanding Central Limit Theorem 3.2.2.1 Sampling Process Given a random variable, the sampling process is the process of getting a sample of cases. Get a case involves two steps: (1) choose and (2) measure. To sum up, Y is a RV &lt;=&gt; \\(\\bar{Y} \\sim N(\\mu, \\sigma^2)\\) choose a random sample from Y &lt;=&gt; \\(Y_1, Y_2, Y_3, ..., Y_n ~ \\overset{iid}{\\sim} ~ \\forall(\\mu,\\sigma^2)\\) choose a sample from Y &lt;=&gt; \\(y_1, y_2, y_3, ..., y_n\\) 3.2.2.2 Inference Given a sample of cases, inference is the process of estimating properties of the original random variable. The first and foremost question to ask is How to estimate the population mean \\(\\mu\\)? An intuitive reaction is to consider using sample mean \\(\\bar{y}\\) to estimate population mean \\(\\mu\\). Since each time we collect a sample, the sample mean will change. It is thus natural to consider the sampling distribution of sample mean and the answer is readily given by the CLT. \\[\\bar{Y} \\sim N \\left( \\mu, \\frac{\\sigma^2}{n} \\right)\\] 3.2.2.3 Further Notes Another way to present CLT is \\[\\sum Y_i = n\\bar{Y} \\sim N( n\\mu, n \\sigma^2 )\\] which suggests that any random disturbances added together would give rise to a normal distribution. Given the CLT \\[\\bar{Y} \\sim N \\left( \\mu, \\frac{\\sigma^2}{n} \\right)\\] we know that \\(\\bar{Y}\\) is called the sample mean; \\(N\\) is a probability distribution; \\(\\mu\\) is a mean; \\(\\sigma^2/n\\) is a variance; and \\(\\sqrt{\\sigma^2/n}\\) is the standard deviation. Since the sampling process and inference are so important, we would invent some new terms. \\(\\bar{Y}\\) is the sample mean that is generally known as a sample statistic \\(N\\) is called a sampling distribution, or in this context, the sampling distribution of the sample mean \\(\\sqrt{\\sigma^2/n}\\) is called the standard error that is the standard deviation of the sampling distribution of the sample mean Definition: sampling distribution is the probability distribution of a sample statistic Definition: standard error that is the standard deviation of a sample statistic 3.3 Statistical Inference Statistical inference is a formal mathematical technique used to infer whether a hypothesis is true based on existing evidence, i.e., data that we have collected. 3.3.1 The Diamond Expert Imagine that you are an expert on diamonds. You have studied and seen so many diamonds for so long that you know everything about them. One day a close friend of yours called you and asked you to help him to inspect whether a newly acquired diamond is authentic or not. Let’s pretend that evaluating a diamond would take quite some time and money to do so. Now that your friend comes to your place and gleefully reveals a surprisingly large diamond of 30 cm in diameter. Without doing any technical assessment formally, simply by your gut feeling Do you think the diamond is authentic or not? Answer: If you ask me, I would say the diamond is too big to be real. It’s got to be a fake. Let’s rationalize this gut feel using some mathematics. Since you are the diamond expert, you know that real diamonds would average 0.5 cm in diameter with a standard deviation of 0.3 cm. As a result, if the diamond is real (this is your hypothesis), 30 cm in diameter would translate into a large z-score of about 98. In other words, the probability of seeing a real diamond as large as 30 cm would be infinitesimally small. At this point you need to make a choice between two possible scenarios: Your hypothesis holds (i.e., the diamond is real) and you are lucky enough to have seen an extremely rare event. You cannot be so lucky to see such a rare event. The hypothesis has got to be wrong (i.e., the diamond is fake). In our case, since the probability of seeing a gigantic diamond of 30 cm is so small, our gut feeling readily accepts scenario two. In an instant, the convoluted but precise mathematical rationale presented above would be encapsulated by our good intuition that this diamond is too big to be real! The gut feeling we have replied on is the essence of statistial inference, i.e., making a decision based on a probability computed under a proposed hypothesis. The process can be formalized as follows: State a hypothesis Compute a probability under the hypotheis Make a decision based on pre-determined rule 3.3.2 Hypothesis Testing With a specific normal distribution, there is always a small probability, no matter how small, that an extreme case is indeed coming from the normal distribution. As a result, we need to agree on certain pre-determined cutoffs beyond which extreme cases are flagged as highly unlikely, i.e., too big to be true. The cutoffs we use would correspond to the tail regions summing up to 5% probability, i.e., \\(\\boldsymbol{\\alpha=0.05}\\), where \\(\\alpha\\) is formally referred to as the Type I Error Rate or False Positive Rate. 3.3.2.1 Hypothesis Testing with \\(N(0,1)\\) Under the standard normal distribution, the cutoff for the lower tail \\(z_L=\\) -1.959964, i.e., qnorm(0.025) and the cutoff for the upper tail \\(z_U=\\) 1.959964, i.e., qnorm(0.975). 3.3.2.2 Hypothesis Testing with Known Variance Let’s now consider the CLT, where both \\(\\mu\\) and \\(\\sigma^2\\) are population parameters and are hence constants. \\[\\bar{Y} \\sim N \\left( \\mu, \\frac{\\sigma^2}{n} \\right)\\] In hypothesis testing, \\(\\mu\\) is a hypothesized value proposed by and hence known to the researcher. For the present, we assume that \\(\\sigma\\) is also known to us. To find the cutoffs for \\(\\bar{Y}\\), i.e., \\(\\bar{Y}_L\\) and \\(\\bar{Y}_U\\), we would first convert them into z-scores, i.e., \\(z_L\\) and \\(z_U\\) \\[z_L = \\frac{\\bar{Y}_L - \\mu}{\\sqrt{\\frac{\\sigma^2}{n}}}\\] and \\[z_U = \\frac{\\bar{Y}_U - \\mu}{\\sqrt{\\frac{\\sigma^2}{n}}}\\] Solving for \\(\\bar{Y}_L\\) and \\(\\bar{Y}_U\\) and substituting the values of \\(z_L\\) and \\(z_U\\), we have \\[\\bar{Y}_L = \\mu + z_L \\sqrt{\\frac{\\sigma^2}{n}} = \\mu - 1.96 \\sqrt{\\frac{\\sigma^2}{n}}\\] and \\[\\bar{Y}_U = \\mu + z_U \\sqrt{\\frac{\\sigma^2}{n}} = \\mu + 1.96 \\sqrt{\\frac{\\sigma^2}{n}}\\] As a result, given a specific \\(\\bar{y}\\) computed from a settled sample, if \\(\\bar{y} &lt; \\bar{Y}_L\\) or \\(\\bar{y} &gt; \\bar{Y}_U\\), we would reject that hypothesis that \\(\\bar{y}\\) is coming from a distribution with mean \\(\\mu\\). We would therefore refer to \\(\\bar{y} &lt; \\bar{Y}_L\\) and \\(\\bar{y} &gt; \\bar{Y}_U\\) as the rejection regions. On the contrary, we will call \\(\\bar{Y}_L \\le \\bar{y} \\le \\bar{Y}_U\\) or \\((\\bar{Y}_L, \\bar{Y}_U)\\) the acceptance regions. 3.3.2.3 Confidence Interval with Known Variance ((Coursera - Johns Hopkins - DSJH6-10 Statistical Inference/Week 3/Module 8/08 01,08 02, 08 03,08 04))(Zhu-Qiuxia) Given the two previous cutoffs, we can have \\[P(\\bar{Y}_L \\le \\bar{Y} \\le \\bar{Y}_U) = 0.95\\] To note, this is a valid probability representation, which must have the form \\(P(a \\le X \\le b) = p\\), where \\(a\\) and \\(b\\) are constants, \\(X\\) is a random variable, and \\(0 \\le p \\le 1\\). Now, let’s substitute the values of \\(\\bar{Y}_L\\) and \\(\\bar{Y}_L\\). \\[P \\left(\\mu - 1.96 \\sqrt{\\frac{\\sigma^2}{n}} \\le \\bar{Y} \\le \\mu + 1.96 \\sqrt{\\frac{\\sigma^2}{n}} \\right) = 0.95\\] It must be emphasized again that the above equation is a valid probability representation since \\(\\bar{Y}\\) is a random variable and \\(\\mu\\) and \\(\\sigma\\) are constants (\\(\\mu\\) is a hypothesized value and \\(\\sigma\\) is known to us). Now, let’s focus on the left part of the inequality within the probability equation. \\[\\mu - 1.96 \\sqrt{\\frac{\\sigma^2}{n}} \\le \\bar{Y}\\] We can easily rearrange it and have \\[\\mu \\le \\bar{Y} + 1.96 \\sqrt{\\frac{\\sigma^2}{n}}\\] Similarly, by focusing on the right part of the inequality and rearrange it, we will have \\[\\bar{Y} - 1.96 \\sqrt{\\frac{\\sigma^2}{n}} \\le \\mu \\] Putting together, we will have \\[P \\left[\\bar{Y} - 1.96 \\sqrt{\\frac{\\sigma^2}{n}} \\le \\mu \\le \\bar{Y} + 1.96 \\sqrt{\\frac{\\sigma^2}{n}} \\right] = 0.95\\] or \\[ P[LB \\le \\mu \\le UB] = 0.95 \\] where \\(LB = \\bar{Y} - 1.96 \\sqrt{\\sigma^2/n}\\) and \\(UB = \\bar{Y} + 1.96 \\sqrt{\\sigma^2/n}\\). To note, LB stands for lower bound and UB for upper bound. Notice that we are using [] instead of () because this equation above is not a valid probability representation. The resulting \\([LB, UP]\\) is called the confidence interval or IC at 95% level, i.e. \\(1-\\alpha\\). 3.3.2.4 Margin of Error Previously, we have introduced \\(\\sqrt{\\sigma^2/n}\\) as the standard error or \\(\\mathbf{se}\\). Now we would introduce a new concept called the margin of error or \\(\\mathbf{me}\\). \\[\\text{me} = z_{(1-\\frac{\\alpha}{2})} \\sqrt{\\frac{\\sigma^2}{n}} \\] where \\(z_{(1-\\alpha/2)}\\) is computed by qnorm(1-alpha/2) and alpha is the type I error rate \\(\\alpha\\). By default, \\(\\alpha = 0.05\\) and hence \\(z_{(1-\\alpha/2)} = 1.96\\). Hence it should be clear that \\(\\mu \\pm \\text{me}\\) gives the acceptance regions and \\(\\bar{y} \\pm \\text{me}\\) gives the confidence interval. In other words, with \\(\\mu\\) in the center and reaching out to the left and right by \\(\\text{me}\\), we will have the acception regions. With \\(\\bar{y}\\) in the center and reaching out to the left and right by \\(\\text{me}\\), we will have the confidence interval. 3.3.2.5 Further Notes Interpretation of Confidence Interval Given the confidence interval \\[P \\left[\\bar{Y} - 1.96 \\sqrt{\\frac{\\sigma^2}{n}} \\le \\mu \\le \\bar{Y} + 1.96 \\sqrt{\\frac{\\sigma^2}{n}} \\right] = 0.95\\] one should NOT interpret it as follows: The probability that the true population mean \\(\\mu\\) would fall between LB and UB is 95%. The correct interpretation is: The numerous sets of confidence interval as generated by the current estimator \\(\\bar{Y}\\) will have a 95% chance of capturing the true population mean \\(\\mu\\). To someone who is new to statistics, chances are that the more you would think about how the two claims are different, the more you will get confused. The differences are more cosmetic than practical. In fact, if you adopt a Bayesian view of statistics, the first claim would turn out to be true. However, frequentists would regard the first claim as categorically wrong, because it violates the valid form of a probability representation, i.e., \\(P(a \\le X \\le b) = p\\). No Acceptance of a Null Hypothesis 3.3.3 Hypothesis Testing with Unknown Variance 3.3.3.1 Pivotal Quantity Previous discussion on hypothesis testing and confidence interval revolves around a critically important equation, where \\(z\\) is a random variable and is known as the z statistic. \\[z = \\frac{\\bar{Y} - \\mu}{\\sqrt{\\frac{\\sigma^2}{n}}}\\] In this equation, \\(\\bar{Y}\\) is called the estimator and \\(\\mu\\) is called the estimand, \\(z\\) is of known distribution, and all the other parameters \\(\\sigma\\) and \\(n\\) are known. In situations like this one, the equation is known as a pivotal quanity. Finding the pivotal quanity is critically important, because both conducting hypothesis testing and finding confidence interval depend on it. 3.3.3.2 From Z to T Statistic Let’s define some new terms. residual \\[R_i = Y_i - \\bar{Y}\\] residual variance \\[s^2 = \\frac{\\sum R_i^2}{df} = \\frac{\\sum (Y_i-\\hat{Y})^2}{df} \\overset{\\hat{Y}=\\bar{Y}}{\\rightarrow} \\frac{\\sum (Y_i-\\bar{Y})^2}{n-1}\\] chi-square \\[\\frac{\\sum R_i^2}{\\sigma^2} \\sim \\chi_{df}^2\\] t-statistic \\[t = \\frac{z}{\\sqrt{\\frac{\\chi^2}{df}}}\\] Now, let’s focus on the t-statistic and substitute the corresponding terms into it. \\[t = \\frac{z}{\\sqrt{\\frac{\\chi^2}{df}}} = \\frac{\\frac{\\bar{Y} - \\mu}{\\sqrt{\\frac{\\sigma^2}{n}}} }{\\sqrt{\\frac{\\sum R_i^2}{\\sigma^2} \\cdot \\frac{1}{df}}} = \\frac{\\frac{\\bar{Y} - \\mu}{\\sqrt{\\frac{\\sigma^2}{n}}} }{\\sqrt{\\frac{s^2 \\cdot df}{\\sigma^2} \\cdot \\frac{1}{df}}} = \\frac{\\bar{Y} - \\mu}{\\sqrt{\\frac{s^2}{\\sigma^2} \\cdot \\frac{\\sigma^2}{n}}} = \\frac{\\bar{Y} - \\mu}{\\sqrt{\\frac{s^2}{n}}}\\] The result can be summary as follows. \\[t = \\frac{\\bar{Y} - \\mu}{\\sqrt{\\frac{s^2}{n}}}\\] To note, compared to the z-statistic, the t-statistic is also a pivotal quantity, where \\(t\\) is of a known distribution and \\(s^2\\) is the sample variance that can be easily computed. 3.3.3.3 Hypothesis Testing Revisited To account for the unknown variance \\(\\sigma^2\\), let’s replace z-statistic with t-statistic and redo the previous derivations. We will have acceptance regions for hypothesis testing \\[\\mu \\pm z_{(1-\\frac{\\alpha}{2})} \\sqrt{\\frac{\\sigma^2}{n}} ~~ \\rightarrow ~~ \\mu \\pm t_{(1-\\frac{\\alpha}{2})} \\sqrt{\\frac{s^2}{n}}\\] confidence interval \\[\\bar{Y} \\pm z_{(1-\\frac{\\alpha}{2})} \\sqrt{\\frac{\\sigma^2}{n}} ~~ \\rightarrow ~~ \\bar{Y} \\pm t_{(1-\\frac{\\alpha}{2})} \\sqrt{\\frac{s^2}{n}}\\] 3.3.4 Examples of Hypothesis Testing 3.3.4.1 One-sample T-Test We are conducting a survey asking students to rate the overall quality of a specific course. The question is framed on a Likert scale from 1 to 6 with 1 being the most negative response and 6 the most positive response. An example of this would be as follows: Do you agree with the following statement: The course is well taught overall. 1 - strongly disagree 2 - disagree 3 - mildly disagree 4 - mildly agree 5 - agree 6 - strongly agree Strictly speaking, student rating is an ordinal variable. In practice, however, we would treat it as an interval variable with the assumption that the spacings between any two consecutive ratings are equidistant. Let’s pretend we have collected data from a sample of 56 students. The average rating is 3.8 with a standard deviation of 1.2. The question is Is the average student rating truly different from being neutral? Let’s summarize and present some statistics. \\(n = 56\\) \\(\\bar{y}=3.8\\) \\(s=1.2\\) \\(t_{(p=0.975,~df=n-1)} = 2.004\\), result of qt(p=0.975, df=55) \\(s/\\sqrt{n} = 0.2/\\sqrt{56} = 0.160\\) propose the hypothesis Under the hypothesis that average student rating is indeed neutral, we have \\(\\mu = 3.5\\). compute a probability under the hypothesis \\[t = \\frac{\\bar{Y} - \\mu}{\\sqrt{\\frac{s^2}{n}}} = \\frac{3.8 - 3.5}{0.160} = 1.875\\] Using R code p = 2 * ( 1 - pt(1.875, df=56-1) ), we have p = 0.067. make a decision Given p = 0.067 and our pre-determined cutoff is \\(\\alpha = 0.05\\), we should NOT reject our hypothesis, since the observed average 3.8 is not statistically significantly different from the neurality of 3.5. 3.3.4.2 Paired T-Test Imagine we are conducting two surveys to measure student interests in the course. One survey is delivered in the beginning of the course and the other by the end of the course. This is a typical pre-test and post-test scenario. Use the following code to generate some data and use the data to conduct the needed statistical test. set.seed(1234) rating1 = sample(1:6, size=56, replace=T, prob=c(1,1,2,2,2,2)) rating2 = sample(1:6, size=56, replace=T, prob=c(1,1,2,2,3,3)) "],
["common-statistical-tests.html", "4 Common Statistical Tests 4.1 Dummy (X) - Continuous (Y) 4.2 Dummy (X) - Dummy (Y) 4.3 Categorical (X) - Dummy (Y) 4.4 Categorical (X) - Continuous (Y)", " 4 Common Statistical Tests \\(_X \\ ^Y\\) dummy continuous dummy (2) difference in proportions or odds ratio (1) two-sample t-test categorical (3) chi-squared test (4) ANOVA continuous (6) logistic regression (5) linear regression 4.1 Dummy (X) - Continuous (Y) RCT or Randomized Controlled Trial is the prime example of an Experiment Design, where the independent variable X is a dummy variable. 4.1.1 Two-sample T-Test Given two samples \\[Y_{T_i} \\sim \\forall \\left( \\mu_T, \\sigma_T^2 \\right) ~~~~ and ~~~~ Y_{C_j} \\sim \\forall \\left( \\mu_C, \\sigma_C^2 \\right) \\] where \\(i = 1,2,3,...,n_T\\) and \\(j = 1,2,3,...,n_C\\) and \\[\\bar{Y}_T = \\frac{1}{n_T} \\sum Y_{T_i} ~~~~ and ~~~~ \\bar{Y}_C = \\frac{1}{n_C} \\sum Y_{C_i}\\] The estimand and estimator that are of interest to us are as follows. estimand: \\(\\Delta = \\mu_T - \\mu_C\\) estimator: \\(\\hat{\\Delta} = \\bar{Y}_T - \\bar{Y}_C\\). The estimand is a constant (because it’s a population parameter) and the estimator is a random variable. Hence, we are immediately interested in the properties of the estimator. Question: What are the properties of the estimator \\(\\hat{\\Delta}\\)? Answers: \\(E(\\hat{\\Delta}) = \\mu_T - \\mu_C\\) \\(Var(\\hat{\\Delta}) = \\frac{\\sigma_T^2}{n_T} + \\frac{\\sigma_C^2}{n_C}\\) \\(shape(\\hat{\\Delta}) = N\\) or equivalently \\[\\hat{\\Delta} \\sim N \\left( \\mu_T - \\mu_C, ~ \\frac{\\sigma_T^2}{n_T} + \\frac{\\sigma_C^2}{n_C} \\right)\\] Proof of the properties of \\(\\hat{\\Delta}\\) is as follows. Proof: Prerequisites \\[Var(X+Y) = Var(X) + 2Cov(X,Y) + Var(Y)\\] \\[Var(X-Y) = Var(X) - 2Cov(X,Y) + Var(Y)\\] When X and Y are independent, i.e., \\(X \\perp Y\\), we have \\(Cov(X,Y)=0\\) and hence \\[Var(X \\pm Y) = Var(X) + Var(Y)\\] Proof Considering the CLT, \\[\\bar{Y}_{T} \\sim N \\left( \\mu_T, ~ \\frac{\\sigma_T^2}{n_T} \\right) ~~~~ and ~~~~ \\bar{Y}_{C} \\sim N \\left( \\mu_C, ~ \\frac{\\sigma_C^2}{n_C} \\right) \\] we have the following \\[E(\\hat{\\Delta}) = E(\\bar{Y}_T - \\bar{Y}_C) = E(\\bar{Y}_T) - E(\\bar{Y}_C) = \\mu_T - \\mu_C = \\Delta\\] and \\[Var(\\hat{\\Delta}) = Var(\\bar{Y}_T - \\bar{Y}_C) = Var(\\bar{Y}_T) + Var(\\bar{Y}_C) = \\frac{\\sigma_T^2}{n_T} + \\frac{\\sigma_C^2}{n_C}\\] Now, given what we know about the properties of \\(\\hat{\\Delta}\\) and the CLT, we have \\[z = \\frac{ \\hat{\\Delta} - \\Delta }{ \\text{se}(\\Delta) } = \\frac{ (\\bar{Y}_T - \\bar{Y}_C) - (\\mu_T - \\mu_C) }{ \\sqrt{ \\frac{\\sigma_T^2}{n_T} + \\frac{\\sigma_C^2}{n_C} } }\\] At this point, \\(\\sigma_T\\) and \\(\\sigma_C\\) are unknown to us. As a result, we need to use sample variances to replace population variances and the z-statistic would thus become a t-statistic. \\[t = \\frac{ (\\bar{Y}_T - \\bar{Y}_C) - (\\mu_T - \\mu_C) }{ \\sqrt{ \\frac{s_T^2}{n_T} + \\frac{s_C^2}{n_C} } }\\] with \\(\\text{df} = \\text{df}_T + \\text{df}_C = (n_T-1) + (n_C-1)\\) To conduct hypothesis test, we have propose the hypothesis \\(H_0: \\Delta = \\mu_T - \\mu_C = 0\\), where \\(H_0\\) is called the null hypothesis \\(H_a: \\Delta = \\mu_T - \\mu_C \\ne 0\\), where \\(H_a\\) is called the alternative hypothesis compute a probability under the hypothesis \\[t = \\frac{ \\bar{Y}_T - \\bar{Y}_C }{ \\sqrt{ \\frac{s_p^2}{n_T} + \\frac{s_p^2}{n_C} } }\\] Under the null hypothesis that the two samples are from the same population, it is natural for us to combine the two sample varinances. \\[s_p^2 = \\frac{\\text{df}_C}{\\text{df}_C+\\text{df}_T} s_C^2 + \\frac{\\text{df}_T}{\\text{df}_C+\\text{df}_T} s_T^2 = \\frac{(n_C-1)}{(n_C-1) + (n_T-1)}s_C^2 + \\frac{(n_T-1)}{(n_C-1) + (n_T-1)}s_T^2\\] We can then compute the p-value using the following R code. p_value = 2 * ( 1 - pt(t, df=(nt-1)+(nc-1)) ) make a decision Make a decison based on pre-determined confidence level, i.e. \\(1 - \\alpha\\), where \\(\\alpha=0.05\\) by default. 4.1.2 Statistical Power (Coursera - Johns Hopkins - DSJH6-10 Statistical Inference/Week 4/Module 11:Power)(Zhu-Qiuxia) type I error rate \\(\\alpha\\) specificity \\(1-\\alpha\\) type II error rate \\(\\beta\\) sensitivity or power \\(1-\\beta\\) Note: CV stands for critical value. Question: Assuming that (a) the treatment and control groups have identical sample size \\(n\\), (b) \\(\\alpha = 0.05\\), and (c) \\(\\beta = 0.80\\), what is the smallest \\(n\\) required to detect a true difference of size \\(\\Delta\\)? Answer: \\[n = \\frac{2 s_p^2 ( z_{(1-\\frac{\\alpha}{2})} - z_\\beta )^2}{\\Delta^2} \\approx \\frac{16s_p^2}{\\Delta^2}\\] where \\[s_p^2 = \\frac{(n_C-1)}{(n_C-1) + (n_T-1)}s_C^2 + \\frac{(n_T-1)}{(n_C-1) + (n_T-1)}s_T^2\\] The proof for the formula above is as follows. Proof: \\[z_{(1-\\frac{\\alpha}{2})} = \\frac{\\text{CV}-0}{\\text{se}} ~~~~ \\rightarrow ~~~~ \\text{CV} = z_{(1-\\frac{\\alpha}{2})} \\cdot \\text{se}\\] where \\(\\alpha=0.05\\) by default and hence \\(z_{(1-\\alpha/2)} = 1.96\\). \\[z_\\beta = \\frac{\\text{CV} - \\Delta}{\\text{se}} ~~~~ \\rightarrow ~~~~ \\text{CV} = \\Delta + z_\\beta \\cdot \\text{se}\\] where \\(\\beta=0.80\\) by default and hence \\(z_\\beta = -0.84\\). Through equating \\(\\text{CV}\\), we have \\[z_{(1-\\frac{\\alpha}{2})} \\cdot \\text{se} = \\Delta + z_\\beta \\cdot \\text{se}\\] and hence \\[(z_{(1-\\frac{\\alpha}{2})} - z_\\beta) \\cdot \\text{se} = \\Delta ~~~~ \\rightarrow ~~~~~ \\text{se} = \\frac{\\Delta}{z_{(1-\\frac{\\alpha}{2})} - z_\\beta}\\] Moreover, with the assumption that treatment and control groups have the same sample size \\(n\\) and using the pooled variance, we have \\[\\text{se}^2 = \\frac{s_T^2}{n_T} + \\frac{s_C^2}{n_C} = \\frac{2s_p^2}{n}\\] \\[\\text{se}^2 = \\left( \\frac{\\Delta}{z_{(1-\\frac{\\alpha}{2})} - z_\\beta} \\right)^2 = \\frac{2s_p^2}{n}\\] and hence \\[n = \\frac{2 s_p^2 ( z_{(1-\\frac{\\alpha}{2})} - z_\\beta )^2}{\\Delta^2} = \\frac{2 \\cdot 7.84 \\cdot s_p^2}{\\Delta^2} \\approx \\frac{16s_p^2}{\\Delta^2} = \\left( \\frac{4s_p}{\\Delta} \\right)^2\\] where \\(( z_{(1-\\frac{\\alpha}{2})} - z_\\beta )^2 = (1.96 + 0.84)^2 = 7.84\\). Given the formula above, we can answer the following questions. Question: What factors can result in a smaller sample size? Answers: larger \\(\\alpha\\) larger \\(\\beta\\) smaller \\(s_p^2\\) larger \\(\\Delta\\) By rearranging the formula above, we have \\[z_\\beta = z_{(1-\\frac{\\alpha}{2})} - \\sqrt{ \\frac{n\\Delta^2}{2s_p^2} }\\] Question: What factors would result in larger statistical power? Answers: larger \\(n\\) smaller \\(s_p^2\\) larger \\(\\alpha\\) larger \\(\\Delta\\) Tip: To answer this question, think about the graph not the formula. 4.2 Dummy (X) - Dummy (Y) 4.2.1 Basics 4.2.1.1 Bernoulli Distribution A random variable Y following a Bernoullie distribution can be expressed as follows. \\[Y \\sim Bern(\\pi)\\] where Y = 1 with probability \\(\\pi\\) and Y = 0 with probability \\(1-\\pi\\). Based on the definitions of E(Y) and Var(Y), we have \\[E(Y) = \\sum Y \\cdot P(Y) = 1 \\cdot \\pi + 0 \\cdot (1-\\pi) = \\pi\\] and \\[Var(Y) = E(Y - EY)^2 = \\sum (Y - EY)^2 \\cdot P(Y) = (1-\\pi)^2 \\cdot \\pi + (0-\\pi)^2 \\cdot (1-\\pi)\\] \\[= (\\pi^2 - 2\\pi + 1)\\pi + \\pi^2 (1-\\pi) = \\pi - \\pi^2 = \\pi(1-\\pi)\\] Hence the properties of the random variable Y can be expressed as \\[Y \\sim Bern(\\pi, \\pi(1-\\pi))\\] Given the CLT, we therefore have \\[\\bar{Y} \\sim N \\left( \\pi, \\frac{\\pi(1-\\pi)}{n} \\right)\\] 4.2.1.2 Inference with One Variance TODO: Contents to be added! 4.2.2 Difference in Proportions \\(_X \\ ^Y\\) Outcome=No (0) Outcome=Yes (1) column total Control (0) a b n Treatment (1) c d n where \\(a+b=n\\) and \\(c+d=n\\). To note, treatment and control groups do not have to have the same sample size n. We deliberately make them to be identical in order to emphasize that row totals can be set beforehand. \\[p_T = P(Y=1|X=1) = \\frac{d}{c+d}\\] and \\[p_C = P(Y=1|X=0) = \\frac{b}{a+b}\\] Similar to two sample t-test, we have \\[\\hat{\\Delta} \\sim N \\left( \\pi_T-\\pi_C, ~ \\sqrt{\\frac{\\pi_T(1-\\pi_T)}{n_T} + \\frac{\\pi_T(1-\\pi_C)}{n_C}} \\right)\\] propose a hypothesis The two samples are from the same distribution, i.e., \\(H_0: \\Delta=\\pi_T-\\pi_C = 0\\) \\(H_a: \\Delta=\\pi_T-\\pi_C \\ne 0\\) compute the probability under the hypothesis Since the two samples are from the same distribution, we can compute a pooled variance \\[\\pi_p = \\frac{\\sum Y_{T_i} + \\sum Y_{C_i}}{n_T+n_C}\\] and hence \\[z = \\frac{\\hat{\\Delta}}{\\text{se}(\\hat{\\Delta})} = \\frac{\\hat{\\Delta}}{\\sqrt{\\pi_p(1-\\pi_p) (\\frac{1}{n_T}+\\frac{1}{n_C})}}\\] To note, since no unknown \\(\\sigma\\) is involved, we can simply use the z-statistic instead of the t-statistic. make a decision Make a decison based on pre-determined confidence level, i.e. \\(1 - \\alpha\\), where \\(\\alpha=0.05\\) by default. One problem with difference in proportions is that it does not handle small proportions well. For example, is \\(\\hat{\\Delta} = 0.05\\) a small or big difference? It would be considered small if it’s between 0.95 and 0.90; but would be a big difference if it’s between 0.10 and 0.15. 4.2.3 Relative Risks An alternative statistic is called relative risks. \\[RR = \\frac{p_T}{p_C}\\] Now we need to derive the properties of RR. In general, it is extremely difficult to deal with products or quotients of random variables. One way to tackle this is to use log-transformation. \\(\\log(XY) = \\log(X) + \\log(Y)\\) \\(\\log(\\frac{X}{Y}) = \\log(X) - \\log(Y)\\) \\(\\log(X^Y) = Y \\log(X)\\) Log-transformation can easily reduce products and quotients into sums. In our case, we have \\[\\log RR = \\log p_T - \\log p_C\\] and \\[\\text{se}(\\log RR) = \\sqrt{\\frac{1-\\pi_T}{n_T}+\\frac{1-\\pi_C}{n_C}}\\] Proof of the formula above is as follows. Proof: Prerequisites \\[Var(f(x)) = Var(X) \\cdot (f&#39;(x))^2\\] where \\(f&#39;(x)\\) is the first derivative of \\(f(x)\\) with respect to x. In our case, \\[Var(\\log p) = Var(p) \\left( \\frac{1}{p} \\right)^2 = \\frac{p(1-p)}{np^2} = \\frac{1-p}{np}\\] Proof \\[Var(\\log RR) = Var(\\log p_T - \\log p_C) = Var(\\log p_T) + Var(\\log p_C) = \\frac{1-p_T}{n_Tp_T} + \\frac{1-p_C}{n_Cp_C}\\] Under the null hypothesis, i.e., \\(\\pi_T = \\pi_C\\), we have \\(E(\\log RR) = 0\\), and hence \\[z = \\frac{\\log p_T - \\log p_C}{\\sqrt{\\frac{1-p_T}{n_T}+\\frac{1-p_C}{n_C}}}\\] For confidence interval, we have \\[[L, U] = \\log \\frac{p_T}{p_C} \\pm z_{(1-\\frac{\\alpha}{2})} \\cdot \\sqrt{\\frac{1-p_T}{n_T}+\\frac{1-p_C}{n_C}}\\] To convert the confidence interval back to the original scale, we have \\([e^L, e^U]\\). One problem with relative risks is that it is not appropriate for case-control design. \\(_X \\ ^Y\\) Cancer=No (0) Cancer=Yes (1) Smoking=No (0) a b Smoking=Yes (1) c d column total n n where \\(a+c=n\\) and \\(b+d=n\\). To note, cancer or non-cancer groups do not have to have the same sample size n. We deliberately make them to be identical in order to emphasize that column totals can be set beforehand. \\[p_T = P(X=1|Y=1) = \\frac{d}{b+d}\\] and \\[p_C = P(X=1|Y=0) = \\frac{c}{a+c}\\] In a case like this one, we cannot perform an experiment and forcibly assign people into treatment and control groups. Instead, we find the same number of people with or without cancer and investigate the proportion of smokers. A study like this is called a case-control study. 4.2.4 Odds Ratio An alternative statistic is called odds ratio. \\[\\text{odds} \\overset{1^*}{=} \\frac{p}{1-p} \\overset{2^*}{=} \\frac{\\text{total # of success}}{\\text{total # of failure}} \\] by definition \\(1^*\\): Given \\(p = 1/4\\), we have odds = (1/4) / (3/4) = 1/3 by definition \\(2^*\\): If odds of success is x, odds of failure is 1/x by definition \\(2^*\\): Given odds = 1/3, we have p = 1/3 / (1/3 + 1) = 1/4 \\[p = \\frac{\\text{odds}}{\\text{odds}+1}\\] Consider the contingency table below, where rows represent the independent variable X and columns represent the dependent variable Y. \\(_X \\ ^Y\\) 0 1 0 a b 1 c d Let \\(p_C\\) represents the probability of Y=1 given X=0 and \\(p_T\\) represents the probability of Y=1 given X=1. We would thus have \\[p_C=\\frac{b}{a+b} ~~~~ \\rightarrow ~~~~ \\text{odds}_C=\\frac{p_C}{1-p_C} = \\frac{a}{a+b} \\cdot \\frac{a+b}{b} = \\frac{a}{b}\\] and \\[p_T=\\frac{d}{c+d} ~~~~ \\rightarrow ~~~~ \\text{odds}_T=\\frac{p_T}{1-p_T} = \\frac{d}{c+d} \\cdot \\frac{c+d}{c} = \\frac{d}{c}\\] Therefore, we have \\[\\hat{\\theta} = \\frac{\\text{odds}_T}{\\text{odds}_C} = \\frac{p_T}{1-p_T} \\cdot \\frac{1-p_C}{p_C} = \\frac{a}{b} \\cdot \\frac{d}{c} = \\frac{ad}{bc}\\] To note, for the contingency table above, if we treat rows as Y and columns as X and recompute the odds ratio \\(\\hat{\\theta}\\). We would end up getting exactly the same result. Compared to relative risks, odds ratio is superior in the aspect such that it can handle the case-control study. Practice: Compute the odds ratio with the assumption of a case-control study. In order to conduct hypothesis testing, we now need to consider the standard error of the odds ratio. Similar to the case of relative risks, to simplify the derivation, we will consider the standard error of the log of odds ratio instead of odds ratio, i.e., \\(\\log(\\hat{\\theta})\\). Under the null hypothesis that the two ratios are identical, we have \\(\\log(\\hat{\\theta}) = \\log(1) = 0\\). Given the CLT, we therefore have \\[z = \\frac{\\log(\\hat{\\theta})}{\\text{se}(\\log(\\hat{\\theta}))}\\] where \\(\\hat{\\theta}\\) is the empirical odds ratio computed from the two samples and \\[\\text{se}(\\log(\\hat{\\theta})) = \\sqrt{ \\frac{1}{a} + \\frac{1}{b} + \\frac{1}{c} + \\frac{1}{d} }\\] The proof of the formula above is as follows. Proof: Prerequisites \\[Var(f(x)) = Var(X) \\cdot (f&#39;(x))^2\\] where \\(f&#39;(x)\\) is the first derivative of \\(f(x)\\) with respect to x. In our case, for example, \\[Var(\\log(\\text{odds})) = Var \\left( \\log(\\frac{p}{1-p}) \\right) = Var(p) \\cdot \\left( \\frac{1}{p(1-p)} \\right)^2 = \\frac{\\pi(1-\\pi)}{n} \\cdot \\frac{1}{(\\pi(1-\\pi))^2}\\] \\[= \\frac{1}{n\\pi(1-\\pi)}\\] where \\[\\left( \\log(\\frac{p}{1-p}) \\right)&#39; = ( \\log(p) - \\log(1-p) )&#39; = \\frac{1}{p} - \\frac{-1}{1-p} = \\frac{1}{p(1-p)}\\] Proof \\[Var(\\log(\\hat{\\theta})) = Var \\left( \\log(\\frac{\\text{odds}_T}{\\text{odds}_C}) \\right) = Var(\\log(\\text{odds}_T) - \\log(\\text{odds}_C))\\] \\[= \\frac{1}{n_Tp_T(1-p_T)} + \\frac{1}{np_C(1-p_C)} = \\frac{c+d}{cd} + \\frac{a+b}{ab} = \\frac{1}{a} + \\frac{1}{b} + \\frac{1}{c} + \\frac{1}{d}\\] where \\[p_T = \\frac{d}{c+d} ~~~~ \\rightarrow ~~~~ n_Tp_T(1-p_T) = (c+d) \\cdot \\frac{d}{c+d} \\cdot \\frac{c}{c+d} = \\frac{cd}{c+d}\\] and \\[p_C = \\frac{a}{a+b} ~~~~ \\rightarrow ~~~~ n_Cp_C(1-p_C) = (a+b) \\cdot \\frac{b}{a+b} \\cdot \\frac{a}{a+b} = \\frac{ab}{a+b}\\] 4.3 Categorical (X) - Dummy (Y) Now that we can deal with a 2-by-2 contingency table. We need to generalize it to handle I-by-J contingency tables, where I and J can be greater than 2. 4.3.1 Basics For the sake of clarity, in the presentation below, we will continue using a 2-by-2 contingency table. However, any conclusions we draw are readily generalizable to I-by-J tables. \\(_X \\ ^Y\\) 0 1 row total 0 a b a+b 1 c d c+d column total a+c b+d n where \\(n = a + b + c + d\\). 4.3.1.1 Marginal, Conditional, and Joint Probabilities marginal probability: \\(P(Y)\\) or \\(P(X)\\) \\[P(Y=0) = \\frac{a+c}{n} ~~~~ and ~~~~ P(Y=1) = \\frac{b+d}{n}\\] or \\[P(X=0) = \\frac{a+b}{n} ~~~~ and ~~~~ P(X=1) = \\frac{c+d}{n}\\] conditional probability: \\(P(Y|X)\\) or \\(P(X|Y)\\) \\[P(Y=0|X=0) = \\frac{a}{a+b} ~~~~ and ~~~~ P(Y=1|X=0) = \\frac{b}{a+b}\\] \\[P(Y=0|X=1) = \\frac{c}{c+d} ~~~~ and ~~~~ P(Y=1|X=1) = \\frac{d}{c+d}\\] or \\[P(X=0|Y=0) = \\frac{a}{a+c} ~~~~ and ~~~~ P(X=1|Y=0) = \\frac{c}{a+c}\\] \\[P(X=0|Y=1) = \\frac{b}{b+d} ~~~~ and ~~~~ P(X=1|Y=1) = \\frac{d}{b+d}\\] joint probability: \\(P(X,Y)\\) \\[P(X=0,Y=0) = \\frac{a}{n}\\] \\[P(X=0,Y=1) = \\frac{b}{n}\\] \\[P(X=1,Y=0) = \\frac{c}{n}\\] \\[P(X=1,Y=1) = \\frac{d}{n}\\] Practice: Given the table below, compute \\(P(X=0,Y=1)\\), \\(P(X=0)\\), \\(P(Y=1)\\). \\(_X \\ ^Y\\) 0 1 row total 0 2 4 6 1 10 20 30 column total 12 24 36 4.3.1.2 Independence in a Contingency Table Two variables are regarded as independent if any one of the three following conditions are met. \\(P(Y|X) = P(Y)\\) \\(P(X|Y) = P(X)\\) \\(P(X,Y) = P(X)P(Y)\\) To further explain it, we will focus on the first condition. The first condition is more complicated than what meets the eye. From the simplest to the most detailed, the first condition can be expressed as follows. simplest expression: \\(P(Y|X) = P(Y)\\) more detailed expression: \\(P(Y|X=0) = P(Y|X=1) = P(Y)\\) most detailed expression: \\(P(Y=0|X=0) = P(Y=0|X=1) = P(Y=0)\\) \\(P(Y=1|X=0) = P(Y=1|X=1) = P(Y=1)\\) These expressions are saying exactly the same thing; they differ only in the amount of details presented. To note, for the claim of independence to hold, \\(P(Y|X=0) = P(Y|X=1) = P(Y)\\) must be true for each level of Y. In other words, all equations subsumed under the most detailed expression must hold so that the more detailed expression would hold. Regardless which form of expression we use, \\(P(Y|X) = P(Y)\\) clearly suggests the distribution of Y does not depend on X the distribution of Y is the same at each level of X whether we consider X or not (i.e., conditioned on X or not) does not affect the distribution of Y 4.3.1.3 Null Hypothesis and Independence In the 2-by-2 contingency table, the following two claims are equivalent. the null hypothesis \\(H_0: p_T = p_C\\) random variables X and Y are independent Proof: By definition, we have \\(p_C = P(Y=1|X=0)\\) and \\(p_T = P(Y=1|X=1)\\). Under the null hypothesis \\(p_C = p_T\\), we have \\(P(Y=1|X=0) = P(Y=1|X=1)\\), which implies \\(P(Y=1|X=0) = P(Y=1|X=1) = P(Y=1)\\) \\(P(Y=0|X=0) = P(Y=0|X=1) = P(Y=0)\\) or equivalently \\(P(Y|X=0) = P(Y|X=1) = P(Y)\\) which clearly shows that X and Y are independent. 4.3.2 Chi-squared Test Given an \\(I \\times J\\) table, where rows represent X with I levels and columns represent Y with J levels. Let’s use \\(n_{ij}\\) to denote the number of counts in the i-th row and j-th column and use \\(n\\) to represent the total number of cases in the sample. 4.3.2.1 Poisson Distribution \\[X \\sim Pois(\\lambda)\\] and \\[f(x) = \\frac{e^{-\\lambda} \\cdot \\lambda^x}{x!}\\] A special property about Poisson distribution is that \\(E(X) = Var(X) = \\lambda\\). Since X is a random variable, with regard to \\(X \\sim \\forall(E(X), Var(X))\\), we have \\[X \\sim Pois(\\lambda, \\lambda)\\] In the introduction of CLT, we have learned that any random disturbances added together would eventually give rise to normality. Poisson distribution is essentially a distribution for counting, i.e., a sum of individual counts. As a result, with reasonably large sample size under the iid assumption, we have \\[X \\sim N(\\lambda, \\lambda)\\] which is a normal approximation to the Poisson distribution. 4.3.2.2 Expected Cell Counts Under Independence In a 2-by-2 table, we can use a, b, c, d to represent the four cell counts. This notation system, however, cannot be readily generalized to I-by-J tables. Starting from here, we will use the following notation. \\(n_{ij}\\) is the cell count for the i-th row and j-th column \\(n_{i+}\\) is the total row count for the i-th row across columns \\(n_{+j}\\) is the total column count for the j-th column across rows \\(n\\) is the total sample size across all rows and columns Given an I-by-J table, under the independence assumption, the expected cell count \\(\\hat{\\mu}_{ij}\\) for the i-th row and j-th column is \\[\\hat{\\mu}_{ij} = \\frac{n_{i+}n_{+j}}{n}\\] Proof: Under the independence assumption, we have \\(p_{ij} = p_{i+}p_{+j}\\) and hence \\[\\frac{\\hat{\\mu}_{ij}}{n} = \\frac{n_{i+}}{n} \\cdot \\frac{n_{+j}}{n} ~~~~ \\rightarrow ~~~~ \\hat{\\mu}_{ij} = \\frac{n_{i+}n_{+j}}{n}\\] 4.3.2.3 Chi-squared Distribution Regardless of its fancy name, chi-squared distribution with 1 degree of freedom is simply a z-score squared. \\[z^2 \\sim \\chi_{df=1}^2\\] and \\[\\sum_{i=1}^n z_i^2 \\sim \\chi_{df=n}^2\\] Example: In our previous discussion, we have \\[\\frac{\\sum R_i^2}{\\sigma^2} \\sim \\chi_{df}^2\\] The proof of the formula above is as follows. Proof: Given \\(Y_1, Y_2, Y_3, ..., Y_n \\sim \\forall(\\mu, \\sigma^2)\\), we have \\[z_i^2 = \\frac{(Y_i-\\mu)^2}{\\sigma^2} \\sim \\chi_{df=1}^2\\] and hence \\[\\sum_{i=1}^n z_i^2 = \\sum_{i=1}^n \\frac{(Y_i-\\mu)^2}{\\sigma^2} = \\frac{\\sum (Y_i-\\mu)^2}{\\sigma^2} \\sim \\chi_{df=n}^2\\] When we use the estimator \\(\\bar{Y}\\) to estimate \\(\\mu\\), we have \\[\\frac{\\sum (Y_i-\\bar{Y})^2}{\\sigma^2} \\sim \\chi_{df=n-1}^2\\] When we use a more complex model \\(\\hat{Y}\\) with \\(p\\) parameters to estimate \\(\\mu\\), we have \\[\\frac{\\sum (Y_i-\\hat{Y})^2}{\\sigma^2} = \\frac{\\sum R_i^2}{\\sigma^2} \\sim \\chi_{df=n-p}^2\\] 4.3.2.4 Chi-squared Statistic Under the hull hypothesis that X and Y are independent, we have \\(E(n_{ij}) = \\hat{n}_{ij}\\). Since cell count follows Poisson distribution and can be approximated by a normal distribution, we have \\(n_{ij} \\sim N(\\hat{n}_{ij}, \\hat{n}_{ij})\\). Therefore, we have \\[z_{ij}^2 = \\frac{(n_{ij}-\\hat{n}_{ij})^2}{\\hat{n}_{ij}} \\sim \\chi_1^2\\] and hence \\[\\sum_{I,J} z_{ij}^2 = \\sum_{I,J} \\frac{(n_{ij}-\\hat{n}_{ij})^2}{\\hat{n}_{ij}} \\sim \\chi_{df=(I-1)(J-1)}^2\\] p = 1 - pchisquare(X, df) Proof of the degree of freedom is as follows. Proof: The df is the difference between the df under \\(H_a\\) and the df under \\(H_0\\). Under the \\(H_0\\), i.e., X and Y are independent, cell counts can be computed directly from row and column counts. There are I rows and hence (I-1) df for the row; J columns and hence (J-1) df for the column. Total df is (I-1) + (J-1). Under the \\(H_a\\), i.e., X and Y are dependent, each cell count need to be specified one by one except for one. Therefore, the total df is (IJ-1). \\[df = (IJ-1) - ((I-1)+(J-1)) = (I-1)(J-1)\\] Let’s use an example to illustrate the point. Question: Given a 2-by-2 table, i.e., I=2 and J=2, knowning the sample size n, how many cells can be freely decided under the null hypothesis? Answer: 2 = (2-1) + (2-1) Question: Under the same conditions, how many cells can be freely decided under the alternative hypothesis? Answer: 3 = 2*2 - 1 You should draw a 2-by-2 table and think in the order of the four cell counts a, b, c, d to convince yourself. 4.3.2.5 Diagnostics It should be clear that the construction of the chi-squared statistic depends on the CLT and the normality assumption in particular. \\(\\chi^2\\)-test is only valid if each expected cell count is greater than 5, i.e., \\(\\hat{n}_{ij} &gt; 5\\) if I and J are large, a few cell counts less than 5, i.e., \\(\\hat{n}_{ij} &lt; 5\\), does not matter From the count table, compute (a) expected cell count table and (b) z-score table. rescaled cell residuals \\[z_{ij} = \\frac{n_{ij} - \\hat{n}_{ij}}{\\sqrt{\\hat{n}_{ij}}}\\] where \\(\\text{se}(n_{ij}) = \\sqrt{\\hat{n}_{ij}}\\). standardized cell residuals \\[z_{ij} = \\frac{n_{ij} - \\hat{n}_{ij}} {\\sqrt{\\hat{n}_{ij}(1-p_{i+})(1-p_{+j})}}\\] where the denominator is the estimated standard error of \\(n_{ij} - \\hat{n}_{ij}\\) under the null hypothesis. Therefore, a standardized residual with absolute value exceeding 2 when there are only a few cells, or exceeding 3 when there are many cells indicates lack of fit of the null hypothesis in that cell. 4.3.3 G-square Test 4.3.3.1 Maximum Likelihood Method Given \\(Y \\sim Bern(\\pi)\\), we draw a random sample \\(Y_1, Y_2, ..., Y_n\\) and compute the sample mean \\(\\bar{Y}\\). \\[P(Y_i) = \\pi^{Y_i}(1-\\pi)^{1-Y_i}\\] \\[P(Y_1,Y_2,...,Y_n) = P(Y_1,Y_2,...,Y_n|\\pi) = \\prod_{i=1}^n Y_i = \\prod_{i=1}^n \\pi^{Y_i}(1-\\pi)^{1-Y_i}\\] At this point, \\(P(Y_1,Y_2,...,Y_n|\\pi)\\) is a probability function, the integration of which sums to 1. \\[L = f(\\pi|Y_1,Y_2,...,Y_n) = \\prod_{i=1}^n \\pi^{Y_i}(1-\\pi)^{1-Y_i}\\] To note, \\(f(\\pi|Y_1,Y_2,...,Y_n)\\) is not a probability function, since its integration does not sum up to 1. As a result, we must give it a different name, which is called the likelihood function. Proof that \\(P(Y_1,Y_2,...,Y_n|\\pi)\\) is a proper probability function is as follows. Proof: Since each \\(P(Y_i)\\) for i = 1, 2, …, n is a probability function, which means \\[\\int P(Y_i)dY_i = 1\\] Hence, we have \\[\\int\\int...\\int_{(n)} P(Y_1,Y_2,...,Y_n)~dY_1dY_2 \\cdots dY_n\\] \\[= \\int\\int...\\int_{(n)} P(Y_1)P(Y_2),...,P(Y_n)~dY_1dY_2 \\cdots dY_n\\] \\[= \\int\\int...\\int_{(n-1)} \\left( \\int P(Y_1)dY_1 \\right) P(Y_2),...,P(Y_n)~dY_2dY_3 \\cdots dY_n\\] \\[= \\int\\int...\\int_{(n-1)} 1 \\cdot P(Y_2),...,P(Y_n)~dY_2dY_3 \\cdots dY_n\\] \\[= \\int\\int...\\int_{(n-2)} 1 \\cdot 1 \\cdot P(Y_3),...,P(Y_n)~dY_3 \\cdots dY_n\\] \\[= \\int\\int...\\int_{(n-3)} 1 \\cdot 1 \\cdot 1 \\cdot P(Y_4),...,P(Y_n)~dY_4 \\cdots dY_n\\] \\[= \\cdots = 1\\] The idea of maximum likelihood is simple: Given the observed data \\(Y_1,Y_2,...,Y_n\\), choose the value of \\(\\pi\\) that would maximize the likelihood function \\(f(\\pi|Y_1,Y_2,...,Y_n)\\). Let’s use the following example to illustrate the point. Example: Let’s use 4 data points \\(Y_1=1, Y_2=0, Y_3=0, Y_4=1\\). \\[L = f(\\pi|Y_1,Y_2,Y_3,Y_4) = \\prod_{i=4}^n \\pi^{Y_i}(1-\\pi)^{1-Y_i} = \\pi(1-\\pi)(1-\\pi)\\pi = \\pi^2(1-\\pi)^2\\] if \\(\\pi=0.0\\), \\(L = 0\\) if \\(\\pi=0.2\\), \\(L = 16/625\\) if \\(\\pi=0.4\\), \\(L = 36/625\\) if \\(\\pi=0.6\\), \\(L = 36/625\\) if \\(\\pi=0.8\\), \\(L = 16/625\\) if \\(\\pi=1.0\\), \\(L = 0\\) We will therefore choose the \\(\\hat{\\pi}\\) that produces the highest probability. In this case, \\(\\hat{\\pi}=0.5\\). Now we need to find a way to solve for the result analytically. \\[L = \\prod_{i=1}^n \\pi^{Y_i}(1-\\pi)^{1-Y_i}\\] and \\[l = \\log(L) = \\sum_{i=1}^n (Y_i \\log(\\pi) + (1-Y_i) \\log(1-\\pi))\\] Get the first derivative of \\(l\\) with respect to \\(\\pi\\) and set it to equal 0. \\[\\frac{dl}{d\\pi} = \\sum_{i=1}^n \\left( Y_i \\cdot \\frac{1}{\\pi} + (1-Y_i) \\cdot \\frac{-1}{1-\\pi} \\right) = \\frac{\\sum Y_i}{\\pi} - \\frac{\\sum (1-Y_i)}{1-\\pi} = 0\\] Solving for the equation, we have \\[\\hat{\\pi} = \\arg\\max(l) = \\frac{\\sum Y_i}{n}\\] Proof: \\[(1-\\pi) \\sum Y_i = \\pi \\sum (1-Y_i) = \\pi (n - \\sum Y_i)\\] \\[\\sum Y_i - \\pi \\sum Y_i = n\\pi - \\pi \\sum Y_i\\] \\[\\pi = \\frac{\\sum Y_i}{n}\\] 4.3.3.2 Likelihood-Ratio Statistic For the same reason, we will deal with log-likelihood (instead of likelihood) in order to simplify derivation. \\[-2 \\log \\left( \\frac{L_0}{L_a} \\right) = -2(\\log L_0 - \\log L_a)\\] where \\(L_0\\) is the likelihood under the \\(H_0\\) and \\(L_a\\) is the likelihood under the \\(H_a\\). The maximum likelihood with less constrained model (aka full model) will always fit at least as good as the more constrained model under the null hypothesis (aka reduced model). Therefore, the quotient \\(L_0/L_a\\) is always between 0 and 1 and its \\(-2 \\log(L_0/L_a)\\) is always positive. For two-way contingency tables, \\(-2 \\log(L_0/L_a)\\) can be simplified to the \\(G^2\\) statistic. \\[G^2 = -2 \\sum n_{ij} \\cdot \\log \\left( \\frac{\\hat{\\mu}_{ij}}{n_{ij}} \\right) ~~~~ \\&amp; ~~~~ df = (I-1)(J-1)\\] Proof: \\[L = P(Y_1,Y_2, ...,Y_n) = \\prod_{I,J} \\frac{e^{-\\lambda} \\cdot (\\lambda_{ij})^{n_{ij}}}{n_{ij}!}\\] and hence \\[l = \\log L = -\\sum \\lambda_{ij} + \\sum (n_{ij} \\cdot \\log \\lambda_{ij}) - \\sum \\log n_{ij}!\\] Under \\(H_0\\), we have \\[l_{H_0} = -\\sum \\hat{\\mu}_{ij} + \\sum (n_{ij} \\cdot \\log \\hat{\\mu}_{ij}) - \\sum \\log n_{ij}!\\] Under \\(H_a\\), we have \\[l_{H_a} = -\\sum n_{ij} + \\sum (n_{ij} \\cdot \\log n_{ij}) - \\sum \\log n_{ij}!\\] To note, \\(\\sum \\hat{\\mu}_{ij} = \\sum n_{ij} = n\\). Therefore, \\[\\log \\frac{L_0}{L_a} = \\sum \\left( n_{ij} \\cdot \\log \\left( \\frac{\\hat{\\mu}_{ij}}{n_{ij}} \\right) \\right)\\] and \\[G^2 = -2 \\sum n_{ij} \\cdot \\log \\left( \\frac{\\hat{\\mu}_{ij}}{n_{ij}} \\right)\\] 4.4 Categorical (X) - Continuous (Y) 4.4.1 ANOVA mid1cat = cut(xdat$mid1, breaks=quantile(xdat$mid1), include.lowest=T) xdat = data.frame(xdat, mid1cat) boxplot(mid3 ~ mid1cat, data=xdat, main=&quot;Boxplot of Exam Results&quot;, xlab=&quot;First Midterm Score Category&quot;, ylab=&quot;Final Exam Score&quot;) abline(h=mean(xdat$mid3,na.rm=T), lty=2) SS DF MS \\[\\mathbf{Total}~~\\] \\[~~\\sum(Y_i-\\bar{Y})^2~~\\] \\[~~n-1~~\\] \\[~~\\frac{1}{n-1} \\sum (Y_i-\\bar{Y})^2\\] \\[\\mathbf{Model}~~\\] \\[~~\\sum(\\bar{Y}_g-\\bar{Y})^2~~\\] \\[~~g-1~~\\] \\[~~\\frac{1}{g-1}\\sum (\\bar{Y}_g-\\bar{Y})^2\\] \\[\\mathbf{Residual}~~\\] \\[~~\\sum(Y_i - \\bar{Y}_g)^2~~\\] \\[~~n-g~~\\] \\[\\frac{1}{n-g}\\sum (Y_i-\\bar{Y}_g)^2\\] where \\(n\\) is the sample size and \\(g\\) is the number of groups. \\[ F = \\frac{MSM}{MSE} = \\frac{ \\frac{1}{p}\\sum (\\hat{Y}_i-\\bar{Y})^2 }{ \\frac{1}{n-1-p}\\sum (Y_i-\\hat{Y}_i)^2 } \\] where \\(F(p, n-1-p)\\) and \\(p=1\\) in the case of simple linear regression. p = 1 - pf(F, df1, df2) "],
["linear-regression.html", "5 Linear Regression 5.1 Table of Contents 5.2 Pearson Product-Moment Correlation Coefficient 5.3 Simple Linear Regression 5.4 Non-linear Relations 5.5 The Assumptions of Simple Linear Regression 5.6 Assumption Diagnostics", " 5 Linear Regression 5.1 Table of Contents Pearson Correlation Simple Linear Regression Scatterplots Transformations 5.2 Pearson Product-Moment Correlation Coefficient 5.2.1 Definition Commonly referred to as Pearson correlation or simply correlation, Pearson product-moment correlation coefficient is a standardized form of the covariance. Given the formula for variance, \\[ Var(Y) = \\frac{\\sum_{i=1}^n(Y_i - \\bar{Y})^2}{n-1} \\] one should not be too surprised to see the formula for covariance. \\[ Cov(X,Y) = \\frac{\\sum_{i=1}^n(X_i - \\bar{X})(Y_i-\\bar{Y})}{n-1} \\] It should thus be clear that variance of Y is the covariance of Y with Y itself, i.e., variance is a special case of covariance. \\[ Cov(Y,Y) = \\frac{\\sum_{i=1}^n(Y_i - \\bar{Y})(Y_i-\\bar{Y})}{n-1} = \\frac{\\sum_{i=1}^n(Y_i - \\bar{Y})^2}{n-1} = Var(Y) \\] If we divide \\(Cov(X,Y)\\) by the corresponding standard deviations involved with regard to X and Y, we get Pearson correlation. In other words, Pearson correlation is the covariance of z-scores. \\[ r = \\frac {\\color{red}{Cov(X,Y)}} {s_Xs_Y} = \\frac {\\color{red}{\\sum_{i=1}^n(X_i - \\bar{X})(Y_i-\\bar{Y})}} {s_X s_Y \\color{red}{(n-1)}} = \\frac{1}{n-1} \\sum_{i=1}^n \\color{gold}{\\frac{(X_i-\\bar{X})}{s_X}} \\color{blue}{\\frac{(Y_i-\\bar{Y})}{s_Y}} = \\frac{1}{n-1} \\sum_{i=1}^n \\color{gold}{z_X} \\color{blue}{z_Y} \\] 5.2.2 Properties of Pearson Correlation The correlation \\(r\\) is always between -1 and 1. Values of \\(r\\) close to 0 indicate a weak linear relationship. Values of \\(r\\) close to 1 imply a strong positive linear relationship, whereas values close to -1 imply a negative linear relationship. The extreme cases of \\(r=-1\\) or \\(r=1\\) occur only when the points in a scatterplot lie exactly along a straight line. In other words, signs of \\(r\\) show the direction and absolute values of \\(r\\) indicate the strength of the relationship. The correlation \\(r\\) does not change when we change the units of measurement of either X or Y or both. This should be clear since correlation \\(r\\) uses the unitless z-scores. The correlation \\(r\\) does not distinguish between independent and dependent variables. Reversing X and Y gives identical results. It has to be emphasized again that statistics knows no causal directions. Statistics on its own only reveals associations, not causations. 5.2.3 Limitations of Pearson Correlation Pearson correlation measures only linear association. Always plot your data before calculating \\(r\\). TODO: need a graph here! Calculation of Pearson correlation invovles means and standard deviations, and hence \\(r\\) is susceptible to outliers. Always plot your data and look for potentially influential data points (which is defined later). 5.2.4 Other Issues on the Usage of Pearson Correlation Pearson correlation based on averaged data is usually higher than the correlation between the same variables using data for individuals. For example, the average weight of infants against their age in months would give a very strong positive correlation near one. A plot of weight against age for individual infants will show much more scatter and lower correlation. On the contrary, using a composite score averaged from multiple items measuring motivation usually gives much more consistent, and hence reliable, result. When the data we use do not contain information on the full range of independent and dependent variables, we have the restricted-range problem. When data suffer from restricted range, \\(r\\) is usually lower than it would be if the full range could be observed. TODO: need an example here! Lurking variables can make correlation results misleading. This problem is not specific to Pearson correlation. All measures of two-way relationships suffer from this problem, which is known as the interaction effect. TODO: need a graph here! Extrapolation (using a model far beyond the range of data used to fit it) often produces unreliable predictions. This problem is not specific to Pearson correlation. All models suffer from some generalizability concern, which might raise the concern of omitted-variable bias (e.g., individuals possessing values beyond the observed range of values might systematically differ from individuals currently in the sample). 5.3 Simple Linear Regression 5.3.1 Independent Variable: From Categorical to Continuous mid1cat = cut(xdat$mid1, breaks=quantile(xdat$mid1), include.lowest=T) xdat = data.frame(xdat, mid1cat) boxplot(mid3 ~ mid1cat, data=xdat, main=&quot;Boxplot of Exam Results&quot;, xlab=&quot;First Midterm Score Category&quot;, ylab=&quot;Final Exam Score&quot;) abline(h=mean(xdat$mid3,na.rm=T), lty=2) plot(mid3 ~ mid1, data=xdat, main=&quot;Scatterplot of Exam Results&quot;, xlab=&quot;First Midterm Score&quot;, ylab=&quot;Final Exam Score&quot;) mod = lm(mid3 ~ mid1, data=xdat) abline(h=mean(xdat$mid3,na.rm=T), lty=2) abline(coef(mod)[1], coef(mod)[2], col=&quot;red&quot;) 5.3.2 Fitting a Straight Line to Data Let us fit a straight line with an intercept and a slope. As a result, the conditional expectation of Y is now a function of X and the prediction model with unknown population parameters can be expressed as follows. To note, “the conditional expectation of Y” is just a fancy way of saying “the predicted values of Y” or “the prediction model of Y”. They are all different ways of saying the same thing. \\[ E(Y|X) = \\beta_0 + \\beta_1X \\] Accordingly, the predition model of Y with sample parameters is: \\[ \\hat{E}(Y_i|X_i) = \\hat{\\beta}_0 + \\hat{\\beta}_1X_i \\] In practice, we often use simplified notations to represent the preceding formula. \\[ \\hat{Y}_i = b_0 + b_1X_i \\] where \\(\\hat{Y}_i \\equiv \\hat{E}(Y_i|X_i)\\), \\(b_0 \\equiv \\hat{\\beta}_0\\) and \\(b_1 \\equiv \\hat{\\beta}_1\\). This is the formula for the deterministic part of the linear regression. By adding the stochastic part, we have \\[ Y_i = \\hat{Y}_i + \\hat{R}_i = b_0 + b_1X_i + e_i \\] We can describe the previous simple linear model as the regression of Y on X. At this moment, this expression might be exceedingly clear to you, since the use of Y and X makes it abundently clear which one is the DV and which one is the IV. In a real case, however, one might be confused over which is which. For example, with the expression “regressing income on education”, one must know that income is Y and education is X. 5.3.3 The Least-Squares Method The least-squares method is one of the ways to solve the above equation for the unknown parameters \\(b_0\\) and \\(b_1\\). The official name for this method is ordinary least squares (OLS) method. \\[ SSR = \\sum_{i=1}^n R_i^2 = \\sum_{i=1}^n (Y_i-\\hat{Y}_i)^2 = \\sum_{i=1}^n (Y_i - (b_0 + b_1X_i))^2 \\] The idea is that we would find \\(b_0\\) and \\(b_1\\) such that \\(SSR\\) would reach its minimum. Remember, the last time we use this idea, we end up finding the mean. \\[ SSR = \\sum_{i=1}^n (Y_i-c)^2 \\] Solving for \\(c\\) that minimizes \\(SSR\\) gives \\(c = \\bar{Y}\\). Applying the idea above, we would eventually find the following solutions. \\[ b_1 = \\frac{\\sum(X_i-\\bar{X})(Y_i-\\bar{Y})} {\\sum(X_i-\\bar{X})^2} \\] and \\[ b_0 = \\bar{Y} - b_1 \\bar{X} \\] 5.3.4 Interpretations of Regression Coefficients It should be clear that \\(b_0\\) is the intercept, which is interpreted as the value of Y when X = 0. To note, values of X oftentimes cannot be exactly 0, e.g., weight or IQ. In such a case, the interpretation of \\(b_0\\) would not make intuitive sense (since no one would have 0 weight or 0 intelligence); the intercept is there only to make the math work. To make the interpretation of \\(b_0\\) meaningful, we can choose to center X with respect to its mean, i.e. \\(X^* = X - \\bar{X}\\). Now \\(b_0\\) is the value of Y when \\(X = \\bar{X}\\). In contrast, \\(b_1\\) is the slope, which is interpreted as the amount of change in Y when X changes by 1 unit of measurement. As a result, the value of \\(b_1\\) would change, if the unit of measurement for Y or X or both changes. 5.3.5 Understanding the Estimators There are several ways to look at \\(b_1\\), each provides some insight from different perspectives. \\[ b_1 \\overset{(1)}{=} \\frac { \\color{blue}{ \\sum(X_i-\\bar{X})(Y_i-\\bar{Y}) } } { \\color{red}{\\sum(X_i-\\bar{X})^2} } \\overset{(2)}{=} r \\frac{s_Y}{s_X} \\overset{(3)}{=} \\color{blue}{\\sum} \\left( \\frac{ \\color{blue}{ (X_i-\\bar{X}) } }{ \\color{red}{SSX} } \\cdot \\color{blue}{(Y_i-\\bar{Y})} \\right) \\overset{(4)}{=} \\frac { \\color{blue}{Cov(X,Y)} } { \\color{red}{Var(X)} } \\] Note: Similar to \\(\\bar{X}\\), \\(SSX = (n-1)Var(X)\\) is treated as a constant and can therefore move in and out of the summation sign \\(\\sum\\) freely. When we change the scale of \\(X\\) to \\(cX\\) (without loss of generality, let’s assume \\(c\\ge1\\)), the term \\((X_i-\\bar{X})\\) would change by \\(c\\) and \\((X_i-\\bar{X})^2\\) would change by \\(c^2\\), and hence we would expect \\(b_1\\) to become \\(1/c \\cdot b_1\\). This result makes sense, since increase in the scale of X is canceled out by decrease in the coefficient of X such that their product remains the same. Similarly, when we change the scale of \\(Y\\) to \\(cY\\), we would expect \\(b_1\\) to become \\(c b_1\\). R_Proof: Linear Transformations x = rnorm(10000, mean=1, sd=5) c = 10 cx = c * x y = 2*x + rnorm(10000, 0, 5) cy = c * y dat = data.frame(x, cx, y, cy) b1 = lm( y ~ x, data=dat)$coefficients[2] b2 = lm( y ~ cx, data=dat)$coefficients[2] b3 = lm(cy ~ x, data=dat)$coefficients[2] bs = c(c,b1,b2,b3) names(bs) = c(&quot;c&quot;,&quot;x&quot;,&quot;cx&quot;,&quot;cy&quot;) bs ## c x cx cy ## 10.0000000 2.0090753 0.2009075 20.0907534 When X and Y are standardized z-scores, which entails that \\(s_X=1\\) and \\(s_Y=1\\), \\(b_1\\) becomes the Pearson correlation \\(r\\). \\[ b_1 = \\color{red}{r} \\cdot \\frac{s_Y}{s_X} = \\color{red}{ \\frac {\\sum_{i=1}^n(X_i - \\bar{X})(Y_i-\\bar{Y})} {s_X s_Y (n-1)} } \\cdot \\frac{s_Y}{s_X} = \\frac { \\sum_{i=1}^n(X_i - \\bar{X})(Y_i-\\bar{Y})} {(n-1)~ \\color{gold}{s_X^2} } = \\frac { \\sum(X_i-\\bar{X})(Y_i-\\bar{Y}) } { \\color{blue}{\\sum(X_i-\\bar{X})^2} } = b_1 \\] where \\[ \\color{gold}{s_X^2} = \\frac{ \\color{blue}{\\sum (X_i-\\bar{X})^2} }{n-1} \\] If we regard \\((X_i-\\bar{X})/SSX\\) as a weight, it is thus clear that \\(X_i\\) that is farther away from \\(\\bar{X}\\) has stronger influence on \\(\\hat{\\beta}_1\\). In fact, if a singular data point has undue (i.e., overly large) influence on regression coefficients, it is regarded as an influential data point. It should be clear that outliers are mostly likely to become influential data points. \\[ b_1 = \\sum \\left( \\color{red}{ \\frac{(X_i-\\bar{X})}{SSX} } \\cdot (Y_i-\\bar{Y}) \\right) \\overset{*}{=} \\sum \\left( \\color{red}{ \\frac{(X_i-\\bar{X})}{SSX} } \\cdot Y_i \\right) = \\sum \\left( \\color{red}{w_i} \\cdot Y_i \\right) \\] Note: To see how \\(\\overset{*}{=}\\) is true, let \\(c\\) be a constant and we have \\[ \\sum (X_i - \\bar{X})(Y_i \\pm c) = \\sum (X_i - \\bar{X})Y_i \\pm \\sum (X_i - \\bar{X})c = \\sum (X_i - \\bar{X})Y_i \\pm c\\sum (X_i - \\bar{X}) = \\sum (X_i - \\bar{X})Y_i \\pm c \\cdot 0 = \\sum (X_i - \\bar{X})Y_i \\] Measurement error in \\(X\\) always shrinks the magnitude of \\(b_1\\) towards 0, whereas measurement error in \\(Y\\) does not affect \\(b_1\\). To note, measurement error is regarded as random noise. To see this, let \\(X^* = X+E\\), where \\(E\\) denotes random error. \\[ Cov(X^*,Y) = Cov(X+E,~Y) = Cov(X,Y) + Cov(E,Y) = Cov(X,Y) \\] \\[ Var(X^*) = Var(X+E) = Var(X) + Var(E) + 2Cov(X,E) = Var(X) + Var(E) \\] Note: Both \\(Cov(Y,E)=0\\) and \\(Cov(X,E)=0\\) are true, because random noise by definition does not co-vary with anything. As a result, we have \\[ b_1^* = \\frac{Cov(X^*,Y)}{Var(X^*)} = \\frac{Cov(X+E,~Y)}{Var(X+E)} = \\frac{Cov(X,Y)}{Var(X)+Var(E)} &lt; \\frac{Cov(X,Y)}{Var(X)} = b_1\\] R_Proof: Measurement Error noise = rnorm(length(x), mean=0, sd=sd(x)) nx = x + noise noise = rnorm(length(y), mean=0, sd=sd(y)) ny = y + noise dat = data.frame(dat, nx, ny) b4 = lm( y ~ nx, data=dat)$coefficients[2] b5 = lm(ny ~ x, data=dat)$coefficients[2] names = names(bs) bs = c(bs, b4, b5) names(bs) = c(names,&quot;noise_x&quot;,&quot;noise_y&quot;) bs ## c x cx cy noise_x noise_y ## 10.0000000 2.0090753 0.2009075 20.0907534 1.0048716 2.0124322 Note: The result from adding noise to Y changes slightly, because the cor(Y,E) in our sample is 0.0065184, which is not strictly 0 due to sampling randomness. 5.3.6 Residual Variance Residual variance (aka conditional variance) is given as follows. \\[ \\hat{\\sigma}^2 = MSR = \\frac{SSR}{DFR} = \\frac{\\sum R_i^2}{df} = \\frac{\\sum_{i=1}^n (Y_i-\\hat{Y_i})^2}{df} = \\frac{\\sum_{i=1}^n (Y_i - (b_0 + b_1X_i))^2}{n-2} \\] It is thus clear that RMSE (i.e., root of mean squares residual) is the residual standard deviation, aka conditional standard deviation. Notation-wise, for the sake of simplicity, we will use \\(s\\) to denote the residual standard deviation \\(\\hat{\\sigma}\\) and hence \\(s^2\\) to represent the residual variance \\(\\hat{\\sigma}^2\\). 5.3.7 Standard Error of the Slope Given the residual standard deviation, \\[ \\hat{\\sigma} = \\sqrt{\\frac{\\sum R_i^2}{df}} = \\sqrt{ \\frac{\\sum_{i=1}^n (Y_i-\\hat{Y_i})^2}{n-2} } = \\sqrt{ \\frac{\\sum_{i=1}^n (Y_i - (b_0 + b_1X_i))^2}{n-2} } \\] The standard errors of \\(b_1\\) and \\(b_0\\) are given as follows. \\[ \\hat{\\sigma}_{b_1}^2 = \\frac{\\hat{\\sigma}^2}{\\sum (X_i-\\bar{X})^2} = \\frac{\\hat{\\sigma}^2}{\\text{SSX}} = \\frac{\\hat{\\sigma}^2}{(n-1) Var(X)}\\] and \\[ \\hat{\\sigma}_{b_0}^2 = \\frac{\\hat{\\sigma}^2}{n} \\cdot \\frac{\\sum (X_i)^2}{\\sum (X_i-\\bar{X})^2} \\] It must be emphasized that we usually do not care about inference regarding \\(b_0\\). The standard error of \\(b_0\\) is provided here primarily for the sake of completeness rather than for any praticial value. It is clear that the standard error of \\(b_1\\) is directly proportional to the residual standard deviation. In other words, if the overall fit of the line is good, the RMSE (i.e., \\(\\hat{\\sigma}\\)) will be small and hence \\(\\hat{\\sigma}_{b_1}\\). When n is large, \\(\\hat{\\sigma}\\) would be small and \\(SSX\\) would be large, which drives \\(\\hat{\\sigma}_{b_1}\\) to become even smaller. All else being equal, greater intrinsic variation in X would result in a larger \\(SSX\\), which in turn produces a small \\(\\hat{\\sigma}_{b_0}\\). Results from surveys using a 1-5 Likert scale is a good example. Sometimes, the wording of a question is phrased in such a way that the responses to the question would only span a restricted range (e.g., registering exclusively positive ratings). This clearly leads to reduced intrinsic variation in X. 5.3.8 Effect Size and Standardized Coefficients Since the magnitude of \\(\\beta_0\\) and \\(\\beta_1\\) is subject to change with respect to the units of measurement adopted in Y and X. Therefore, it would be helpful to derive some kind of standardized coefficients such that sizes of the coefficients are invariant to the choice of measurement units. After standardizing both X and Y, the resultant coefficients are standardized coefficients, known as the beta coefficients. As shown previously, in simple OLS regression, standardized \\(b_1\\) is identical to the Pearson correlation \\(r\\). Beta coefficients are therefore used as an effect size measure in communicating the substantive practical significance of a linear relationship. Let \\(s_X\\) and \\(s_Y\\) be the standard deviations of X and Y, we have \\[ r = b_{\\text{standardized}} = \\frac{s_X}{s_Y} \\cdot b_{\\text{original}} \\] For certain problems, sometimes we would only want to standardize either X or Y, but not both of them. For example, when X is an ordinal variable measured on a 1-5 Likert scale. We might want to standardize only Y and claim that 1 point increase on the Likert scale would result in \\(b_1\\) standard deviations of increase in Y. 5.3.9 ANOVA for Regression HOWTO: how to add lines above and below the table? HOWTO: how to &quot;center&quot;&quot; content in a cell SS DF MS \\[\\mathbf{Total}~~\\] \\[~~\\sum(Y_i-\\bar{Y})^2~~\\] \\[~~n-1~~\\] \\[~~\\frac{1}{n-1} \\sum (Y_i-\\bar{Y})^2\\] \\[\\mathbf{Model}~~\\] \\[~~\\sum(\\hat{Y}_i-\\bar{Y})^2~~\\] \\[~~(1+p)-1~~\\] \\[~~\\frac{1}{p}\\sum (\\hat{Y}_i-\\bar{Y})^2\\] \\[\\mathbf{Residual}~~\\] \\[~~\\sum(Y_i - \\hat{Y}_i)^2~~\\] \\[~~n-(1+p)~~\\] \\[\\frac{1}{n-1-p}\\sum (Y_i-\\hat{Y}_i)^2\\] where \\(\\hat{Y}_i = b_0 + b_1X_i\\) and \\(p\\) is the number of features, i.e., the number of parameters minus 1 to exclude the intercept. \\[ R^2 = \\frac{\\text{SSM}}{\\text{SST}} = \\frac{ \\sum(\\hat{Y}_i-\\bar{Y})^2 }{ \\sum(Y_i-\\bar{Y})^2 } \\] and \\[ F = \\frac{MSM}{MSE} = \\frac{ \\frac{1}{p}\\sum (\\hat{Y}_i-\\bar{Y})^2 }{ \\frac{1}{n-1-p}\\sum (Y_i-\\hat{Y}_i)^2 } \\] where \\(F(p, n-1-p)\\) and \\(p=1\\) in the case of simple linear regression. TODO: check out p594 in IPS R_Proof: R-square and F-test mod = lm(mid3 ~ mid1, data=xdat) yhat = predict(mod) ybar = mean(xdat$mid3, na.rm=T) ssm = sum((yhat-ybar)^2, na.rm=T) sst = sum((xdat$mid3-ybar)^2, na.rm=T) r2 = ssm/sst r = cor(xdat$mid3, xdat$mid1, use=&quot;pairwise&quot;) # cor(na.omit(xdat$mid3), yhat) c(r2, r^2) ## [1] 0.5494976 0.5494976 5.4 Non-linear Relations 5.4.1 Log-transformations log-transformation is one way for handling non-linearity if the relation between IV and DV is indeed connected by a log-based relationship. log-transformation is commonly used for handling highly skewed variable by taknig advantage of the suppressive property of logarithm (e.g., \\(\\log_{10}(10)=1\\) and \\(\\log_{10}(100)=2\\)). As a result, many highly skewed variables, after log-transformations, would look much like a normal distribution. For simple linear regression, the linear model is represented as follows. \\[ Y = b_0 + b_1X + e \\] Take the derivative with respect to \\(X\\) on both sides, we have \\[ \\frac{dY}{dX} \\approx \\frac{\\Delta Y}{\\Delta X} = \\frac{d(b_0+b_1X)}{dX} = b_1 \\] and hence \\[ \\Delta Y = b_1 \\Delta X \\] In this case, the interpretation derived from a Calculus perspective readily agrees with that from an arithmetic perspective, i.e., when X increases by 1 unit (i.e., \\(\\Delta X=1\\)), Y would increase by an average of \\(b_1\\) (i.e., \\(\\Delta Y = b_1\\)). This situation would not be true under log-transformations. 5.4.1.1 Linear-log Model: Log-transformation on X Now, let’s consider log-transforming X. The linear model would become \\[ Y = b_0 + b_1 \\log(X) + e \\] To note, the log notation in this textbook would always denote the natural logarithm with base \\(e \\approx 2.718\\), which in some textbooks is represented by ln. Simple rules of arithmetic dictate that \\(b_1\\) is the expected units of change in Y given a one-unit increase in log(X), which means \\[ \\log(X) + 1 = \\log(X) + \\log(e) = \\log(eX) \\] This result makes it clear that adding 1 to log(X) is equivalent of multipling X by \\(e \\approx 2.718\\). In other words, Y is expected to change by \\(b_1\\) units when X increases by approximately 1.72 times or 172%. Let’s look at this from a Calculus perspective by taking the derivative with respect to X on the both sides. \\[ \\frac{dY}{dX} \\approx \\frac{\\Delta Y}{\\Delta X} = \\frac{d(b_0+b_1 \\log(X))}{dX} = b_1 \\cdot \\frac{1}{X} \\] and hence \\[ \\Delta Y = b_1 \\cdot \\frac{\\Delta X}{X} = \\frac{b_1}{100} \\cdot \\left( 100 \\cdot \\frac{\\Delta X}{X} \\right) = \\frac{b_1}{100} \\cdot p \\] where \\(\\Delta X/X\\) represents the proportion of change in X and hence \\(100 \\cdot \\Delta X/X\\) denotes the percentage of change in X. In other words, 1% increase in X, namely \\((100 \\cdot \\Delta X / X) = 1\\), would translate into \\(b_1/100\\) units of increase in \\(Y\\) or one hundredth of Y. To note, this Calculus-inspired percentage interpretation is only true (i.e., the \\(\\approx\\) sign would hold), if \\(\\Delta X\\) is small (e.g., 1%). Now let’s re-examine this interpretation from the arithmetic perspective, where a p percent increase in X would be \\(X \\cdot (100+p)/100 = X \\cdot (1+p/100)\\). Let \\(Y_0\\) and \\(Y_p\\) be the values of Y corresponding to X and X with p percent increase. \\[ Y_0 = b_0 + b_1 \\log(X) + e \\] and \\[ Y_p = b_0 + b_1 \\log \\left( X \\cdot \\frac{100+p}{100} \\right) + e \\] and \\[ \\frac{\\Delta X}{X} = \\frac{X \\cdot (1+p/100) - X}{X} = \\frac{p}{100} \\] or \\[ 100 \\cdot \\frac{\\Delta X}{X} = p \\] Hence, a p percent increase in X would result in \\[ \\Delta Y = Y_p - Y_0 = b_1 \\left( \\log \\left( (1+\\frac{p}{100}) \\cdot X \\right) - \\log(X) \\right) = b_1 \\log \\left( 1 + \\frac{p}{100} \\right) \\] This result indicates that p = 1 or 1% increase in X would result in \\(b_1 \\log(1.01) \\approx b_1 \\cdot 0.01\\). This interpretation is scalable up to about 10% (i.e., the exact arithmetic and the approximate Calculus interpretations agree only up to about 10%), since p = 10 or 10% increase in X would result in \\(b_1 \\log(1.10) \\approx b_1 \\cdot 0.10\\). The following table shows the degree of deviations under and beyond 10% change in X. proportion increase in X percentage increase in X increase in Y \\(\\Delta X/X = p/100\\) \\(100 \\cdot \\Delta X/X = p\\) \\(b_1 \\log(1+p/100)\\) 0.01 1% \\(b_1 \\cdot 0.01\\) 0.05 5% \\(b_1 \\cdot 0.05\\) 0.10 10% \\(b_1 \\cdot 0.10\\) 0.20 20% \\(b_1 \\cdot 0.18\\) 0.50 50% \\(b_1 \\cdot 0.41\\) 1.00 100% \\(b_1 \\cdot 0.69\\) 1.72 172% \\(b_1 \\cdot 1.00\\) TODO: add a shiny app to supplement the table! curve(log, from=0.01, to=4, ylim=c(-4,4)) curve(x-1, add=T, col=2) abline(v=0, h=0, lty=2) In summary, given the discussions above, the following statements are equivalent (or almost equivalent). When log(X) increases by 1 unit, Y would change by an average of \\(b_1\\). When X is multiplied by 2.72, Y would change by an average of \\(b_1\\). When X increases by 172%, Y would change by an average of \\(b_1\\). When X increases by p%, Y would change by an average of \\(b_1 \\log(1+p/100)\\). When X increases by 1%, Y would change by an average of \\(b_1/100\\). When X increases by 10%, Y would change by an average of \\(b_1/10\\). 5.4.1.2 Log-linear Model: Log-transformation on Y \\[ \\log(Y) = b_0 + b_1X + e \\] Take the derivative with respect to X on both sides, we have \\[ \\frac{1}{Y} \\cdot \\frac{dY}{dX} \\approx \\frac{1}{Y} \\cdot \\frac{\\Delta Y}{\\Delta X} = b_1 \\] and hence \\[ \\frac{\\Delta Y}{Y} = b_1 \\cdot \\Delta X \\] Replacing proportion of change by percentage of change gives \\[ 100 \\cdot \\frac{\\Delta Y}{Y} = 100 \\cdot b_1 \\cdot \\Delta X \\] which leads to the interpretation that 1 unit increase in X would result in \\(100 \\cdot b_1\\) percent increase in Y. Similar to the previous case, this Calculus interpretation is only true when \\(b_1\\) is less than 0.1 or 10%. Let’s examine the precise arithmetic interpretation. From \\(\\log(Y) = b_0 + b_1X + e\\), we have \\[ Y = e^{(b_0 + b_1X + e)} \\] Let \\(Y_0\\), \\(Y_1\\), and \\(Y_p\\) be the values of Y corresponding to X, X+1, and X+p. We have \\[ Y_0 = e^{(b_0 + b_1X + e)} \\] and \\[ Y_1 = e^{(b_0 + b_1(X+1) + e)} = e^{(b_0 + b_1X + e)} \\cdot e^{b_1} = Y_0 \\cdot e^{b_1} \\] which implies that 1 unit increase in X would result in Y multiplied by \\(e^{b_1}\\). \\[ \\frac{\\Delta Y}{Y_0} = \\frac{Y_1 - Y_0}{Y_0} = \\frac{Y_0 \\cdot e^{b_1} - Y_0}{Y_0} = \\frac{Y_0 \\cdot (e^{b_1} - 1)}{Y_0} = (e^{b_1}-1) \\] Replacing proportion of change by percentage of change gives \\[ 100 \\cdot \\frac{\\Delta Y}{Y_0} = 100 \\cdot (e^{b_1}-1) \\] The Taylor expansion of \\(e^b\\) gives the following result \\[ e^b = 1 + b + \\frac{b^2}{2!} + \\frac{b^3}{3!} + \\cdots+\\frac{b^n}{n!} \\] When b is small (e.g. \\(b \\le 0.1\\)), \\(e^b \\approx 1 + b\\) and hence \\(e^b - 1 \\approx b\\). In other words, when \\(b_1\\) is less than 0.1 (as suggested by the table below), the exact arithmetic interpretation agrees with the approximate Calculus interpretation. b = c(c(0.01,0.05,0.1,0.15), seq(0.2, 1, by=0.1)) p = round(exp(b)-1, 2) mat = cbind(b,p) colnames(mat) = c(&quot;b&quot;,&quot;exp(b)-1&quot;) mat ## b exp(b)-1 ## [1,] 0.01 0.01 ## [2,] 0.05 0.05 ## [3,] 0.10 0.11 ## [4,] 0.15 0.16 ## [5,] 0.20 0.22 ## [6,] 0.30 0.35 ## [7,] 0.40 0.49 ## [8,] 0.50 0.65 ## [9,] 0.60 0.82 ## [10,] 0.70 1.01 ## [11,] 0.80 1.23 ## [12,] 0.90 1.46 ## [13,] 1.00 1.72 In summary, When X increases by 1, log(Y) would increase by \\(b_1\\). When X increases by 1, Y would be multiplied by \\(e^{b_1}\\). When X increases by c, Y would be multiplied by \\(e^{c b_1}\\). When X increases by 1, Y would increase by \\(100 \\cdot (e^{b_1}-1)\\) percent. When X increases by 1 and when \\(b_1 \\le 0.1\\), Y would increase by \\(100 \\cdot b_1\\) percent. 5.4.1.3 Log-log Model: log-transformations on X and Y \\[ \\log(Y) = b_0 + b_1 \\log(X) + e \\] Taking the derivative with respect to X on both sides gives \\[ \\frac{1}{Y} \\cdot \\frac{dY}{dX} \\approx \\frac{1}{Y} \\cdot \\frac{\\Delta Y}{\\Delta X} = \\frac{b_1}{X} \\] and hence \\[ 100 \\cdot \\frac{\\Delta Y}{Y} = b_1 \\cdot \\left( 100 \\cdot \\frac{\\Delta X}{X} \\right) \\] which implies that 1 percent increase in X would result in \\(b_1\\) percent increase in Y. With the log-log model, the coefficient \\(b_1\\) is called elasticity in economics. In summary, When log(X) increase by 1, log(Y) would increase by \\(b_1\\). When X is multiplied by \\(e\\), the expected value of Y would increase by multiplying \\(e^{b_1}\\). When X increases by p percent, the expected value of Y would increase by multiplying \\(e^{c b_1}\\) where \\(c = \\log(1+p/100)\\). 5.4.2 Quadratic Relations 5.5 The Assumptions of Simple Linear Regression When the OLS assumptions hold, OLS estimators have been shown to be unbiased and have minimum standard error of among all unbiased linear estimators, a property known as Best Linear Unbiased Estimators (BLUE). \\[ Y_i|X_i \\overset{id}{\\sim} N(\\beta_0 + \\beta_1X_i, \\sigma^2) \\] 5.6 Assumption Diagnostics "],
["multiple-linear-regression.html", "6 Multiple Linear Regression 6.1 Regression Models - The Base Form 6.2 The Ordinary Least-Squres Method 6.3 Standard Errors of Regression Coefficients 6.4 \\(R^2\\) and ANOVA for Multiple Regression 6.5 Regression Models Extended - Categorical Features 6.6 Regression Models Extended - Interaction Effects 6.7 Regression Models Extended - Nonlinear Relations 6.8 Model Comparision &amp; Evaluation", " 6 Multiple Linear Regression Multiple linear regression, or simply multiple regression, is a powerful statistical modeling technique that can handle independent variables of assorted data types, i.e., ditchotomous, nominal, continuous complex relations among the IVs, e.g., nonlinear relations and interaction effects 6.1 Regression Models - The Base Form 6.1.1 Model Specification with Population Parameters In a simple linear regression model, we use only one independent variable X to predict a continuous dependent variable Y and the prediction model has two parameters, i.e. the intercept and the slope. In this chapter, instead of usign one IV, we will use p IVs, i.e. \\(X_1, X_2, \\dots, X_p\\). As a result, with the population parameters in place, the multiple linear regression model is \\[ Y_i = \\mu_i + \\varepsilon_i = \\beta_0 + \\beta_1X_{1i} + \\beta_2X_{2i} + \\cdots + \\beta_pX_{pi} + \\varepsilon_i \\] where \\(i = 1,2,\\dots,n\\) and \\(\\mu_i \\equiv E(Y_i|\\boldsymbol{X}_i)\\) is commonly referred to as the mean response model or the prediction model \\[ E(Y_i|\\boldsymbol{X}_i) \\equiv \\mu_i = \\beta_0 + \\beta_1X_{1i} + \\beta_2X_{2i} + \\cdots + \\beta_pX_{pi} \\] and \\(\\varepsilon_i\\) is called the error and \\[\\varepsilon_i \\overset{iid}{\\text{~}} N(0,\\sigma^2)\\] To note, the population parameters of this linear model are \\(\\beta_0, \\beta_1, \\dots, \\beta_p\\) and \\(\\sigma\\). 6.1.2 Model Specification with Sample Parameters Accordingly, using the sample parameters, the multiple linear regression model is \\[ Y_i = \\hat{Y}_i + e_i = b_0 + b_1X_{1i} + b_2X_{2i} + \\cdots + b_pX_{pi} + e_i \\] where \\(i = 1,2,\\dots,n\\) and the prediction model is \\[ \\hat{Y}_i = b_0 + b_1X_{1i} + b_2X_{2i} + \\cdots + b_pX_{pi} \\] and \\(e_i\\) is called the residual and \\[ e_i \\overset{iid}{\\sim} N(0,s^2)\\] To note, \\(b_0, b_1, \\dots, b_p\\) and \\(s^2\\) are parameters estimated from a given sample to approximate the unknown and constant population parameters \\(\\beta_0, \\beta_1, \\dots, \\beta_p\\) and \\(\\sigma\\). 6.2 The Ordinary Least-Squres Method Similar to the case of simple linear regression, the ordinary least squares (OLS) method can also be applied to estimate sample parameters for multiple linear regression. \\[ SSR = \\sum_{i=1}^n R_i^2 = \\sum_{i=1}^n (Y_i-\\hat{Y}_i)^2 = \\sum_{i=1}^n (Y_i - (b_0 + b_1X_{1i} + b_2X_{2i} + \\cdots + b_pX_{pi}))^2 \\] The solutions for \\(b_j\\), \\(j = 1,2,\\dots,p\\), should minimize \\(SSR\\), hence the name least-squares method. 6.2.1 Interpretations of Regression Coefficients Given the following mean response model, \\[ \\hat{Y}_i = b_0 + b_1X_{1i} + b_2X_{2i} + \\cdots + b_pX_{pi} \\] intercept \\(b_0\\): The expected value (i.e., mean) of Y when all IVs are zero. slope \\(b_i\\): Holding other IVs constant, increase in \\(X_i\\) by 1 unit corresponds to an average of \\(b_i\\) units of increase in Y. 6.2.2 Residual Variance The residual variance (aka conditional variance) is given as follows. \\[ s^2 = MSR = \\frac{\\sum R_i^2}{df} = \\frac{\\sum_{i=1}^n (Y_i-\\hat{Y_i})^2}{df} = \\frac{\\sum_{i=1}^n (Y_i - (b_0 + b_1X_{1i} + b_2X_{2i} + \\cdots + b_pX_{pi}))^2}{n-(1+p)} \\] It should be clear that \\(1+p\\) is the number of parameters required to estimate \\(\\hat{Y}_i\\) (i.e., the degrees of freedom we have to spend or give up for fitting the model), which equals \\(p\\) slope parameters plus 1 for the intercept. The denominator \\(n-(1+p)\\) is the degrees of freedom that are left for us after fitting the model. As a result, \\(s\\) is called the residual standard deviation or conditional standard deviation or RMSE. 6.3 Standard Errors of Regression Coefficients As explained previously, we are usually only interested in the standard error of a slope. Let \\(b_j\\) denote the slope parameter for the jth feature \\(X_j\\) where \\(j = 1,2,\\dots,p\\). We have \\[ s_{b_j}^2 = \\frac{s^2}{\\text{SSX}_j} \\cdot \\text{VIF}_j = \\frac{s^2}{(n-1) Var(X_j)} \\cdot \\text{VIF}_j \\] where \\(s^2\\) is the residual variance, \\(\\text{SSX}_j = \\sum_{i=1}^m (X_{ij}-\\bar{X}_j)^2\\) is the sum of squares for \\(X_j\\), and \\(\\text{VIF}_j\\) is an important concept called variance inflation factor. 6.3.1 Variance Inflation Factor To compute VIF for \\(X_j\\), use the following equation. \\[ \\text{VIF}_j = \\frac{1}{1-R_j^2} \\] where \\(R_j^2\\) is the R-squared of regressing \\(X_j\\) on all the other IVs except for \\(X_j\\). To be clear, the VIF of \\(X_j\\) can be computed in three steps. Step 1: For \\(X_1,X_2,\\dots,X_p\\), regress \\(Y_j\\) on the rest of the IVs: There are only \\(p-1\\) features and \\(Y\\) is completely ignored. \\[ X_j = a_0 + a_1X_1 + a_2X_2 + \\cdots + a_pX_p \\] Step 2: Compute the \\(R^2\\) for this regression model, which results in \\(R_j^2\\). Step 3: Compute VIF based on the formula given above. 6.3.2 Significance of VIF Remember that \\(R^2\\) is interpreted as the proportion of variance explained by the IVs and \\(1-R^2\\) is the proportion of unexplained variance. It should be clear that when \\(X_j\\) highly correlates with other features, \\(R_j^2\\) would be high and \\(1-R_j^2\\) would be low and hence \\(VIF_j\\) tends to be high. In fact, when \\(X_j\\) does not correlate with any other features, i.e., \\(R_j^2 = 0\\), we will have \\(VIF_j = 1\\), which would give the smallest possible standard error for \\(b_j\\), as shown in the following equation. To note, this is the same formula for the standard error of \\(b_j\\) in simple linear regression. \\[ s_{b_j}^2 = \\frac{s^2}{\\text{SSX}_j} \\cdot \\text{VIF}_j = \\frac{s^2}{\\text{SSX}_j} \\] In contrast, if \\(X_j\\) highly correlates with some other features such that the \\(R_j^2 = 0.90\\), i.e., 90% of the variance in \\(X_j\\) can be explained by the other features, then \\(VIF_j = 10\\). In other words, the standard error of \\(b_j\\) derived from multiple linear regression would be inflated 10 times as compared to the standard error of \\(b_j\\) that we would have got from a simple linear regression model where \\(X_j\\) is the sole predictor. 6.3.3 Multicollinearity This previous example illustrates how the term VIF gets its name variance inflation factor and the phenonemon of variance inflation due to presence of highly correlated featuers is called multicollinearity. As a rule of thumb, multicollinearity is usually not regarded as an issue unless VIF is greater than 10. Another point worth noticing is that, since in most situations, a given feature will always correlate with some other features, IVs or features in the context of multiple linear regression are also called covariates (since features co-vary), which is particularly true for continuous features. 6.3.4 Influencing Factors on Standard Errors With the meaning of VIF explained, we can now discuss the factors affecting the standard error of the slope parameter \\(b_j\\) for the covariate \\(X_j\\). \\[ s_{b_j}^2 = \\frac{s^2}{\\text{SSX}_j} \\cdot \\text{VIF}_j = \\frac{s^2}{(n-1) Var(X_j)} \\cdot \\frac{1}{1-R_j^2} \\] \\(s^2\\): Better model fit leads to smaller standard errors, or greater scatter in data around the regression line leads to more uncertainties in slope parameter estimations. \\(n\\): Larger sample size results in smaller standard errors. \\(Var(X_j)\\): Greater instrinsic variability in a covariate yields smaller standard errors. \\(R_j^2\\): Features that highly correlate with other features might associate with large standard errors. 6.4 \\(R^2\\) and ANOVA for Multiple Regression When dealing with simple linear regression, we have already studied how to derive the ANOVA table, compute the \\(R^2\\) and perform the \\(F\\) test. The same results also hold here, which will not be repeated except for one point. The formula for computing \\(R^2\\) is \\[ R^2 = \\frac{\\text{SSM}}{\\text{SST}} = \\frac{ \\sum(\\hat{Y}_i-\\bar{Y})^2 }{ \\sum(Y_i-\\bar{Y})^2 } \\] In the context of multiple regression, the statistic \\(R^2\\) is called the coefficient of determination of the linear model, and the square root of \\(R^2\\) is called the multiple correlation coefficient, which interestingly enough is the correlation between observed values \\(Y_i\\) and predicted values \\(\\hat{Y}_i\\). R_Proof: Multiple Correlation Coefficient tdat = na.omit(xdat[c(&quot;mid3&quot;,&quot;satmath&quot;,&quot;satreading&quot;,&quot;gpa&quot;)]) mod1 = lm(mid3 ~ satmath + satreading + gpa, data=tdat) yhat = predict(mod1) ybar = mean(tdat$mid3, na.rm=T) ssm = sum((yhat-ybar)^2, na.rm=T) sst = sum((tdat$mid3-ybar)^2, na.rm=T) r2 = ssm/sst r = cor(tdat$mid3, yhat) c(sqrt(r2), r) ## [1] 0.07316646 0.07316646 6.5 Regression Models Extended - Categorical Features 6.5.1 Dummy Variables A dummy variable has only two unique levels. For comparison purposes, we would designate one level as the reference level and code it as 0, whereas the other level is coded as 1. Since 1 is usually used to indicate the presence of something of interest, a dummy variable is also called a indicator variable. When a dummy variable is the only feature in a linear regression model, it gives identical results to a two-sample t-test. To see how this is true, let’s use a concrete example, where we regress final exam scores on gender to see if males outperformed females in a Chemistry class. Let us use M to indicate a dummy variable, where M=0 for females and M=1 for males. The prediction model would be as follows. \\[ \\hat{Y}_i = b_0 + b_1M_i \\] Three points can be inferred from this equation: When \\(M_i = 0\\), the group average for females is \\(\\hat{Y}_i=b_0\\). When \\(M_i = 1\\), the group average for males is \\(\\hat{Y}_i=b_0+b_1\\). The interpretation of \\(b_1\\) is that 1 unit increase in \\(M_i\\), i.e., changing from female to male, \\(\\hat{Y}_i\\) on average would increase by \\(b_1\\). Putting together, it should be clear that (a) \\(b_0\\) is the group mean for females, (b) \\(b_0+b_1\\) is the group mean for males, and (c) \\(b_1\\) is the difference in group means. R_Proof: Regression vs Two-sample T-test # two-sample t-test with equal variance tt = t.test(mid3 ~ gender, data=xdat, var.equal=TRUE) # simple linear regression with a dummy feature mod = lm(mid3 ~ gender, data=xdat) modsum = summary(mod) modsum$coefficients ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 98.426230 3.720447 26.455485 1.282478e-65 ## genderMale 8.830913 6.161646 1.433207 1.534421e-01 c(diff(tt$estimate), tt$statistic, tt$p.value) ## mean in group Male t ## 8.8309133 -1.4332070 0.1534421 r2 = modsum$r.squared r = cor(xdat$mid3, as.numeric(xdat$gender)-1, use=&quot;pair&quot;) c(r2, r^2) ## [1] 0.01069533 0.01069533 6.5.2 Nominal Variables In regression analysis, a nominal variable is routinely converted into a set of dummy features, designate one level/feature as the reference level, and put the rest of the levels/features into the regression model. To illustrate this, let’s regress final exam scores on year to examine if students from different years performed differently. There are three sessions (i.e., years) in the dataset. Let’s use S1, S2, and S3 to denote the first, second, and third sessions, and designate S1 as the reference level. The prediction model would be as follows. \\[ \\hat{Y}_i = b_0 + b_1S_{2i} + b_2S_{3i} \\] To note, since the dummy feature S1 is used as the reference level, it is absent as a feature from the equation. Moreover, the three dummy variables are mutually exlucsive, e.g., when S1=1, S2 and S3 have to be zero. Three points can be inferred from this equation: When \\(S_{1i} = 1\\), the group average for students from the first session is \\(\\hat{Y}_i=b_0\\). When \\(S_{2i} = 1\\), the group average for students from the second session is \\(\\hat{Y}_i=b_0+b_1\\). When \\(S_{3i} = 1\\), the group average for students from the third session is \\(\\hat{Y}_i=b_0+b_2\\). Putting together, it should be clear that (a) \\(b_1\\) is the difference in group means between the second and first sessions, and (b) \\(b_2\\) is the difference in group means between the third and first sessions. If there are more sessions up to p levels, then \\(b_3,b_4,\\dots,b_p\\) would be difference in group means between each of these levels and the first session. Everything is compared to the first session, which is why it is termed as the reference level or base level or reference group. When a nominal variable is converted into a set of dummies and used as the only feature in a linear regression model, it gives identical results to the one-way ANOVA. R_Proof: Regression vs ANOVA # one-way ANOVA aovmod = aov(mid3 ~ session, data=xdat) aovsum = summary(aovmod) aovsum ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## session 2 114 56.8 0.033 0.967 ## Residuals 189 324206 1715.4 ## 8 observations deleted due to missingness # simple linear regression with a nominal feature mod = lm(mid3 ~ session, data=xdat) anova(mod) ## Analysis of Variance Table ## ## Response: mid3 ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## session 2 114 56.81 0.0331 0.9674 ## Residuals 189 324206 1715.38 modsum = summary(mod) modsum ## ## Call: ## lm(formula = mid3 ~ session, data = xdat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -86.620 -32.910 -0.779 32.221 89.380 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 100.571 5.535 18.171 &lt;2e-16 *** ## sessionSS10 2.049 8.058 0.254 0.800 ## sessionSS11 1.208 7.112 0.170 0.865 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 41.42 on 189 degrees of freedom ## (8 observations deleted due to missingness) ## Multiple R-squared: 0.0003503, Adjusted R-squared: -0.01023 ## F-statistic: 0.03312 on 2 and 189 DF, p-value: 0.9674 modsum$fstatistic ## value numdf dendf ## 0.0331181 2.0000000 189.0000000 Let’s create a nominal feature with three levels (i.e., ‘a’,‘b’,‘c’) and use the model.matrix() function in R to generate a set of dummy features. xvec = gl(3, 2, labels=c(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;)) tdat = data.frame(xvec) tdat ## xvec ## 1 a ## 2 a ## 3 b ## 4 b ## 5 c ## 6 c model.matrix(~xvec, data=tdat) ## (Intercept) xvecb xvecc ## 1 1 0 0 ## 2 1 0 0 ## 3 1 1 0 ## 4 1 1 0 ## 5 1 0 1 ## 6 1 0 1 ## attr(,&quot;assign&quot;) ## [1] 0 1 1 ## attr(,&quot;contrasts&quot;) ## attr(,&quot;contrasts&quot;)$xvec ## [1] &quot;contr.treatment&quot; Notice that (a) a column of 1s is automatically created, and (b) the first level ‘a’ is missing from the results. model.matrix(~xvec-1, data=tdat) ## xveca xvecb xvecc ## 1 1 0 0 ## 2 1 0 0 ## 3 0 1 0 ## 4 0 1 0 ## 5 0 0 1 ## 6 0 0 1 ## attr(,&quot;assign&quot;) ## [1] 1 1 1 ## attr(,&quot;contrasts&quot;) ## attr(,&quot;contrasts&quot;)$xvec ## [1] &quot;contr.treatment&quot; 6.6 Regression Models Extended - Interaction Effects The interaction effects are also called moderation effects, which are critically important in regression analysis. It is the idea that the effect of X on Y is moderated by Z. For example, we might be interested in the effect of study time X on course performance Y. It is conceivable that the way study time affects exam results might depend on whether the time is spent willingly Z. If students are forced to spend a lot of time studying, then the more time spent may or may not translate to stronger performance. If students are willingly investing time in a course, more time spent should have a much stronger effect on exam performance. 6.6.1 Dummy-Dummy Interactions In our dataset, let’s consider the effect of repeater status R (i.e., repeater=1) on final exam scores Y moderated by gender Male (i.e., male=1). \\[ \\hat{Y}_i = b_0 + b_1\\cdot\\text{R} + b_2\\cdot\\text{Male} + b_3\\cdot\\text{R}\\cdot\\text{Male} \\] For \\(Male=0\\) (i.e., females), we have \\[ \\hat{Y}_i = b_0 + b_1\\cdot\\text{R} \\] which shows (a) that the average score for female non-repeaters is \\(b_0\\) and (b) that females enjoyed an average of \\(b_1\\) points increase when switching from non-repeater to repeater status. For \\(Male=1\\) (i.e., males), we have \\[ \\hat{Y}_i = b_0 + b_1\\cdot\\text{R} + b_2 + b_3\\cdot\\text{R} = (b_0+b_2) + (b_1+b_3)\\cdot\\text{R}\\] which shows (a) that the average score for male non-repeaters was \\(b_2\\) points higher than that for female non-repeaters and (b) that for the same switch from non-repeater to repeater status, males enjoyed an additional \\(b_3\\) points increase as compared to the score gain for females making the same status switch. Now let’s fit a linear regression model and examine the coefficients. mod = lm(mid3 ~ repeater*gender, data=xdat) modsum = summary(mod) modsum ## ## Call: ## lm(formula = mid3 ~ repeater * gender, data = xdat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -91.109 -26.337 2.514 24.455 80.976 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 111.024 3.861 28.752 &lt; 2e-16 *** ## repeaterRepeater -41.537 7.012 -5.924 1.47e-08 *** ## genderMale 13.085 6.516 2.008 0.0461 * ## repeaterRepeater:genderMale -7.613 11.381 -0.669 0.5043 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 35.6 on 188 degrees of freedom ## (8 observations deleted due to missingness) ## Multiple R-squared: 0.2653, Adjusted R-squared: 0.2536 ## F-statistic: 22.63 on 3 and 188 DF, p-value: 1.488e-12 The visualize a dummy-continuous interaction effect, fit a simple linear regression model between X and Y conditioned on Z. newdat = expand.grid(mod$xlevels) newdat$yhat = predict(mod, newdat) yrange = range(newdat$yhat) yrange = yrange + c(-1,1)*diff(yrange)*0.1 plot(1:2, newdat$yhat[1:2], ylim=yrange, type=&quot;o&quot;, xaxt=&quot;n&quot;, ylab=&quot;Final Exam Scores&quot;, xlab=NA) axis(1, at=1:2, labels=levels(newdat[,1])) points(1:2, newdat$yhat[3:4], type=&quot;o&quot;, pty=2, lty=2) 6.6.2 Dummy-Continuous Interactions In our dataset, let’s consider the effect of high school GPA X on final exam scores Y moderated by repeater status R. \\[ \\hat{Y}_i = b_0 + b_1\\cdot\\text{GPA} + b_2\\cdot\\text{R} + b_3\\cdot\\text{GPA}\\cdot\\text{R} \\] For \\(R=0\\) (i.e., non-repeaters), we have \\[ \\hat{Y}_i = b_0 + b_1\\cdot\\text{GPA} \\] which shows that (a) a non-repeater with 0 highschool GPA got an average of \\(b_0\\) points in the final exam, and (b) that the average effect of 1 point increase in GPA on exam performance is \\(b_1\\) points for non-repeaters. For \\(R=1\\) (i.e., repeaters), we have \\[ \\hat{Y}_i = b_0 + b_1\\cdot\\text{GPA} + b_2 + b_3\\cdot\\text{GPA} = (b_0+b_2) + (b_1+b_3)\\cdot\\text{GPA}\\] which shows that (a) repeaters with 0 highschool GPA would on average get \\(b_2\\) points higher on the final exam, and (b) that the average effect of 1 point increase in GPA on exam performance is \\(b_1+b_3\\) for repeaters. Hence, for the same 1 point increase in GPA, repeaters enjoyed an extra \\(b_3\\) points increase in the final exam as compared to the \\(b_1\\) points gain in final scores for non-repeaters. The visualize a dummy-continuous interaction effect, fit a simple linear regression model between X and Y conditioned on Z. car::scatterplot(mid3 ~ gpa*repeater, data=xdat) In R, interaction effect between X and Z is represented by X*Z. To note, whenever an interaction effect (e.g., X*Z) is added to a linear model, the component variables involved (e.g., X and Z) will be automatically added to the linear model as well. In other words, the following two linear models give identical results. lm(mid3 ~ repeater*gpa, data=xdat) lm(mid3 ~ repeater + gpa + repeater*gpa, data=xdat) Now let’s fit a linear regression model and examine the coefficients. mod = lm(mid3 ~ repeater*gpa, data=xdat) modsum = summary(mod) modsum ## ## Call: ## lm(formula = mid3 ~ repeater * gpa, data = xdat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -83.477 -26.153 4.577 24.992 77.077 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 129.451 15.230 8.499 5.79e-15 *** ## repeaterRepeater -99.104 29.478 -3.362 0.000938 *** ## gpa -4.738 5.107 -0.928 0.354678 ## repeaterRepeater:gpa 18.394 9.668 1.903 0.058618 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 35.67 on 188 degrees of freedom ## (8 observations deleted due to missingness) ## Multiple R-squared: 0.2624, Adjusted R-squared: 0.2507 ## F-statistic: 22.3 on 3 and 188 DF, p-value: 2.135e-12 6.6.3 Continuous-Continuous Interactions Let’s consider the effect of highschool GPA on final exam scores Y moderated by perceived usefulness of the course U. \\[ \\hat{Y}_i = b_0 + b_1\\cdot\\text{GPA} + b_2 \\cdot U + b_3\\cdot\\text{GPA} \\cdot U \\] When \\(U=0\\), we have \\[ \\hat{Y}_i = b_0 + b_1\\cdot\\text{GPA} \\] When \\(U=1\\), we have \\[ \\hat{Y}_i = (b_0 +b_2) + (b_1+b_3)\\cdot\\text{GPA} \\] At this point, the interpretations of the coefficients are very similar to those we have encountered in the case of dummy-continuous interactions. To be noted, if we use \\(U\\) and \\(U+1\\) instead of using \\(U=0\\) and \\(U=1\\), we would end up with very similar interpretations: (a) 1 unit increase in perceived course usefulness would result in \\(b_0\\) points increase in final exam scores for students with 0 GPA, and (b) 1 unit increase in perceived course usefulness would enhance the effect of GPA by \\(b_3\\) points per 1 unit increase in GPA. mod = lm(mid3 ~ gpa*utcourse, data=xdat) modsum = summary(mod) modsum ## ## Call: ## lm(formula = mid3 ~ gpa * utcourse, data = xdat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -88.790 -31.017 1.257 30.518 91.027 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 48.479 57.140 0.848 0.397 ## gpa 13.523 19.300 0.701 0.484 ## utcourse 13.042 12.221 1.067 0.287 ## gpa:utcourse -3.354 4.090 -0.820 0.413 ## ## Residual standard error: 41.6 on 171 degrees of freedom ## (25 observations deleted due to missingness) ## Multiple R-squared: 0.01214, Adjusted R-squared: -0.005192 ## F-statistic: 0.7004 on 3 and 171 DF, p-value: 0.553 xs = quantile(xdat$gpa, na.rm=TRUE) ys = quantile(xdat$utcourse, na.rm=TRUE, probs=c(0.25, 0.75)) newdat = expand.grid(xs,ys) colnames(newdat) = c(&quot;gpa&quot;,&quot;utcourse&quot;) newdat$yhat = predict(mod, newdat) yrange = range(newdat$yhat) yrange = yrange + c(-1,1)*diff(yrange)*0.1 plot(xdat$gpa, xdat$mid3,xlab=&quot;GPA&quot;, ylab=&quot;Final Exam Scores&quot;) points(newdat$gpa[1:5], newdat$yhat[1:5], type=&quot;o&quot;, pch=2) points(newdat$gpa[6:10], newdat$yhat[6:10], type=&quot;o&quot;, pch=5, lty=2) legend(&quot;topleft&quot;, legend=c(&quot;25% Quantile&quot;,&quot;75% Quantile&quot;), bty=&quot;n&quot;, pch=c(2,5), lty=c(1,2)) To note, do not plot only the fitted lines without the actual data points. plot(newdat$gpa[1:5], newdat$yhat[1:5], ylim=yrange, type=&quot;o&quot;, xlab=&quot;GPA&quot;, ylab=&quot;predicted&quot;, pch=2) points(newdat$gpa[6:10], newdat$yhat[6:10], type=&quot;o&quot;, pch=5, lty=2) 6.7 Regression Models Extended - Nonlinear Relations 6.8 Model Comparision &amp; Evaluation 6.8.1 F-test for Nested Models 6.8.2 Adjusted R-Squared 6.8.3 Information Criteria "]
]
