<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Quantitative Research Methods for Education</title>
  <meta name="description" content="This is the 1st Edition">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="Quantitative Research Methods for Education" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is the 1st Edition" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Quantitative Research Methods for Education" />
  
  <meta name="twitter:description" content="This is the 1st Edition" />
  

<meta name="author" content="Wenliang He">



  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="statistical-inference.html">
<link rel="next" href="linear-regression.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="libs/htmlwidgets-1.2/htmlwidgets.js"></script>
<script src="libs/d3-3.3.8/d3.min.js"></script>
<script src="libs/dagre-0.4.0/dagre-d3.min.js"></script>
<link href="libs/mermaid-0.3.0/dist/mermaid.css" rel="stylesheet" />
<script src="libs/mermaid-0.3.0/dist/mermaid.slim.min.js"></script>
<link href="libs/DiagrammeR-styles-0.2/styles.css" rel="stylesheet" />
<script src="libs/chromatography-0.1/chromatography.js"></script>
<script src="libs/DiagrammeR-binding-1.0.0/DiagrammeR.js"></script>
<link href="libs/datatables-css-0.0.0/datatables-crosstalk.css" rel="stylesheet" />
<script src="libs/datatables-binding-0.4/datatables.js"></script>
<link href="libs/dt-core-1.10.16/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="libs/dt-core-1.10.16/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="libs/dt-core-1.10.16/js/jquery.dataTables.min.js"></script>
<link href="libs/crosstalk-1.0.0/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk-1.0.0/js/crosstalk.min.js"></script>


<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#install"><i class="fa fa-check"></i><b>1.1</b> Software Installation</a><ul>
<li class="chapter" data-level="1.1.1" data-path="index.html"><a href="index.html#install-r"><i class="fa fa-check"></i><b>1.1.1</b> Install R</a></li>
<li class="chapter" data-level="1.1.2" data-path="index.html"><a href="index.html#install-rstudio"><i class="fa fa-check"></i><b>1.1.2</b> Install RStudio</a></li>
<li class="chapter" data-level="1.1.3" data-path="index.html"><a href="index.html#install-r-packages"><i class="fa fa-check"></i><b>1.1.3</b> Install R Packages</a></li>
<li class="chapter" data-level="1.1.4" data-path="index.html"><a href="index.html#update-r-and-r-packages"><i class="fa fa-check"></i><b>1.1.4</b> Update R and R Packages</a></li>
<li class="chapter" data-level="1.1.5" data-path="index.html"><a href="index.html#register-a-github-account"><i class="fa fa-check"></i><b>1.1.5</b> Register a GitHub account</a></li>
<li class="chapter" data-level="1.1.6" data-path="index.html"><a href="index.html#install-git"><i class="fa fa-check"></i><b>1.1.6</b> Install Git</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#introduction-to-r"><i class="fa fa-check"></i><b>1.2</b> Introduction to R</a><ul>
<li class="chapter" data-level="1.2.1" data-path="index.html"><a href="index.html#r-basics"><i class="fa fa-check"></i><b>1.2.1</b> Essential Basics</a></li>
<li class="chapter" data-level="1.2.2" data-path="index.html"><a href="index.html#atomic-dtype"><i class="fa fa-check"></i><b>1.2.2</b> Atomic Data Types</a></li>
<li class="chapter" data-level="1.2.3" data-path="index.html"><a href="index.html#object-types"><i class="fa fa-check"></i><b>1.2.3</b> R Object Types</a></li>
<li class="chapter" data-level="1.2.4" data-path="index.html"><a href="index.html#indexing"><i class="fa fa-check"></i><b>1.2.4</b> Indexing</a></li>
<li class="chapter" data-level="1.2.5" data-path="index.html"><a href="index.html#factor"><i class="fa fa-check"></i><b>1.2.5</b> Factors</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#introduction-to-r-markdown"><i class="fa fa-check"></i><b>1.3</b> Introduction to R Markdown</a><ul>
<li class="chapter" data-level="1.3.1" data-path="index.html"><a href="index.html#basic-markdown"><i class="fa fa-check"></i><b>1.3.1</b> Basic Markdown</a></li>
<li class="chapter" data-level="1.3.2" data-path="index.html"><a href="index.html#r-markdown"><i class="fa fa-check"></i><b>1.3.2</b> R Markdown</a></li>
<li class="chapter" data-level="1.3.3" data-path="index.html"><a href="index.html#mathematical-symbols-in-latex"><i class="fa fa-check"></i><b>1.3.3</b> Mathematical Symbols in Latex</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#introduction-to-git-github"><i class="fa fa-check"></i><b>1.4</b> Introduction to Git &amp; GitHub</a></li>
<li class="chapter" data-level="1.5" data-path="index.html"><a href="index.html#introduction-to-ggplot2"><i class="fa fa-check"></i><b>1.5</b> Introduction to ggplot2</a></li>
<li class="chapter" data-level="1.6" data-path="index.html"><a href="index.html#introduction-to-shiny"><i class="fa fa-check"></i><b>1.6</b> Introduction to Shiny</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="the-fundamentals-of-statistics.html"><a href="the-fundamentals-of-statistics.html"><i class="fa fa-check"></i><b>2</b> The Fundamentals of Statistics</a><ul>
<li class="chapter" data-level="2.1" data-path="the-fundamentals-of-statistics.html"><a href="the-fundamentals-of-statistics.html#why-studying-statistics"><i class="fa fa-check"></i><b>2.1</b> Why Studying Statistics</a><ul>
<li class="chapter" data-level="2.1.1" data-path="the-fundamentals-of-statistics.html"><a href="the-fundamentals-of-statistics.html#understanding-the-world"><i class="fa fa-check"></i><b>2.1.1</b> Understanding the World</a></li>
<li class="chapter" data-level="2.1.2" data-path="the-fundamentals-of-statistics.html"><a href="the-fundamentals-of-statistics.html#science-vs.philosophy"><i class="fa fa-check"></i><b>2.1.2</b> Science vs. Philosophy</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="the-fundamentals-of-statistics.html"><a href="the-fundamentals-of-statistics.html#the-big-picture"><i class="fa fa-check"></i><b>2.2</b> The Big Picture</a><ul>
<li class="chapter" data-level="2.2.1" data-path="the-fundamentals-of-statistics.html"><a href="the-fundamentals-of-statistics.html#central-themes-in-statistics"><i class="fa fa-check"></i><b>2.2.1</b> Central Themes in Statistics</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="the-fundamentals-of-statistics.html"><a href="the-fundamentals-of-statistics.html#the-fundamentals-of-statistics-1"><i class="fa fa-check"></i><b>2.3</b> The Fundamentals of Statistics</a><ul>
<li class="chapter" data-level="2.3.1" data-path="the-fundamentals-of-statistics.html"><a href="the-fundamentals-of-statistics.html#storing-data-in-tables"><i class="fa fa-check"></i><b>2.3.1</b> Storing Data in Tables</a></li>
<li class="chapter" data-level="2.3.2" data-path="the-fundamentals-of-statistics.html"><a href="the-fundamentals-of-statistics.html#variable-types"><i class="fa fa-check"></i><b>2.3.2</b> Variable Types</a></li>
<li class="chapter" data-level="2.3.3" data-path="the-fundamentals-of-statistics.html"><a href="the-fundamentals-of-statistics.html#exploratory-data-analysis-checking-distributions"><i class="fa fa-check"></i><b>2.3.3</b> Exploratory Data Analysis: Checking Distributions</a></li>
<li class="chapter" data-level="2.3.4" data-path="the-fundamentals-of-statistics.html"><a href="the-fundamentals-of-statistics.html#normal-distributions"><i class="fa fa-check"></i><b>2.3.4</b> Normal Distributions</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="statistical-inference.html"><a href="statistical-inference.html"><i class="fa fa-check"></i><b>3</b> Statistical Inference</a><ul>
<li class="chapter" data-level="3.1" data-path="statistical-inference.html"><a href="statistical-inference.html#random-variable"><i class="fa fa-check"></i><b>3.1</b> Random Variable</a><ul>
<li class="chapter" data-level="3.1.1" data-path="statistical-inference.html"><a href="statistical-inference.html#iid---independently-and-identically-distributed"><i class="fa fa-check"></i><b>3.1.1</b> IID - Independently and Identically Distributed</a></li>
<li class="chapter" data-level="3.1.2" data-path="statistical-inference.html"><a href="statistical-inference.html#random-variable-vs.case"><i class="fa fa-check"></i><b>3.1.2</b> Random Variable vs. Case</a></li>
<li class="chapter" data-level="3.1.3" data-path="statistical-inference.html"><a href="statistical-inference.html#properties-of-a-random-variable"><i class="fa fa-check"></i><b>3.1.3</b> Properties of a Random Variable</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="statistical-inference.html"><a href="statistical-inference.html#central-limit-theorem"><i class="fa fa-check"></i><b>3.2</b> Central Limit Theorem</a><ul>
<li class="chapter" data-level="3.2.1" data-path="statistical-inference.html"><a href="statistical-inference.html#presenting-central-limit-theorem"><i class="fa fa-check"></i><b>3.2.1</b> Presenting Central Limit Theorem</a></li>
<li class="chapter" data-level="3.2.2" data-path="statistical-inference.html"><a href="statistical-inference.html#understanding-central-limit-theorem"><i class="fa fa-check"></i><b>3.2.2</b> Understanding Central Limit Theorem</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="statistical-inference.html"><a href="statistical-inference.html#statistical-inference-1"><i class="fa fa-check"></i><b>3.3</b> Statistical Inference</a><ul>
<li class="chapter" data-level="3.3.1" data-path="statistical-inference.html"><a href="statistical-inference.html#the-diamond-expert"><i class="fa fa-check"></i><b>3.3.1</b> The Diamond Expert</a></li>
<li class="chapter" data-level="3.3.2" data-path="statistical-inference.html"><a href="statistical-inference.html#hypothesis-testing"><i class="fa fa-check"></i><b>3.3.2</b> Hypothesis Testing</a></li>
<li class="chapter" data-level="3.3.3" data-path="statistical-inference.html"><a href="statistical-inference.html#hypothesis-testing-with-unknown-variance"><i class="fa fa-check"></i><b>3.3.3</b> Hypothesis Testing with Unknown Variance</a></li>
<li class="chapter" data-level="3.3.4" data-path="statistical-inference.html"><a href="statistical-inference.html#examples-of-hypothesis-testing"><i class="fa fa-check"></i><b>3.3.4</b> Examples of Hypothesis Testing</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="common-statistical-tests.html"><a href="common-statistical-tests.html"><i class="fa fa-check"></i><b>4</b> Common Statistical Tests</a><ul>
<li class="chapter" data-level="4.1" data-path="common-statistical-tests.html"><a href="common-statistical-tests.html#dummy-x---continuous-y"><i class="fa fa-check"></i><b>4.1</b> Dummy (X) - Continuous (Y)</a><ul>
<li class="chapter" data-level="4.1.1" data-path="common-statistical-tests.html"><a href="common-statistical-tests.html#two-sample-t-test"><i class="fa fa-check"></i><b>4.1.1</b> Two-sample T-Test</a></li>
<li class="chapter" data-level="4.1.2" data-path="common-statistical-tests.html"><a href="common-statistical-tests.html#statistical-power"><i class="fa fa-check"></i><b>4.1.2</b> Statistical Power</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="common-statistical-tests.html"><a href="common-statistical-tests.html#dummy-x---dummy-y"><i class="fa fa-check"></i><b>4.2</b> Dummy (X) - Dummy (Y)</a><ul>
<li class="chapter" data-level="4.2.1" data-path="common-statistical-tests.html"><a href="common-statistical-tests.html#basics"><i class="fa fa-check"></i><b>4.2.1</b> Basics</a></li>
<li class="chapter" data-level="4.2.2" data-path="common-statistical-tests.html"><a href="common-statistical-tests.html#difference-in-proportions"><i class="fa fa-check"></i><b>4.2.2</b> Difference in Proportions</a></li>
<li class="chapter" data-level="4.2.3" data-path="common-statistical-tests.html"><a href="common-statistical-tests.html#relative-risks"><i class="fa fa-check"></i><b>4.2.3</b> Relative Risks</a></li>
<li class="chapter" data-level="4.2.4" data-path="common-statistical-tests.html"><a href="common-statistical-tests.html#odds-ratio"><i class="fa fa-check"></i><b>4.2.4</b> Odds Ratio</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="common-statistical-tests.html"><a href="common-statistical-tests.html#categorical-x---dummy-y"><i class="fa fa-check"></i><b>4.3</b> Categorical (X) - Dummy (Y)</a><ul>
<li class="chapter" data-level="4.3.1" data-path="common-statistical-tests.html"><a href="common-statistical-tests.html#basics-1"><i class="fa fa-check"></i><b>4.3.1</b> Basics</a></li>
<li class="chapter" data-level="4.3.2" data-path="common-statistical-tests.html"><a href="common-statistical-tests.html#chi-squared-test"><i class="fa fa-check"></i><b>4.3.2</b> Chi-squared Test</a></li>
<li class="chapter" data-level="4.3.3" data-path="common-statistical-tests.html"><a href="common-statistical-tests.html#g-square-test"><i class="fa fa-check"></i><b>4.3.3</b> G-square Test</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="common-statistical-tests.html"><a href="common-statistical-tests.html#categorical-x---continuous-y"><i class="fa fa-check"></i><b>4.4</b> Categorical (X) - Continuous (Y)</a><ul>
<li class="chapter" data-level="4.4.1" data-path="common-statistical-tests.html"><a href="common-statistical-tests.html#anova"><i class="fa fa-check"></i><b>4.4.1</b> ANOVA</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="linear-regression.html"><a href="linear-regression.html"><i class="fa fa-check"></i><b>5</b> Linear Regression</a><ul>
<li class="chapter" data-level="5.1" data-path="linear-regression.html"><a href="linear-regression.html#table-of-contents"><i class="fa fa-check"></i><b>5.1</b> Table of Contents</a></li>
<li class="chapter" data-level="5.2" data-path="linear-regression.html"><a href="linear-regression.html#pearson-product-moment-correlation-coefficient"><i class="fa fa-check"></i><b>5.2</b> Pearson Product-Moment Correlation Coefficient</a><ul>
<li class="chapter" data-level="5.2.1" data-path="linear-regression.html"><a href="linear-regression.html#definition"><i class="fa fa-check"></i><b>5.2.1</b> Definition</a></li>
<li class="chapter" data-level="5.2.2" data-path="linear-regression.html"><a href="linear-regression.html#properties-of-pearson-correlation"><i class="fa fa-check"></i><b>5.2.2</b> Properties of Pearson Correlation</a></li>
<li class="chapter" data-level="5.2.3" data-path="linear-regression.html"><a href="linear-regression.html#limitations-of-pearson-correlation"><i class="fa fa-check"></i><b>5.2.3</b> Limitations of Pearson Correlation</a></li>
<li class="chapter" data-level="5.2.4" data-path="linear-regression.html"><a href="linear-regression.html#other-issues-on-the-usage-of-pearson-correlation"><i class="fa fa-check"></i><b>5.2.4</b> Other Issues on the Usage of Pearson Correlation</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="linear-regression.html"><a href="linear-regression.html#simple-linear-regression"><i class="fa fa-check"></i><b>5.3</b> Simple Linear Regression</a><ul>
<li class="chapter" data-level="5.3.1" data-path="linear-regression.html"><a href="linear-regression.html#independent-variable-from-categorical-to-continuous"><i class="fa fa-check"></i><b>5.3.1</b> Independent Variable: From Categorical to Continuous</a></li>
<li class="chapter" data-level="5.3.2" data-path="linear-regression.html"><a href="linear-regression.html#fitting-a-straight-line-to-data"><i class="fa fa-check"></i><b>5.3.2</b> Fitting a Straight Line to Data</a></li>
<li class="chapter" data-level="5.3.3" data-path="linear-regression.html"><a href="linear-regression.html#the-least-squares-method"><i class="fa fa-check"></i><b>5.3.3</b> The Least-Squares Method</a></li>
<li class="chapter" data-level="5.3.4" data-path="linear-regression.html"><a href="linear-regression.html#interpretations-of-regression-coefficients"><i class="fa fa-check"></i><b>5.3.4</b> Interpretations of Regression Coefficients</a></li>
<li class="chapter" data-level="5.3.5" data-path="linear-regression.html"><a href="linear-regression.html#understanding-the-estimators"><i class="fa fa-check"></i><b>5.3.5</b> Understanding the Estimators</a></li>
<li class="chapter" data-level="5.3.6" data-path="linear-regression.html"><a href="linear-regression.html#residual-variance"><i class="fa fa-check"></i><b>5.3.6</b> Residual Variance</a></li>
<li class="chapter" data-level="5.3.7" data-path="linear-regression.html"><a href="linear-regression.html#standard-error-of-the-slope"><i class="fa fa-check"></i><b>5.3.7</b> Standard Error of the Slope</a></li>
<li class="chapter" data-level="5.3.8" data-path="linear-regression.html"><a href="linear-regression.html#effect-size-and-standardized-coefficients"><i class="fa fa-check"></i><b>5.3.8</b> Effect Size and Standardized Coefficients</a></li>
<li class="chapter" data-level="5.3.9" data-path="linear-regression.html"><a href="linear-regression.html#anova-for-regression"><i class="fa fa-check"></i><b>5.3.9</b> ANOVA for Regression</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="linear-regression.html"><a href="linear-regression.html#non-linear-relations"><i class="fa fa-check"></i><b>5.4</b> Non-linear Relations</a><ul>
<li class="chapter" data-level="5.4.1" data-path="linear-regression.html"><a href="linear-regression.html#log-transformations"><i class="fa fa-check"></i><b>5.4.1</b> Log-transformations</a></li>
<li class="chapter" data-level="5.4.2" data-path="linear-regression.html"><a href="linear-regression.html#quadratic-relations"><i class="fa fa-check"></i><b>5.4.2</b> Quadratic Relations</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="linear-regression.html"><a href="linear-regression.html#the-assumptions-of-simple-linear-regression"><i class="fa fa-check"></i><b>5.5</b> The Assumptions of Simple Linear Regression</a></li>
<li class="chapter" data-level="5.6" data-path="linear-regression.html"><a href="linear-regression.html#assumption-diagnostics"><i class="fa fa-check"></i><b>5.6</b> Assumption Diagnostics</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html"><i class="fa fa-check"></i><b>6</b> Multiple Linear Regression</a><ul>
<li class="chapter" data-level="6.1" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#regression-models---the-base-form"><i class="fa fa-check"></i><b>6.1</b> Regression Models - The Base Form</a><ul>
<li class="chapter" data-level="6.1.1" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#model-specification-with-population-parameters"><i class="fa fa-check"></i><b>6.1.1</b> Model Specification with Population Parameters</a></li>
<li class="chapter" data-level="6.1.2" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#model-specification-with-sample-parameters"><i class="fa fa-check"></i><b>6.1.2</b> Model Specification with Sample Parameters</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#the-ordinary-least-squres-method"><i class="fa fa-check"></i><b>6.2</b> The Ordinary Least-Squres Method</a><ul>
<li class="chapter" data-level="6.2.1" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#interpretations-of-regression-coefficients-1"><i class="fa fa-check"></i><b>6.2.1</b> Interpretations of Regression Coefficients</a></li>
<li class="chapter" data-level="6.2.2" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#residual-variance-1"><i class="fa fa-check"></i><b>6.2.2</b> Residual Variance</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#standard-errors-of-regression-coefficients"><i class="fa fa-check"></i><b>6.3</b> Standard Errors of Regression Coefficients</a><ul>
<li class="chapter" data-level="6.3.1" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#variance-inflation-factor"><i class="fa fa-check"></i><b>6.3.1</b> Variance Inflation Factor</a></li>
<li class="chapter" data-level="6.3.2" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#significance-of-vif"><i class="fa fa-check"></i><b>6.3.2</b> Significance of VIF</a></li>
<li class="chapter" data-level="6.3.3" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#multicollinearity"><i class="fa fa-check"></i><b>6.3.3</b> Multicollinearity</a></li>
<li class="chapter" data-level="6.3.4" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#influencing-factors-on-standard-errors"><i class="fa fa-check"></i><b>6.3.4</b> Influencing Factors on Standard Errors</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#r2-and-anova-for-multiple-regression"><i class="fa fa-check"></i><b>6.4</b> <span class="math inline">\(R^2\)</span> and ANOVA for Multiple Regression</a></li>
<li class="chapter" data-level="6.5" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#regression-models-extended---categorical-features"><i class="fa fa-check"></i><b>6.5</b> Regression Models Extended - Categorical Features</a><ul>
<li class="chapter" data-level="6.5.1" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#dummy-variables"><i class="fa fa-check"></i><b>6.5.1</b> Dummy Variables</a></li>
<li class="chapter" data-level="6.5.2" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#nominal-variables"><i class="fa fa-check"></i><b>6.5.2</b> Nominal Variables</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#regression-models-extended---interaction-effects"><i class="fa fa-check"></i><b>6.6</b> Regression Models Extended - Interaction Effects</a><ul>
<li class="chapter" data-level="6.6.1" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#dummy-dummy-interactions"><i class="fa fa-check"></i><b>6.6.1</b> Dummy-Dummy Interactions</a></li>
<li class="chapter" data-level="6.6.2" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#dummy-continuous-interactions"><i class="fa fa-check"></i><b>6.6.2</b> Dummy-Continuous Interactions</a></li>
<li class="chapter" data-level="6.6.3" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#continuous-continuous-interactions"><i class="fa fa-check"></i><b>6.6.3</b> Continuous-Continuous Interactions</a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#regression-models-extended---nonlinear-relations"><i class="fa fa-check"></i><b>6.7</b> Regression Models Extended - Nonlinear Relations</a></li>
<li class="chapter" data-level="6.8" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#model-comparision-evaluation"><i class="fa fa-check"></i><b>6.8</b> Model Comparision &amp; Evaluation</a><ul>
<li class="chapter" data-level="6.8.1" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#f-test-for-nested-models"><i class="fa fa-check"></i><b>6.8.1</b> F-test for Nested Models</a></li>
<li class="chapter" data-level="6.8.2" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#adjusted-r-squared"><i class="fa fa-check"></i><b>6.8.2</b> Adjusted R-Squared</a></li>
<li class="chapter" data-level="6.8.3" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#information-criteria"><i class="fa fa-check"></i><b>6.8.3</b> Information Criteria</a></li>
</ul></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Quantitative Research Methods for Education</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="common-statistical-tests" class="section level1">
<h1><span class="header-section-number">4</span> Common Statistical Tests</h1>
<table>
<thead>
<tr class="header">
<th><span class="math inline">\(_X \ ^Y\)</span></th>
<th>dummy</th>
<th>continuous</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>dummy</td>
<td>(2) <strong>difference in proportions</strong> or <strong>odds ratio</strong></td>
<td>(1) <strong>two-sample t-test</strong></td>
</tr>
<tr class="even">
<td>categorical</td>
<td>(3) <strong>chi-squared test</strong></td>
<td>(4) <strong>ANOVA</strong></td>
</tr>
<tr class="odd">
<td>continuous</td>
<td>(6) <strong>logistic regression</strong></td>
<td>(5) <strong>linear regression</strong></td>
</tr>
</tbody>
</table>
<div id="dummy-x---continuous-y" class="section level2">
<h2><span class="header-section-number">4.1</span> Dummy (X) - Continuous (Y)</h2>
<p><strong>RCT</strong> or <strong>Randomized Controlled Trial</strong> is the prime example of an Experiment Design, where the independent variable X is a dummy variable.</p>
<div id="two-sample-t-test" class="section level3">
<h3><span class="header-section-number">4.1.1</span> Two-sample T-Test</h3>
<p>Given two samples</p>
<p><span class="math display">\[Y_{T_i} \sim \forall \left( \mu_T, \sigma_T^2 \right) 
~~~~ and ~~~~
Y_{C_j} \sim \forall \left( \mu_C, \sigma_C^2 \right) \]</span> where <span class="math inline">\(i = 1,2,3,...,n_T\)</span> and <span class="math inline">\(j = 1,2,3,...,n_C\)</span></p>
<p>and</p>
<p><span class="math display">\[\bar{Y}_T = \frac{1}{n_T} \sum Y_{T_i} 
~~~~ and ~~~~
\bar{Y}_C = \frac{1}{n_C} \sum Y_{C_i}\]</span></p>
<p>The <strong>estimand</strong> and <strong>estimator</strong> that are of interest to us are as follows.</p>
<ul>
<li>estimand: <span class="math inline">\(\Delta = \mu_T - \mu_C\)</span></li>
<li>estimator: <span class="math inline">\(\hat{\Delta} = \bar{Y}_T - \bar{Y}_C\)</span>.</li>
</ul>
<p>The estimand is a constant (because it’s a population parameter) and the estimator is a random variable. Hence, we are immediately interested in the properties of the estimator.</p>
<blockquote>
<p><strong>Question</strong>: What are the properties of the estimator <span class="math inline">\(\hat{\Delta}\)</span>?</p>
</blockquote>
<blockquote>
<p><strong>Answers</strong>:</p>
<ul>
<li><span class="math inline">\(E(\hat{\Delta}) = \mu_T - \mu_C\)</span></li>
<li><span class="math inline">\(Var(\hat{\Delta}) = \frac{\sigma_T^2}{n_T} + \frac{\sigma_C^2}{n_C}\)</span></li>
<li><span class="math inline">\(shape(\hat{\Delta}) = N\)</span></li>
</ul>
<p>or equivalently</p>
<p><span class="math display">\[\hat{\Delta} \sim N \left( \mu_T - \mu_C, ~ \frac{\sigma_T^2}{n_T} + \frac{\sigma_C^2}{n_C} \right)\]</span></p>
</blockquote>
<p>Proof of the properties of <span class="math inline">\(\hat{\Delta}\)</span> is as follows.</p>
<blockquote>
<p><strong>Proof</strong>:</p>
<p><code>Prerequisites</code></p>
<p><span class="math display">\[Var(X+Y) = Var(X) + 2Cov(X,Y) + Var(Y)\]</span></p>
<p><span class="math display">\[Var(X-Y) = Var(X) - 2Cov(X,Y) + Var(Y)\]</span></p>
<p>When X and Y are independent, i.e., <span class="math inline">\(X \perp Y\)</span>, we have <span class="math inline">\(Cov(X,Y)=0\)</span> and hence</p>
<p><span class="math display">\[Var(X \pm Y) = Var(X) + Var(Y)\]</span></p>
<p><code>Proof</code></p>
<p>Considering the CLT, <span class="math display">\[\bar{Y}_{T} \sim N \left( \mu_T, ~ \frac{\sigma_T^2}{n_T} \right) 
~~~~ and ~~~~
\bar{Y}_{C} \sim N \left( \mu_C, ~ \frac{\sigma_C^2}{n_C} \right) \]</span></p>
<p>we have the following</p>
<p><span class="math display">\[E(\hat{\Delta}) = E(\bar{Y}_T - \bar{Y}_C) 
= E(\bar{Y}_T) - E(\bar{Y}_C) = \mu_T - \mu_C = \Delta\]</span> and <span class="math display">\[Var(\hat{\Delta}) = Var(\bar{Y}_T - \bar{Y}_C) 
= Var(\bar{Y}_T) + Var(\bar{Y}_C) 
= \frac{\sigma_T^2}{n_T} + \frac{\sigma_C^2}{n_C}\]</span></p>
</blockquote>
<p>Now, given what we know about the properties of <span class="math inline">\(\hat{\Delta}\)</span> and the CLT, we have</p>
<p><span class="math display">\[z = \frac{ \hat{\Delta} - \Delta }{ \text{se}(\Delta) } 
= \frac{ (\bar{Y}_T - \bar{Y}_C) - (\mu_T - \mu_C) }{ \sqrt{ \frac{\sigma_T^2}{n_T} + \frac{\sigma_C^2}{n_C} } }\]</span></p>
<p>At this point, <span class="math inline">\(\sigma_T\)</span> and <span class="math inline">\(\sigma_C\)</span> are unknown to us. As a result, we need to use sample variances to replace population variances and the z-statistic would thus become a t-statistic.</p>
<p><span class="math display">\[t = \frac{ (\bar{Y}_T - \bar{Y}_C) - (\mu_T - \mu_C) }{ \sqrt{ \frac{s_T^2}{n_T} + \frac{s_C^2}{n_C} } }\]</span> with <span class="math inline">\(\text{df} = \text{df}_T + \text{df}_C = (n_T-1) + (n_C-1)\)</span></p>
<p>To conduct hypothesis test, we have</p>
<ol style="list-style-type: decimal">
<li>propose the hypothesis</li>
</ol>
<p><span class="math inline">\(H_0: \Delta = \mu_T - \mu_C = 0\)</span>, where <span class="math inline">\(H_0\)</span> is called the <strong>null hypothesis</strong></p>
<p><span class="math inline">\(H_a: \Delta = \mu_T - \mu_C \ne 0\)</span>, where <span class="math inline">\(H_a\)</span> is called the <strong>alternative hypothesis</strong></p>
<ol start="2" style="list-style-type: decimal">
<li>compute a probability under the hypothesis</li>
</ol>
<p><span class="math display">\[t = \frac{ \bar{Y}_T - \bar{Y}_C }{ \sqrt{ \frac{s_p^2}{n_T} + \frac{s_p^2}{n_C} } }\]</span></p>
<p>Under the null hypothesis that the two samples are from the same population, it is natural for us to combine the two sample varinances.</p>
<p><span class="math display">\[s_p^2 = \frac{\text{df}_C}{\text{df}_C+\text{df}_T} s_C^2 + 
\frac{\text{df}_T}{\text{df}_C+\text{df}_T} s_T^2 
= \frac{(n_C-1)}{(n_C-1) + (n_T-1)}s_C^2 + 
\frac{(n_T-1)}{(n_C-1) + (n_T-1)}s_T^2\]</span></p>
<p>We can then compute the p-value using the following R code.</p>
<pre><code>p_value = 2 * ( 1 - pt(t, df=(nt-1)+(nc-1)) )</code></pre>
<ol start="3" style="list-style-type: decimal">
<li>make a decision</li>
</ol>
<p>Make a decison based on pre-determined confidence level, i.e. <span class="math inline">\(1 - \alpha\)</span>, where <span class="math inline">\(\alpha=0.05\)</span> by default.</p>
</div>
<div id="statistical-power" class="section level3">
<h3><span class="header-section-number">4.1.2</span> Statistical Power</h3>
<p>(Coursera - Johns Hopkins - DSJH6-10 Statistical Inference/Week 4/Module 11:Power)(Zhu-Qiuxia)</p>
<div class="figure">
<img src="images/statsTests_power.png" />

</div>
<ul>
<li><strong>type I error rate</strong> <span class="math inline">\(\alpha\)</span></li>
<li><strong>specificity</strong> <span class="math inline">\(1-\alpha\)</span></li>
<li><strong>type II error rate</strong> <span class="math inline">\(\beta\)</span></li>
<li><strong>sensitivity</strong> or <strong>power</strong> <span class="math inline">\(1-\beta\)</span></li>
</ul>
<p>Note: CV stands for critical value.</p>
<blockquote>
<p><strong>Question</strong>: Assuming that (a) the treatment and control groups have identical sample size <span class="math inline">\(n\)</span>, (b) <span class="math inline">\(\alpha = 0.05\)</span>, and (c) <span class="math inline">\(\beta = 0.80\)</span>, what is the smallest <span class="math inline">\(n\)</span> required to detect a true difference of size <span class="math inline">\(\Delta\)</span>?</p>
</blockquote>
<blockquote>
<p><strong>Answer</strong>:</p>
<p><span class="math display">\[n = \frac{2 s_p^2 ( z_{(1-\frac{\alpha}{2})} - z_\beta )^2}{\Delta^2}
\approx \frac{16s_p^2}{\Delta^2}\]</span> where <span class="math display">\[s_p^2 = \frac{(n_C-1)}{(n_C-1) + (n_T-1)}s_C^2 + 
\frac{(n_T-1)}{(n_C-1) + (n_T-1)}s_T^2\]</span></p>
</blockquote>
<p>The proof for the formula above is as follows.</p>
<blockquote>
<p><strong>Proof</strong>:</p>
<p><span class="math display">\[z_{(1-\frac{\alpha}{2})} = \frac{\text{CV}-0}{\text{se}} 
~~~~ \rightarrow ~~~~
\text{CV} = z_{(1-\frac{\alpha}{2})} \cdot \text{se}\]</span> where <span class="math inline">\(\alpha=0.05\)</span> by default and hence <span class="math inline">\(z_{(1-\alpha/2)} = 1.96\)</span>.</p>
<p><span class="math display">\[z_\beta = \frac{\text{CV} - \Delta}{\text{se}}
~~~~ \rightarrow ~~~~
\text{CV} = \Delta + z_\beta \cdot \text{se}\]</span> where <span class="math inline">\(\beta=0.80\)</span> by default and hence <span class="math inline">\(z_\beta = -0.84\)</span>.</p>
<p>Through equating <span class="math inline">\(\text{CV}\)</span>, we have</p>
<p><span class="math display">\[z_{(1-\frac{\alpha}{2})} \cdot \text{se} = \Delta + z_\beta \cdot \text{se}\]</span> and hence</p>
<p><span class="math display">\[(z_{(1-\frac{\alpha}{2})} - z_\beta) \cdot \text{se} = \Delta 
~~~~ \rightarrow ~~~~~
\text{se} = \frac{\Delta}{z_{(1-\frac{\alpha}{2})} - z_\beta}\]</span></p>
<p>Moreover, with the assumption that treatment and control groups have the same sample size <span class="math inline">\(n\)</span> and using the pooled variance, we have</p>
<p><span class="math display">\[\text{se}^2 = \frac{s_T^2}{n_T} + \frac{s_C^2}{n_C} = \frac{2s_p^2}{n}\]</span></p>
<p><span class="math display">\[\text{se}^2 = \left( \frac{\Delta}{z_{(1-\frac{\alpha}{2})} - z_\beta} \right)^2 = \frac{2s_p^2}{n}\]</span> and hence <span class="math display">\[n = \frac{2 s_p^2 ( z_{(1-\frac{\alpha}{2})} - z_\beta )^2}{\Delta^2}
= \frac{2 \cdot 7.84 \cdot s_p^2}{\Delta^2} \approx \frac{16s_p^2}{\Delta^2} 
= \left( \frac{4s_p}{\Delta} \right)^2\]</span> where <span class="math inline">\(( z_{(1-\frac{\alpha}{2})} - z_\beta )^2 = (1.96 + 0.84)^2 = 7.84\)</span>.</p>
</blockquote>
<p>Given the formula above, we can answer the following questions.</p>
<blockquote>
<p><strong>Question</strong>: What factors can result in a smaller sample size?</p>
</blockquote>
<blockquote>
<p><strong>Answers</strong>:</p>
<ul>
<li>larger <span class="math inline">\(\alpha\)</span></li>
<li>larger <span class="math inline">\(\beta\)</span></li>
<li>smaller <span class="math inline">\(s_p^2\)</span></li>
<li>larger <span class="math inline">\(\Delta\)</span></li>
</ul>
</blockquote>
<p>By rearranging the formula above, we have</p>
<p><span class="math display">\[z_\beta = z_{(1-\frac{\alpha}{2})} - \sqrt{ \frac{n\Delta^2}{2s_p^2} }\]</span></p>
<blockquote>
<p><strong>Question</strong>: What factors would result in larger statistical power?</p>
</blockquote>
<blockquote>
<p><strong>Answers</strong>:</p>
<ul>
<li>larger <span class="math inline">\(n\)</span></li>
<li>smaller <span class="math inline">\(s_p^2\)</span></li>
<li>larger <span class="math inline">\(\alpha\)</span></li>
<li>larger <span class="math inline">\(\Delta\)</span></li>
</ul>
<p>Tip: To answer this question, think about the graph not the formula.</p>
</blockquote>
</div>
</div>
<div id="dummy-x---dummy-y" class="section level2">
<h2><span class="header-section-number">4.2</span> Dummy (X) - Dummy (Y)</h2>
<div id="basics" class="section level3">
<h3><span class="header-section-number">4.2.1</span> Basics</h3>
<div id="bernoulli-distribution" class="section level4">
<h4><span class="header-section-number">4.2.1.1</span> Bernoulli Distribution</h4>
<p>A random variable Y following a Bernoullie distribution can be expressed as follows.</p>
<p><span class="math display">\[Y \sim Bern(\pi)\]</span> where Y = 1 with probability <span class="math inline">\(\pi\)</span> and Y = 0 with probability <span class="math inline">\(1-\pi\)</span>.</p>
<p>Based on the definitions of E(Y) and Var(Y), we have</p>
<p><span class="math display">\[E(Y) = \sum Y \cdot P(Y) = 1 \cdot \pi + 0 \cdot (1-\pi) = \pi\]</span> and</p>
<p><span class="math display">\[Var(Y) = E(Y - EY)^2 = \sum (Y - EY)^2 \cdot P(Y) 
= (1-\pi)^2 \cdot \pi + (0-\pi)^2 \cdot (1-\pi)\]</span> <span class="math display">\[= (\pi^2 - 2\pi + 1)\pi + \pi^2 (1-\pi) = \pi - \pi^2 = \pi(1-\pi)\]</span></p>
<p>Hence the properties of the random variable Y can be expressed as <span class="math display">\[Y \sim Bern(\pi, \pi(1-\pi))\]</span></p>
<p>Given the CLT, we therefore have <span class="math display">\[\bar{Y} \sim N \left( \pi, \frac{\pi(1-\pi)}{n} \right)\]</span></p>
</div>
<div id="inference-with-one-variance" class="section level4">
<h4><span class="header-section-number">4.2.1.2</span> Inference with One Variance</h4>
<p><code>TODO</code>: Contents to be added!</p>
</div>
</div>
<div id="difference-in-proportions" class="section level3">
<h3><span class="header-section-number">4.2.2</span> Difference in Proportions</h3>
<table>
<thead>
<tr class="header">
<th align="left"><span class="math inline">\(_X \ ^Y\)</span></th>
<th align="center">Outcome=No (0)</th>
<th align="center">Outcome=Yes (1)</th>
<th align="center">column total</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><strong>Control (0)</strong></td>
<td align="center">a</td>
<td align="center">b</td>
<td align="center">n</td>
</tr>
<tr class="even">
<td align="left"><strong>Treatment (1)</strong></td>
<td align="center">c</td>
<td align="center">d</td>
<td align="center">n</td>
</tr>
</tbody>
</table>
<p>where <span class="math inline">\(a+b=n\)</span> and <span class="math inline">\(c+d=n\)</span>. To note, treatment and control groups do not have to have the same sample size n. We deliberately make them to be identical in order to emphasize that <em>row totals</em> can be set beforehand.</p>
<p><span class="math display">\[p_T = P(Y=1|X=1) = \frac{d}{c+d}\]</span> and <span class="math display">\[p_C = P(Y=1|X=0) = \frac{b}{a+b}\]</span></p>
<p>Similar to two sample t-test, we have</p>
<p><span class="math display">\[\hat{\Delta} \sim N \left( \pi_T-\pi_C, ~ \sqrt{\frac{\pi_T(1-\pi_T)}{n_T} + \frac{\pi_T(1-\pi_C)}{n_C}} \right)\]</span></p>
<ol style="list-style-type: decimal">
<li>propose a hypothesis</li>
</ol>
<p>The two samples are from the same distribution, i.e.,</p>
<p><span class="math inline">\(H_0: \Delta=\pi_T-\pi_C = 0\)</span></p>
<p><span class="math inline">\(H_a: \Delta=\pi_T-\pi_C \ne 0\)</span></p>
<ol start="2" style="list-style-type: decimal">
<li>compute the probability under the hypothesis</li>
</ol>
<p>Since the two samples are from the same distribution, we can compute a pooled variance</p>
<p><span class="math display">\[\pi_p = \frac{\sum Y_{T_i} + \sum Y_{C_i}}{n_T+n_C}\]</span> and hence</p>
<p><span class="math display">\[z = \frac{\hat{\Delta}}{\text{se}(\hat{\Delta})} 
= \frac{\hat{\Delta}}{\sqrt{\pi_p(1-\pi_p) (\frac{1}{n_T}+\frac{1}{n_C})}}\]</span></p>
<p>To note, since no unknown <span class="math inline">\(\sigma\)</span> is involved, we can simply use the z-statistic instead of the t-statistic.</p>
<ol start="3" style="list-style-type: decimal">
<li>make a decision</li>
</ol>
<p>Make a decison based on pre-determined confidence level, i.e. <span class="math inline">\(1 - \alpha\)</span>, where <span class="math inline">\(\alpha=0.05\)</span> by default.</p>
<p>One problem with <em>difference in proportions</em> is that it does not handle small proportions well. For example, is <span class="math inline">\(\hat{\Delta} = 0.05\)</span> a small or big difference? It would be considered small if it’s between 0.95 and 0.90; but would be a big difference if it’s between 0.10 and 0.15.</p>
</div>
<div id="relative-risks" class="section level3">
<h3><span class="header-section-number">4.2.3</span> Relative Risks</h3>
<p>An alternative statistic is called <strong>relative risks</strong>.</p>
<p><span class="math display">\[RR = \frac{p_T}{p_C}\]</span></p>
<p>Now we need to derive the properties of RR. In general, it is extremely difficult to deal with products or quotients of random variables. One way to tackle this is to use log-transformation.</p>
<ul>
<li><span class="math inline">\(\log(XY) = \log(X) + \log(Y)\)</span></li>
<li><span class="math inline">\(\log(\frac{X}{Y}) = \log(X) - \log(Y)\)</span></li>
<li><span class="math inline">\(\log(X^Y) = Y \log(X)\)</span></li>
</ul>
<p>Log-transformation can easily reduce products and quotients into sums. In our case, we have</p>
<p><span class="math display">\[\log RR = \log p_T - \log p_C\]</span> and</p>
<p><span class="math display">\[\text{se}(\log RR) = \sqrt{\frac{1-\pi_T}{n_T}+\frac{1-\pi_C}{n_C}}\]</span></p>
<p>Proof of the formula above is as follows.</p>
<blockquote>
<p><strong>Proof</strong>:</p>
<p><code>Prerequisites</code></p>
<p><span class="math display">\[Var(f(x)) = Var(X) \cdot (f&#39;(x))^2\]</span> where <span class="math inline">\(f&#39;(x)\)</span> is the first derivative of <span class="math inline">\(f(x)\)</span> with respect to x.</p>
<p>In our case, <span class="math display">\[Var(\log p) = Var(p) \left( \frac{1}{p} \right)^2 
= \frac{p(1-p)}{np^2} = \frac{1-p}{np}\]</span></p>
<p><code>Proof</code></p>
<p><span class="math display">\[Var(\log RR) = Var(\log p_T - \log p_C) = Var(\log p_T) + Var(\log p_C) = \frac{1-p_T}{n_Tp_T} + \frac{1-p_C}{n_Cp_C}\]</span></p>
</blockquote>
<p>Under the null hypothesis, i.e., <span class="math inline">\(\pi_T = \pi_C\)</span>, we have <span class="math inline">\(E(\log RR) = 0\)</span>, and hence</p>
<p><span class="math display">\[z = \frac{\log p_T - \log p_C}{\sqrt{\frac{1-p_T}{n_T}+\frac{1-p_C}{n_C}}}\]</span></p>
<p>For confidence interval, we have</p>
<p><span class="math display">\[[L, U] = \log \frac{p_T}{p_C} \pm z_{(1-\frac{\alpha}{2})} \cdot \sqrt{\frac{1-p_T}{n_T}+\frac{1-p_C}{n_C}}\]</span></p>
<p>To convert the confidence interval back to the original scale, we have <span class="math inline">\([e^L, e^U]\)</span>.</p>
<p>One problem with <em>relative risks</em> is that it is not appropriate for case-control design.</p>
<table>
<thead>
<tr class="header">
<th align="center"><span class="math inline">\(_X \ ^Y\)</span></th>
<th align="center">Cancer=No (0)</th>
<th align="center">Cancer=Yes (1)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><strong>Smoking=No (0)</strong></td>
<td align="center">a</td>
<td align="center">b</td>
</tr>
<tr class="even">
<td align="center"><strong>Smoking=Yes (1)</strong></td>
<td align="center">c</td>
<td align="center">d</td>
</tr>
<tr class="odd">
<td align="center"><strong>column total</strong></td>
<td align="center">n</td>
<td align="center">n</td>
</tr>
</tbody>
</table>
<p>where <span class="math inline">\(a+c=n\)</span> and <span class="math inline">\(b+d=n\)</span>. To note, cancer or non-cancer groups do not have to have the same sample size n. We deliberately make them to be identical in order to emphasize that <em>column totals</em> can be set beforehand.</p>
<p><span class="math display">\[p_T = P(X=1|Y=1) = \frac{d}{b+d}\]</span> and <span class="math display">\[p_C = P(X=1|Y=0) = \frac{c}{a+c}\]</span></p>
<p>In a case like this one, we cannot perform an experiment and forcibly assign people into treatment and control groups. Instead, we find the same number of people with or without cancer and investigate the proportion of smokers. A study like this is called a <strong>case-control study</strong>.</p>
</div>
<div id="odds-ratio" class="section level3">
<h3><span class="header-section-number">4.2.4</span> Odds Ratio</h3>
<p>An alternative statistic is called <strong>odds ratio</strong>.</p>
<p><span class="math display">\[\text{odds} \overset{1^*}{=} \frac{p}{1-p} \overset{2^*}{=} \frac{\text{total # of success}}{\text{total # of failure}} \]</span></p>
<ul>
<li>by definition <span class="math inline">\(1^*\)</span>: Given <span class="math inline">\(p = 1/4\)</span>, we have odds = (1/4) / (3/4) = 1/3</li>
<li>by definition <span class="math inline">\(2^*\)</span>: If odds of success is x, odds of failure is 1/x</li>
<li>by definition <span class="math inline">\(2^*\)</span>: Given odds = 1/3, we have p = 1/3 / (1/3 + 1) = 1/4</li>
</ul>
<p><span class="math display">\[p = \frac{\text{odds}}{\text{odds}+1}\]</span></p>
<p>Consider the <strong>contingency table</strong> below, where rows represent the independent variable X and columns represent the dependent variable Y.</p>
<table>
<thead>
<tr class="header">
<th align="left"><span class="math inline">\(_X \ ^Y\)</span></th>
<th align="left">0</th>
<th align="left">1</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">0</td>
<td align="left">a</td>
<td align="left">b</td>
</tr>
<tr class="even">
<td align="left">1</td>
<td align="left">c</td>
<td align="left">d</td>
</tr>
</tbody>
</table>
<p>Let <span class="math inline">\(p_C\)</span> represents the probability of Y=1 given X=0 and <span class="math inline">\(p_T\)</span> represents the probability of Y=1 given X=1. We would thus have</p>
<p><span class="math display">\[p_C=\frac{b}{a+b} ~~~~ \rightarrow ~~~~ \text{odds}_C=\frac{p_C}{1-p_C}
= \frac{a}{a+b} \cdot \frac{a+b}{b} = \frac{a}{b}\]</span> and</p>
<p><span class="math display">\[p_T=\frac{d}{c+d} ~~~~ \rightarrow ~~~~ \text{odds}_T=\frac{p_T}{1-p_T}
= \frac{d}{c+d} \cdot \frac{c+d}{c} = \frac{d}{c}\]</span></p>
<p>Therefore, we have</p>
<p><span class="math display">\[\hat{\theta} = \frac{\text{odds}_T}{\text{odds}_C} 
= \frac{p_T}{1-p_T} \cdot \frac{1-p_C}{p_C} 
= \frac{a}{b} \cdot \frac{d}{c} = \frac{ad}{bc}\]</span></p>
<p>To note, for the contingency table above, if we treat rows as Y and columns as X and recompute the odds ratio <span class="math inline">\(\hat{\theta}\)</span>. We would end up getting exactly the same result. Compared to relative risks, odds ratio is superior in the aspect such that it can handle the case-control study.</p>
<blockquote>
<p><strong>Practice</strong>: Compute the odds ratio with the assumption of a case-control study.</p>
</blockquote>
<p>In order to conduct hypothesis testing, we now need to consider the standard error of the odds ratio. Similar to the case of relative risks, to simplify the derivation, we will consider the standard error of the <strong><em>log of odds ratio</em></strong> instead of <strong><em>odds ratio</em></strong>, i.e., <span class="math inline">\(\log(\hat{\theta})\)</span>.</p>
<p>Under the null hypothesis that the two ratios are identical, we have <span class="math inline">\(\log(\hat{\theta}) = \log(1) = 0\)</span>. Given the CLT, we therefore have</p>
<p><span class="math display">\[z = \frac{\log(\hat{\theta})}{\text{se}(\log(\hat{\theta}))}\]</span> where <span class="math inline">\(\hat{\theta}\)</span> is the empirical odds ratio computed from the two samples and</p>
<p><span class="math display">\[\text{se}(\log(\hat{\theta})) = \sqrt{ \frac{1}{a} + \frac{1}{b} + \frac{1}{c} + \frac{1}{d} }\]</span></p>
<p>The proof of the formula above is as follows.</p>
<blockquote>
<p><strong>Proof</strong>:</p>
<p><code>Prerequisites</code></p>
<p><span class="math display">\[Var(f(x)) = Var(X) \cdot (f&#39;(x))^2\]</span> where <span class="math inline">\(f&#39;(x)\)</span> is the first derivative of <span class="math inline">\(f(x)\)</span> with respect to x.</p>
<p>In our case, for example,</p>
<p><span class="math display">\[Var(\log(\text{odds})) = Var \left( \log(\frac{p}{1-p}) \right) 
= Var(p) \cdot \left( \frac{1}{p(1-p)} \right)^2 
= \frac{\pi(1-\pi)}{n} \cdot \frac{1}{(\pi(1-\pi))^2}\]</span> <span class="math display">\[= \frac{1}{n\pi(1-\pi)}\]</span> where <span class="math display">\[\left( \log(\frac{p}{1-p}) \right)&#39; = ( \log(p) - \log(1-p) )&#39;
= \frac{1}{p} - \frac{-1}{1-p} = \frac{1}{p(1-p)}\]</span></p>
<p><code>Proof</code> <span class="math display">\[Var(\log(\hat{\theta})) = Var \left( \log(\frac{\text{odds}_T}{\text{odds}_C}) \right) 
= Var(\log(\text{odds}_T) - \log(\text{odds}_C))\]</span> <span class="math display">\[= \frac{1}{n_Tp_T(1-p_T)} + \frac{1}{np_C(1-p_C)}
= \frac{c+d}{cd} + \frac{a+b}{ab} 
= \frac{1}{a} + \frac{1}{b} + \frac{1}{c} + \frac{1}{d}\]</span> where <span class="math display">\[p_T = \frac{d}{c+d} ~~~~ \rightarrow ~~~~ 
n_Tp_T(1-p_T) = (c+d) \cdot \frac{d}{c+d} \cdot \frac{c}{c+d} = \frac{cd}{c+d}\]</span> and <span class="math display">\[p_C = \frac{a}{a+b} ~~~~ \rightarrow ~~~~ 
n_Cp_C(1-p_C) = (a+b) \cdot \frac{b}{a+b} \cdot \frac{a}{a+b} = \frac{ab}{a+b}\]</span></p>
</blockquote>
</div>
</div>
<div id="categorical-x---dummy-y" class="section level2">
<h2><span class="header-section-number">4.3</span> Categorical (X) - Dummy (Y)</h2>
<p>Now that we can deal with a 2-by-2 contingency table. We need to generalize it to handle I-by-J contingency tables, where I and J can be greater than 2.</p>
<div id="basics-1" class="section level3">
<h3><span class="header-section-number">4.3.1</span> Basics</h3>
<p>For the sake of clarity, in the presentation below, we will continue using a 2-by-2 contingency table. However, any conclusions we draw are readily generalizable to I-by-J tables.</p>
<table>
<thead>
<tr class="header">
<th align="center"><span class="math inline">\(_X \ ^Y\)</span></th>
<th align="center">0</th>
<th align="center">1</th>
<th align="center">row total</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><strong>0</strong></td>
<td align="center">a</td>
<td align="center">b</td>
<td align="center">a+b</td>
</tr>
<tr class="even">
<td align="center"><strong>1</strong></td>
<td align="center">c</td>
<td align="center">d</td>
<td align="center">c+d</td>
</tr>
<tr class="odd">
<td align="center"><strong>column total</strong></td>
<td align="center">a+c</td>
<td align="center">b+d</td>
<td align="center">n</td>
</tr>
</tbody>
</table>
<p>where <span class="math inline">\(n = a + b + c + d\)</span>.</p>
<div id="marginal-conditional-and-joint-probabilities" class="section level4">
<h4><span class="header-section-number">4.3.1.1</span> Marginal, Conditional, and Joint Probabilities</h4>
<ul>
<li><strong>marginal probability</strong>: <span class="math inline">\(P(Y)\)</span> or <span class="math inline">\(P(X)\)</span></li>
</ul>
<p><span class="math display">\[P(Y=0) = \frac{a+c}{n} ~~~~ and ~~~~ P(Y=1) = \frac{b+d}{n}\]</span> or <span class="math display">\[P(X=0) = \frac{a+b}{n} ~~~~ and ~~~~ P(X=1) = \frac{c+d}{n}\]</span></p>
<ul>
<li><strong>conditional probability</strong>: <span class="math inline">\(P(Y|X)\)</span> or <span class="math inline">\(P(X|Y)\)</span></li>
</ul>
<p><span class="math display">\[P(Y=0|X=0) = \frac{a}{a+b} ~~~~ and ~~~~ P(Y=1|X=0) = \frac{b}{a+b}\]</span> <span class="math display">\[P(Y=0|X=1) = \frac{c}{c+d} ~~~~ and ~~~~ P(Y=1|X=1) = \frac{d}{c+d}\]</span> or <span class="math display">\[P(X=0|Y=0) = \frac{a}{a+c} ~~~~ and ~~~~ P(X=1|Y=0) = \frac{c}{a+c}\]</span> <span class="math display">\[P(X=0|Y=1) = \frac{b}{b+d} ~~~~ and ~~~~ P(X=1|Y=1) = \frac{d}{b+d}\]</span></p>
<ul>
<li><strong>joint probability</strong>: <span class="math inline">\(P(X,Y)\)</span></li>
</ul>
<p><span class="math display">\[P(X=0,Y=0) = \frac{a}{n}\]</span> <span class="math display">\[P(X=0,Y=1) = \frac{b}{n}\]</span> <span class="math display">\[P(X=1,Y=0) = \frac{c}{n}\]</span> <span class="math display">\[P(X=1,Y=1) = \frac{d}{n}\]</span></p>
<blockquote>
<p><strong>Practice</strong>: Given the table below, compute <span class="math inline">\(P(X=0,Y=1)\)</span>, <span class="math inline">\(P(X=0)\)</span>, <span class="math inline">\(P(Y=1)\)</span>.</p>
</blockquote>
<table>
<thead>
<tr class="header">
<th align="center"><span class="math inline">\(_X \ ^Y\)</span></th>
<th align="center">0</th>
<th align="center">1</th>
<th align="center">row total</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><strong>0</strong></td>
<td align="center">2</td>
<td align="center">4</td>
<td align="center">6</td>
</tr>
<tr class="even">
<td align="center"><strong>1</strong></td>
<td align="center">10</td>
<td align="center">20</td>
<td align="center">30</td>
</tr>
<tr class="odd">
<td align="center"><strong>column total</strong></td>
<td align="center">12</td>
<td align="center">24</td>
<td align="center">36</td>
</tr>
</tbody>
</table>
</div>
<div id="independence-in-a-contingency-table" class="section level4">
<h4><span class="header-section-number">4.3.1.2</span> Independence in a Contingency Table</h4>
<p>Two variables are regarded as <strong>independent</strong> if any <strong>one</strong> of the three following conditions are met.</p>
<ul>
<li><span class="math inline">\(P(Y|X) = P(Y)\)</span></li>
<li><span class="math inline">\(P(X|Y) = P(X)\)</span></li>
<li><span class="math inline">\(P(X,Y) = P(X)P(Y)\)</span></li>
</ul>
<p>To further explain it, we will focus on the first condition. The first condition is more complicated than what meets the eye. From the simplest to the most detailed, the first condition can be expressed as follows.</p>
<ul>
<li>simplest expression:
<ul>
<li><span class="math inline">\(P(Y|X) = P(Y)\)</span></li>
</ul></li>
<li>more detailed expression:
<ul>
<li><span class="math inline">\(P(Y|X=0) = P(Y|X=1) = P(Y)\)</span></li>
</ul></li>
<li>most detailed expression:
<ul>
<li><span class="math inline">\(P(Y=0|X=0) = P(Y=0|X=1) = P(Y=0)\)</span></li>
<li><span class="math inline">\(P(Y=1|X=0) = P(Y=1|X=1) = P(Y=1)\)</span></li>
</ul></li>
</ul>
<p>These expressions are saying exactly the same thing; they differ only in the amount of details presented. To note, for the claim of independence to hold, <span class="math inline">\(P(Y|X=0) = P(Y|X=1) = P(Y)\)</span> must be true for <strong>each</strong> level of Y. In other words, all equations subsumed under the <strong><em>most detailed expression</em></strong> must hold so that the <strong><em>more detailed expression</em></strong> would hold.</p>
<p>Regardless which form of expression we use, <span class="math inline">\(P(Y|X) = P(Y)\)</span> clearly suggests</p>
<ul>
<li>the distribution of Y does not depend on X</li>
<li>the distribution of Y is the same at each level of X</li>
<li>whether we consider X or not (i.e., conditioned on X or not) does not affect the distribution of Y</li>
</ul>
</div>
<div id="null-hypothesis-and-independence" class="section level4">
<h4><span class="header-section-number">4.3.1.3</span> Null Hypothesis and Independence</h4>
<p>In the 2-by-2 contingency table, the following two claims are equivalent.</p>
<ul>
<li>the null hypothesis <span class="math inline">\(H_0: p_T = p_C\)</span></li>
<li>random variables X and Y are independent</li>
</ul>
<blockquote>
<p><strong>Proof</strong>:</p>
<p>By definition, we have <span class="math inline">\(p_C = P(Y=1|X=0)\)</span> and <span class="math inline">\(p_T = P(Y=1|X=1)\)</span>.</p>
<p>Under the null hypothesis <span class="math inline">\(p_C = p_T\)</span>, we have <span class="math inline">\(P(Y=1|X=0) = P(Y=1|X=1)\)</span>, which implies</p>
<ul>
<li><span class="math inline">\(P(Y=1|X=0) = P(Y=1|X=1) = P(Y=1)\)</span></li>
<li><span class="math inline">\(P(Y=0|X=0) = P(Y=0|X=1) = P(Y=0)\)</span></li>
</ul>
<p>or equivalently</p>
<ul>
<li><span class="math inline">\(P(Y|X=0) = P(Y|X=1) = P(Y)\)</span></li>
</ul>
<p>which clearly shows that X and Y are independent.</p>
</blockquote>
</div>
</div>
<div id="chi-squared-test" class="section level3">
<h3><span class="header-section-number">4.3.2</span> Chi-squared Test</h3>
<p>Given an <span class="math inline">\(I \times J\)</span> table, where rows represent X with I levels and columns represent Y with J levels. Let’s use <span class="math inline">\(n_{ij}\)</span> to denote the number of counts in the i-th row and j-th column and use <span class="math inline">\(n\)</span> to represent the total number of cases in the sample.</p>
<div id="poisson-distribution" class="section level4">
<h4><span class="header-section-number">4.3.2.1</span> Poisson Distribution</h4>
<p><span class="math display">\[X \sim Pois(\lambda)\]</span> and <span class="math display">\[f(x) = \frac{e^{-\lambda} \cdot \lambda^x}{x!}\]</span></p>
<p>A special property about Poisson distribution is that <span class="math inline">\(E(X) = Var(X) = \lambda\)</span>. Since X is a random variable, with regard to <span class="math inline">\(X \sim \forall(E(X), Var(X))\)</span>, we have</p>
<p><span class="math display">\[X \sim Pois(\lambda, \lambda)\]</span> In the introduction of CLT, we have learned that any random disturbances added together would eventually give rise to normality. Poisson distribution is essentially a distribution for counting, i.e., a sum of individual counts. As a result, with reasonably large sample size under the <strong>iid</strong> assumption, we have</p>
<p><span class="math display">\[X \sim N(\lambda, \lambda)\]</span> which is a normal approximation to the Poisson distribution.</p>
</div>
<div id="expected-cell-counts-under-independence" class="section level4">
<h4><span class="header-section-number">4.3.2.2</span> Expected Cell Counts Under Independence</h4>
<p>In a 2-by-2 table, we can use a, b, c, d to represent the four cell counts. This notation system, however, cannot be readily generalized to I-by-J tables. Starting from here, we will use the following notation.</p>
<ul>
<li><span class="math inline">\(n_{ij}\)</span> is the cell count for the i-th row and j-th column</li>
<li><span class="math inline">\(n_{i+}\)</span> is the total row count for the i-th row across columns</li>
<li><span class="math inline">\(n_{+j}\)</span> is the total column count for the j-th column across rows</li>
<li><span class="math inline">\(n\)</span> is the total sample size across all rows and columns</li>
</ul>
<p>Given an I-by-J table, under the independence assumption, the expected cell count <span class="math inline">\(\hat{\mu}_{ij}\)</span> for the i-th row and j-th column is</p>
<p><span class="math display">\[\hat{\mu}_{ij} = \frac{n_{i+}n_{+j}}{n}\]</span></p>
<blockquote>
<p><strong>Proof</strong>:</p>
<p>Under the independence assumption, we have <span class="math inline">\(p_{ij} = p_{i+}p_{+j}\)</span> and hence</p>
<p><span class="math display">\[\frac{\hat{\mu}_{ij}}{n} = \frac{n_{i+}}{n} \cdot \frac{n_{+j}}{n} 
~~~~ \rightarrow ~~~~
\hat{\mu}_{ij} = \frac{n_{i+}n_{+j}}{n}\]</span></p>
</blockquote>
</div>
<div id="chi-squared-distribution" class="section level4">
<h4><span class="header-section-number">4.3.2.3</span> Chi-squared Distribution</h4>
<p>Regardless of its fancy name, chi-squared distribution with 1 degree of freedom is simply a z-score squared.</p>
<p><span class="math display">\[z^2 \sim \chi_{df=1}^2\]</span> and <span class="math display">\[\sum_{i=1}^n z_i^2 \sim \chi_{df=n}^2\]</span></p>
<blockquote>
<p><strong>Example</strong>:</p>
<p>In our previous discussion, we have</p>
<p><span class="math display">\[\frac{\sum R_i^2}{\sigma^2} \sim \chi_{df}^2\]</span></p>
</blockquote>
<p>The proof of the formula above is as follows.</p>
<blockquote>
<p><strong>Proof</strong>:</p>
<p>Given <span class="math inline">\(Y_1, Y_2, Y_3, ..., Y_n \sim \forall(\mu, \sigma^2)\)</span>, we have</p>
<p><span class="math display">\[z_i^2 = \frac{(Y_i-\mu)^2}{\sigma^2} \sim \chi_{df=1}^2\]</span> and hence <span class="math display">\[\sum_{i=1}^n z_i^2 = \sum_{i=1}^n \frac{(Y_i-\mu)^2}{\sigma^2} = \frac{\sum (Y_i-\mu)^2}{\sigma^2} \sim \chi_{df=n}^2\]</span> When we use the estimator <span class="math inline">\(\bar{Y}\)</span> to estimate <span class="math inline">\(\mu\)</span>, we have</p>
<p><span class="math display">\[\frac{\sum (Y_i-\bar{Y})^2}{\sigma^2} \sim \chi_{df=n-1}^2\]</span></p>
<p>When we use a more complex model <span class="math inline">\(\hat{Y}\)</span> with <span class="math inline">\(p\)</span> parameters to estimate <span class="math inline">\(\mu\)</span>, we have</p>
<p><span class="math display">\[\frac{\sum (Y_i-\hat{Y})^2}{\sigma^2} = \frac{\sum R_i^2}{\sigma^2} \sim \chi_{df=n-p}^2\]</span></p>
</blockquote>
</div>
<div id="chi-squared-statistic" class="section level4">
<h4><span class="header-section-number">4.3.2.4</span> Chi-squared Statistic</h4>
<p>Under the hull hypothesis that X and Y are independent, we have <span class="math inline">\(E(n_{ij}) = \hat{n}_{ij}\)</span>. Since cell count follows Poisson distribution and can be approximated by a normal distribution, we have <span class="math inline">\(n_{ij} \sim N(\hat{n}_{ij}, \hat{n}_{ij})\)</span>. Therefore, we have</p>
<p><span class="math display">\[z_{ij}^2 = \frac{(n_{ij}-\hat{n}_{ij})^2}{\hat{n}_{ij}} \sim \chi_1^2\]</span> and hence</p>
<p><span class="math display">\[\sum_{I,J} z_{ij}^2 = \sum_{I,J} \frac{(n_{ij}-\hat{n}_{ij})^2}{\hat{n}_{ij}} \sim \chi_{df=(I-1)(J-1)}^2\]</span></p>
<p><code>p = 1 - pchisquare(X, df)</code></p>
<p>Proof of the degree of freedom is as follows.</p>
<blockquote>
<p><strong>Proof</strong>:</p>
<p>The df is the difference between the df under <span class="math inline">\(H_a\)</span> and the df under <span class="math inline">\(H_0\)</span>.</p>
<p>Under the <span class="math inline">\(H_0\)</span>, i.e., X and Y are independent, cell counts can be computed directly from row and column counts. There are I rows and hence (I-1) df for the row; J columns and hence (J-1) df for the column. Total df is <strong>(I-1) + (J-1)</strong>.</p>
<p>Under the <span class="math inline">\(H_a\)</span>, i.e., X and Y are dependent, each cell count need to be specified one by one except for one. Therefore, the total df is <strong>(IJ-1)</strong>.</p>
<p><span class="math display">\[df = (IJ-1) - ((I-1)+(J-1)) = (I-1)(J-1)\]</span></p>
</blockquote>
<p>Let’s use an example to illustrate the point.</p>
<blockquote>
<p><strong>Question</strong>: Given a 2-by-2 table, i.e., I=2 and J=2, knowning the sample size n, how many cells can be freely decided under the null hypothesis?</p>
</blockquote>
<blockquote>
<p><strong>Answer</strong>: 2 = (2-1) + (2-1)</p>
</blockquote>
<blockquote>
<p><strong>Question</strong>: Under the same conditions, how many cells can be freely decided under the alternative hypothesis?</p>
</blockquote>
<blockquote>
<p><strong>Answer</strong>: 3 = 2*2 - 1</p>
</blockquote>
<p>You should draw a 2-by-2 table and think in the order of the four cell counts a, b, c, d to convince yourself.</p>
</div>
<div id="diagnostics" class="section level4">
<h4><span class="header-section-number">4.3.2.5</span> Diagnostics</h4>
<p>It should be clear that the construction of the chi-squared statistic depends on the CLT and the normality assumption in particular.</p>
<ul>
<li><span class="math inline">\(\chi^2\)</span>-test is only valid if each expected cell count is greater than 5, i.e., <span class="math inline">\(\hat{n}_{ij} &gt; 5\)</span></li>
<li>if I and J are large, a few cell counts less than 5, i.e., <span class="math inline">\(\hat{n}_{ij} &lt; 5\)</span>, does not matter</li>
</ul>
<p>From the count table, compute (a) expected cell count table and (b) z-score table.</p>
<ul>
<li><strong>rescaled cell residuals</strong></li>
</ul>
<p><span class="math display">\[z_{ij} = \frac{n_{ij} - \hat{n}_{ij}}{\sqrt{\hat{n}_{ij}}}\]</span> where <span class="math inline">\(\text{se}(n_{ij}) = \sqrt{\hat{n}_{ij}}\)</span>.</p>
<ul>
<li><strong>standardized cell residuals</strong></li>
</ul>
<p><span class="math display">\[z_{ij} = \frac{n_{ij} - \hat{n}_{ij}} {\sqrt{\hat{n}_{ij}(1-p_{i+})(1-p_{+j})}}\]</span> where the denominator is the estimated standard error of <span class="math inline">\(n_{ij} - \hat{n}_{ij}\)</span> under the null hypothesis.</p>
<p>Therefore, a standardized residual with absolute value</p>
<ol style="list-style-type: lower-alpha">
<li>exceeding 2 when there are only a few cells, or</li>
<li>exceeding 3 when there are many cells</li>
</ol>
<p>indicates lack of fit of the null hypothesis in that cell.</p>
</div>
</div>
<div id="g-square-test" class="section level3">
<h3><span class="header-section-number">4.3.3</span> G-square Test</h3>
<div id="maximum-likelihood-method" class="section level4">
<h4><span class="header-section-number">4.3.3.1</span> Maximum Likelihood Method</h4>
<p>Given <span class="math inline">\(Y \sim Bern(\pi)\)</span>, we draw a random sample <span class="math inline">\(Y_1, Y_2, ..., Y_n\)</span> and compute the sample mean <span class="math inline">\(\bar{Y}\)</span>.</p>
<p><span class="math display">\[P(Y_i) = \pi^{Y_i}(1-\pi)^{1-Y_i}\]</span></p>
<p><span class="math display">\[P(Y_1,Y_2,...,Y_n) = P(Y_1,Y_2,...,Y_n|\pi) = \prod_{i=1}^n Y_i
= \prod_{i=1}^n \pi^{Y_i}(1-\pi)^{1-Y_i}\]</span></p>
<p>At this point, <span class="math inline">\(P(Y_1,Y_2,...,Y_n|\pi)\)</span> is a <strong>probability function</strong>, the integration of which sums to 1.</p>
<p><span class="math display">\[L = f(\pi|Y_1,Y_2,...,Y_n) = \prod_{i=1}^n \pi^{Y_i}(1-\pi)^{1-Y_i}\]</span></p>
<p>To note, <span class="math inline">\(f(\pi|Y_1,Y_2,...,Y_n)\)</span> is not a probability function, since its integration does not sum up to 1. As a result, we must give it a different name, which is called the <strong>likelihood function</strong>.</p>
<p>Proof that <span class="math inline">\(P(Y_1,Y_2,...,Y_n|\pi)\)</span> is a proper probability function is as follows.</p>
<blockquote>
<p><strong>Proof</strong>:</p>
<p>Since each <span class="math inline">\(P(Y_i)\)</span> for i = 1, 2, …, n is a probability function, which means</p>
<p><span class="math display">\[\int P(Y_i)dY_i = 1\]</span></p>
<p>Hence, we have</p>
<p><span class="math display">\[\int\int...\int_{(n)} P(Y_1,Y_2,...,Y_n)~dY_1dY_2 \cdots dY_n\]</span> <span class="math display">\[= \int\int...\int_{(n)} P(Y_1)P(Y_2),...,P(Y_n)~dY_1dY_2 \cdots dY_n\]</span> <span class="math display">\[= \int\int...\int_{(n-1)} \left( \int P(Y_1)dY_1 \right) P(Y_2),...,P(Y_n)~dY_2dY_3 \cdots dY_n\]</span> <span class="math display">\[= \int\int...\int_{(n-1)} 1 \cdot P(Y_2),...,P(Y_n)~dY_2dY_3 \cdots dY_n\]</span> <span class="math display">\[= \int\int...\int_{(n-2)} 1 \cdot 1 \cdot P(Y_3),...,P(Y_n)~dY_3 \cdots dY_n\]</span> <span class="math display">\[= \int\int...\int_{(n-3)} 1 \cdot 1 \cdot 1 \cdot P(Y_4),...,P(Y_n)~dY_4 \cdots dY_n\]</span> <span class="math display">\[= \cdots = 1\]</span></p>
</blockquote>
<p>The idea of maximum likelihood is simple: Given the observed data <span class="math inline">\(Y_1,Y_2,...,Y_n\)</span>, choose the value of <span class="math inline">\(\pi\)</span> that would maximize the likelihood function <span class="math inline">\(f(\pi|Y_1,Y_2,...,Y_n)\)</span>.</p>
<p>Let’s use the following example to illustrate the point.</p>
<blockquote>
<p><strong>Example</strong>:</p>
<p>Let’s use 4 data points <span class="math inline">\(Y_1=1, Y_2=0, Y_3=0, Y_4=1\)</span>.</p>
<p><span class="math display">\[L = f(\pi|Y_1,Y_2,Y_3,Y_4) = \prod_{i=4}^n \pi^{Y_i}(1-\pi)^{1-Y_i} 
= \pi(1-\pi)(1-\pi)\pi = \pi^2(1-\pi)^2\]</span></p>
<ul>
<li>if <span class="math inline">\(\pi=0.0\)</span>, <span class="math inline">\(L = 0\)</span></li>
<li>if <span class="math inline">\(\pi=0.2\)</span>, <span class="math inline">\(L = 16/625\)</span></li>
<li>if <span class="math inline">\(\pi=0.4\)</span>, <span class="math inline">\(L = 36/625\)</span></li>
<li>if <span class="math inline">\(\pi=0.6\)</span>, <span class="math inline">\(L = 36/625\)</span></li>
<li>if <span class="math inline">\(\pi=0.8\)</span>, <span class="math inline">\(L = 16/625\)</span></li>
<li>if <span class="math inline">\(\pi=1.0\)</span>, <span class="math inline">\(L = 0\)</span></li>
</ul>
<p><img src="_main_files/figure-html/unnamed-chunk-60-1.png" width="672" /></p>
<p>We will therefore choose the <span class="math inline">\(\hat{\pi}\)</span> that produces the highest probability. In this case, <span class="math inline">\(\hat{\pi}=0.5\)</span>.</p>
</blockquote>
<p>Now we need to find a way to solve for the result analytically.</p>
<p><span class="math display">\[L = \prod_{i=1}^n \pi^{Y_i}(1-\pi)^{1-Y_i}\]</span> and <span class="math display">\[l = \log(L) = \sum_{i=1}^n (Y_i \log(\pi) + (1-Y_i) \log(1-\pi))\]</span></p>
<p>Get the first derivative of <span class="math inline">\(l\)</span> with respect to <span class="math inline">\(\pi\)</span> and set it to equal 0.</p>
<p><span class="math display">\[\frac{dl}{d\pi} = \sum_{i=1}^n \left( Y_i \cdot \frac{1}{\pi} + (1-Y_i) \cdot \frac{-1}{1-\pi} \right)
= \frac{\sum Y_i}{\pi} - \frac{\sum (1-Y_i)}{1-\pi} = 0\]</span></p>
<p>Solving for the equation, we have <span class="math display">\[\hat{\pi} = \arg\max(l) = \frac{\sum Y_i}{n}\]</span></p>
<blockquote>
<p><strong>Proof</strong>:</p>
<p><span class="math display">\[(1-\pi) \sum Y_i = \pi \sum (1-Y_i) = \pi (n - \sum Y_i)\]</span> <span class="math display">\[\sum Y_i - \pi \sum Y_i = n\pi - \pi \sum Y_i\]</span> <span class="math display">\[\pi = \frac{\sum Y_i}{n}\]</span></p>
</blockquote>
</div>
<div id="likelihood-ratio-statistic" class="section level4">
<h4><span class="header-section-number">4.3.3.2</span> Likelihood-Ratio Statistic</h4>
<p>For the same reason, we will deal with log-likelihood (instead of likelihood) in order to simplify derivation.</p>
<p><span class="math display">\[-2 \log \left( \frac{L_0}{L_a} \right) = -2(\log L_0 - \log L_a)\]</span> where <span class="math inline">\(L_0\)</span> is the likelihood under the <span class="math inline">\(H_0\)</span> and <span class="math inline">\(L_a\)</span> is the likelihood under the <span class="math inline">\(H_a\)</span>.</p>
<p>The maximum likelihood with less constrained model (aka <strong>full model</strong>) will always fit at least as good as the more constrained model under the null hypothesis (aka <strong>reduced model</strong>). Therefore, the quotient <span class="math inline">\(L_0/L_a\)</span> is always between 0 and 1 and its <span class="math inline">\(-2 \log(L_0/L_a)\)</span> is always positive.</p>
<p>For two-way contingency tables, <span class="math inline">\(-2 \log(L_0/L_a)\)</span> can be simplified to the <span class="math inline">\(G^2\)</span> statistic.</p>
<p><span class="math display">\[G^2 = -2 \sum n_{ij} \cdot \log \left( \frac{\hat{\mu}_{ij}}{n_{ij}} \right) ~~~~ \&amp; ~~~~ df = (I-1)(J-1)\]</span></p>
<blockquote>
<p><strong>Proof</strong>:</p>
<p><span class="math display">\[L = P(Y_1,Y_2, ...,Y_n) = \prod_{I,J} \frac{e^{-\lambda} \cdot (\lambda_{ij})^{n_{ij}}}{n_{ij}!}\]</span></p>
<p>and hence</p>
<p><span class="math display">\[l = \log L = -\sum \lambda_{ij} + \sum (n_{ij} \cdot \log \lambda_{ij}) - \sum \log n_{ij}!\]</span></p>
<p>Under <span class="math inline">\(H_0\)</span>, we have</p>
<p><span class="math display">\[l_{H_0} = -\sum \hat{\mu}_{ij} + \sum (n_{ij} \cdot \log \hat{\mu}_{ij}) - \sum \log n_{ij}!\]</span></p>
<p>Under <span class="math inline">\(H_a\)</span>, we have</p>
<p><span class="math display">\[l_{H_a} = -\sum n_{ij} + \sum (n_{ij} \cdot \log n_{ij}) - \sum \log n_{ij}!\]</span> To note, <span class="math inline">\(\sum \hat{\mu}_{ij} = \sum n_{ij} = n\)</span>.</p>
<p>Therefore,</p>
<p><span class="math display">\[\log \frac{L_0}{L_a} = \sum \left( n_{ij} \cdot \log \left( \frac{\hat{\mu}_{ij}}{n_{ij}} \right) \right)\]</span></p>
<p>and</p>
<p><span class="math display">\[G^2 = -2 \sum n_{ij} \cdot \log \left( \frac{\hat{\mu}_{ij}}{n_{ij}} \right)\]</span></p>
</blockquote>
</div>
</div>
</div>
<div id="categorical-x---continuous-y" class="section level2">
<h2><span class="header-section-number">4.4</span> Categorical (X) - Continuous (Y)</h2>
<div id="anova" class="section level3">
<h3><span class="header-section-number">4.4.1</span> ANOVA</h3>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mid1cat =<span class="st"> </span><span class="kw">cut</span>(xdat$mid1, <span class="dt">breaks=</span><span class="kw">quantile</span>(xdat$mid1), <span class="dt">include.lowest=</span>T)
xdat =<span class="st"> </span><span class="kw">data.frame</span>(xdat, mid1cat)
<span class="kw">boxplot</span>(mid3 ~<span class="st"> </span>mid1cat, <span class="dt">data=</span>xdat, 
        <span class="dt">main=</span><span class="st">&quot;Boxplot of Exam Results&quot;</span>, 
        <span class="dt">xlab=</span><span class="st">&quot;First Midterm Score Category&quot;</span>,
        <span class="dt">ylab=</span><span class="st">&quot;Final Exam Score&quot;</span>)
<span class="kw">abline</span>(<span class="dt">h=</span><span class="kw">mean</span>(xdat$mid3,<span class="dt">na.rm=</span>T), <span class="dt">lty=</span><span class="dv">2</span>)</code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-62-1.png" width="672" /></p>
<table style="width:31%;">
<colgroup>
<col width="5%" />
<col width="8%" />
<col width="8%" />
<col width="8%" />
</colgroup>
<thead>
<tr class="header">
<th></th>
<th align="center">SS</th>
<th align="center">DF</th>
<th align="center">MS</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math display">\[\mathbf{Total}~~\]</span></td>
<td align="center"><span class="math display">\[~~\sum(Y_i-\bar{Y})^2~~\]</span></td>
<td align="center"><span class="math display">\[~~n-1~~\]</span></td>
<td align="center"><span class="math display">\[~~\frac{1}{n-1} \sum (Y_i-\bar{Y})^2\]</span></td>
</tr>
<tr class="even">
<td><span class="math display">\[\mathbf{Model}~~\]</span></td>
<td align="center"><span class="math display">\[~~\sum(\bar{Y}_g-\bar{Y})^2~~\]</span></td>
<td align="center"><span class="math display">\[~~g-1~~\]</span></td>
<td align="center"><span class="math display">\[~~\frac{1}{g-1}\sum (\bar{Y}_g-\bar{Y})^2\]</span></td>
</tr>
<tr class="odd">
<td><span class="math display">\[\mathbf{Residual}~~\]</span></td>
<td align="center"><span class="math display">\[~~\sum(Y_i - \bar{Y}_g)^2~~\]</span></td>
<td align="center"><span class="math display">\[~~n-g~~\]</span></td>
<td align="center"><span class="math display">\[\frac{1}{n-g}\sum (Y_i-\bar{Y}_g)^2\]</span></td>
</tr>
</tbody>
</table>
<p>where <span class="math inline">\(n\)</span> is the sample size and <span class="math inline">\(g\)</span> is the number of groups.</p>
<p><span class="math display">\[ F = \frac{MSM}{MSE} = \frac{ \frac{1}{p}\sum (\hat{Y}_i-\bar{Y})^2 }{ \frac{1}{n-1-p}\sum (Y_i-\hat{Y}_i)^2 } \]</span> where <span class="math inline">\(F(p, n-1-p)\)</span> and <span class="math inline">\(p=1\)</span> in the case of simple linear regression.</p>
<p><code>p = 1 - pf(F, df1, df2)</code></p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="statistical-inference.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="linear-regression.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
