
# Common Statistical Tests

| $_X \ ^Y$ | dummy | continuous |
|---|---|---|
| dummy | (2) **difference in proportions** or **odds ratio** | (1) **two-sample t-test** |
| categorical | (3) **chi-square test** | (4) **ANOVA** |
| continuous | (6) **logistic regression** | (5) **linear regression** |


## Dummy (X) - Continuous (Y)

**RCT** or **Randomized Controlled Trial** is the prime example of an Experiment Design, where the independent variable X is a dummy variable. 

### Two-sample T-Test

Given two samples

$$Y_{T_i} \sim \forall \left( \mu_T, \sigma_T^2 \right) 
~~~~ and ~~~~
Y_{C_j} \sim \forall \left( \mu_C, \sigma_C^2 \right) $$
where $i = 1,2,3,...,n_T$ and $j = 1,2,3,...,n_C$

and

$$\bar{Y}_T = \frac{1}{n_T} \sum Y_{T_i} 
~~~~ and ~~~~
\bar{Y}_C = \frac{1}{n_C} \sum Y_{C_i}$$

The **estimand** and **estimator** that are of interest to us are as follows.

- estimand: $\Delta = \mu_T - \mu_C$ 
- estimator: $\hat{\Delta} = \bar{Y}_T - \bar{Y}_C$. 

The estimand is a constant (because it's a population parameter) and the estimator is a random variable. Hence, we are immediately interested in the properties of the estimator. 

> Question: What are the properties of $\hat{\Delta}$?

**Answers**:

- $E(\hat{\Delta}) = \mu_T - \mu_C$
- $Var(\hat{\Delta}) = \frac{\sigma_T^2}{n_T} + \frac{\sigma_C^2}{n_C}$
- $shape(\hat{\Delta}) = N$

or equivalently

$$\hat{\Delta} \sim N \left( \mu_T - \mu_C, ~ \frac{\sigma_T^2}{n_T} + \frac{\sigma_C^2}{n_C} \right)$$

`Proof`:

Considering the CLT, 
$$\bar{Y}_{T} \sim N \left( \mu_T, ~ \frac{\sigma_T^2}{n_T} \right) 
~~~~ and ~~~~
\bar{Y}_{C} \sim N \left( \mu_C, ~ \frac{\sigma_C^2}{n_C} \right) $$

we have the following

$$E(\hat{\Delta}) = E(\bar{Y}_T - \bar{Y}_C) 
= E(\bar{Y}_T) - E(\bar{Y}_C) = \mu_T - \mu_C = \Delta$$
and
$$Var(\hat{\Delta}) = Var(\bar{Y}_T - \bar{Y}_C) 
= Var(\bar{Y}_T) + Var(\bar{Y}_C) 
= \frac{\sigma_T^2}{n_T} + \frac{\sigma_C^2}{n_C}$$

Note: 

$$Var(X+Y) = Var(X) + 2Cov(X,Y) + Var(Y)$$

$$Var(X-Y) = Var(X) - 2Cov(X,Y) + Var(Y)$$
When X and Y are independent, i.e., $X \perp Y$, we have $Cov(X,Y)=0$ and hence

$$Var(X \pm Y) = Var(X) + Var(Y)$$


Now, given what we know about the properties of $\hat{\Delta}$ and the CLT, we have

$$z = \frac{ \hat{\Delta} - \Delta }{ \text{se}(\Delta) } 
= \frac{ (\bar{Y}_T - \bar{Y}_C) - (\mu_T - \mu_C) }{ \sqrt{ \frac{\sigma_T^2}{n_T} + \frac{\sigma_C^2}{n_C} } }$$

At this point, $\sigma_T$ and $\sigma_C$ are unknown to us. As a result, we need to use sample variances to replace population variances and the z-statistic would thus become a t-statistic. 

$$t = \frac{ (\bar{Y}_T - \bar{Y}_C) - (\mu_T - \mu_C) }{ \sqrt{ \frac{s_T^2}{n_T} + \frac{s_C^2}{n_C} } }$$
with $\text{df} = \text{df}_T + \text{df}_C = (n_T-1) + (n_C-1)$

To conduct hypothesis test, we have

1. propose the hypothesis

$H_0: \Delta = \mu_T - \mu_C = 0$, where $H_0$ is called the **null hypothesis** 

$H_a: \Delta = \mu_T - \mu_C \ne 0$, where $H_a$ is called the **alternative hypothesis** 

2. compute a probability under the hypothesis

$$t = \frac{ \bar{Y}_T - \bar{Y}_C }{ \sqrt{ \frac{s_p^2}{n_T} + \frac{s_p^2}{n_C} } }$$

Under the null hypothesis that the two samples are from the same population, it is natural for us to combine the two sample varinances. 

$$s_p^2 = \frac{\text{df}_C}{\text{df}_C+\text{df}_T} s_C^2 + 
\frac{\text{df}_T}{\text{df}_C+\text{df}_T} s_T^2 
= \frac{(n_C-1)}{(n_C-1) + (n_T-1)}s_C^2 + 
\frac{(n_T-1)}{(n_C-1) + (n_T-1)}s_T^2$$

We can then compute the p-value using the following R code.

```
p_value = 2 * ( 1 - pt(t, df=(nt-1)+(nc-1)) )
```

3. make a decision

Make a decison based on pre-determined confidence level, i.e. $1 - \alpha$, where $\alpha=0.05$ by default. 

### Statistical Power

`TODO`: add a graph in the beginning of page 3 in notes_20180504.pdf

- type I error rate $\alpha$
- specificity $1-\alpha$
- type II error rate $\beta$
- sensitivity or power $1-\beta$

Note: CV stands for critical value.

$$z_{(1-\frac{\alpha}{2})} = \frac{\text{CV}-0}{\text{se}} 
~~~~ \rightarrow ~~~~
\text{CV} = z_{(1-\frac{\alpha}{2})} \cdot \text{se}$$
where $\alpha=0.05$ by default and hence $z_{(1-\alpha/2)} = 1.96$. 

$$z_\beta = \frac{\text{CV} - \Delta}{\text{se}}
~~~~ \rightarrow ~~~~
\text{CV} = \Delta + z_\beta \cdot \text{se}$$
where $\beta=0.80$ by default and hence $z_\beta = -0.84$. 

Through equating $\text{CV}$, we have

$$z_{(1-\frac{\alpha}{2})} \cdot \text{se} = \Delta + z_\beta \cdot \text{se}$$
and hence

$$(z_{(1-\frac{\alpha}{2})} - z_\beta) \cdot \text{se} = \Delta 
~~~~ \rightarrow ~~~~~
\text{se} = \frac{\Delta}{z_{(1-\frac{\alpha}{2})} - z_\beta}$$

Moreover, let's assume that treatment and control groups have the same sample size $n$ and hence the pooled variance is 

$$\text{se}^2 = \frac{s_T^2}{n_T} + \frac{s_C^2}{n_C} = \frac{2s_p^2}{n}$$


$$\text{se}^2 = \left( \frac{\Delta}{z_{(1-\frac{\alpha}{2})} - z_\beta} \right)^2 = \frac{2s_p^2}{n}$$
and hence
$$n = \frac{2 s_p^2 ( z_{(1-\frac{\alpha}{2})} - z_\beta )^2}{\Delta^2}
= \frac{2 \cdot 7.84 \cdot s_p^2}{\Delta^2} \approx \frac{16s_p^2}{\Delta^2} 
= \left( \frac{4s_p}{\Delta} \right)^2$$
where $( z_{(1-\frac{\alpha}{2})} - z_\beta )^2 = (1.96 + 0.84)^2 = 7.84$.

Given the formula above, we can answer the following questions. 

> Question: What factors can result in smaller sample sizes?

**Answers**:

- larger $\alpha$
- larger $\beta$
- smaller $s_p^2$
- larger $\Delta$

By rearranging the formula above, we have

$$z_\beta = z_{(1-\frac{\alpha}{2})} - \sqrt{ \frac{n\Delta^2}{2s_p^2} }$$

> Question: What factors would result in larger power?

**Answers**:

- larger $n$
- smaller $s_p^2$
- larger $\alpha$
- larger $\Delta$

Tip: To answer this question, think about the graph not the formula. 


## Dummy (X) - Dummy (Y)

### Basics

#### Bernoulli Distribution

A random variable Y following a Bernoullie distribution can be expressed as follows.

$$Y \sim Bern(\pi)$$
where Y = 1 with probability $\pi$ and Y = 0 with probability $1-\pi$.

Based on the definitions of E(Y) and Var(Y), we have

$$E(Y) = \sum Y \cdot P(Y) = 1 \cdot \pi + 0 \cdot (1-\pi) = \pi$$
and

$$Var(Y) = E(Y - EY)^2 = \sum (Y - EY)^2 \cdot P(Y) 
= (1-\pi)^2 \cdot \pi + (0-\pi)^2 \cdot (1-\pi)$$
$$= (\pi^2 - 2\pi + 1)\pi + \pi^2 (1-\pi) = \pi - \pi^2 = \pi(1-\pi)$$

Hence the properties of the random variable Y can be expressed as 
$$Y \sim Bern(\pi, \pi(1-\pi))$$

Given the CLT, we therefore have 
$$\bar{Y} \sim N \left( \pi, \frac{\pi(1-\pi)}{n} \right)$$

#### Inference with One Variance

`TODO`: Contents to be added!

### Difference in Proportions

Similar to two sample t-test, we have

$$\hat{\Delta} \sim N \left( \pi_T-\pi_C, ~ \sqrt{\frac{\pi_T(1-\pi_T)}{n_T} + \frac{\pi_T(1-\pi_C)}{n_C}} \right)$$

1. propose a hypothesis

The two samples are from the same distribution, i.e., 

$H_0: \Delta=\pi_T-\pi_C = 0$

$H_a: \Delta=\pi_T-\pi_C \ne 0$ 

2. compute the probability under the hypothesis

Since the two samples are from the same distribution, we can compute a pooled variance

$$\pi_p = \frac{\sum Y_{T_i} + \sum Y_{C_i}}{n_T+n_C}$$
and hence

$$z = \frac{\hat{\Delta}}{\text{se}(\hat{\Delta})} 
= \frac{\hat{\Delta}}{\sqrt{\pi_p(1-\pi_p) (\frac{1}{n_T}+\frac{1}{n_C})}}$$

To note, since no unknown $\sigma$ is involved, we can simply use the z-statistic instead of the t-statistic. 

3. make a decision


### Relative Risks

`TODO`: Contents to be added from page 8 in notes_20180504.pdf

### Odds Ratio

The definition of odds is as follows. 

$$\text{odds} \overset{1^*}{=} \frac{p}{1-p} \overset{2^*}{=} \frac{\text{total # of success}}{\text{total # of failure}} $$

- $1^*$: Given $p = 1/4$, we have odds = (1/4) / (3/4) = 1/3
- $2^*$: If odds of success is x, odds of failure is 1/x
- $2^*$: Given odds = 1/3, we have p = 1/3 / (1/3 + 1) = 1/4

$$p = \frac{\text{odds}}{\text{odds}+1}$$

Consider the **contingency table** below, where rows represent the independent variable X and columns represent the dependent variable Y.

|$_X \ ^Y$| 0 | 1 |
|:---|:---|:---|
| 0 | a | b |
| 1 | c | d |

Let $p_C$ represents the probability of Y=1 given X=0 and $p_T$ represents the probability of Y=1 given X=1. We would thus have

$$p_C=\frac{b}{a+b} ~~~~ \rightarrow ~~~~ \text{odds}_C=\frac{p_C}{1-p_C}
= \frac{a}{a+b} \cdot \frac{a+b}{b} = \frac{a}{b}$$
and

$$p_T=\frac{d}{c+d} ~~~~ \rightarrow ~~~~ \text{odds}_T=\frac{p_T}{1-p_T}
= \frac{d}{c+d} \cdot \frac{c+d}{c} = \frac{d}{c}$$

Therefore, we have

$$\hat{\theta} = \frac{\text{odds}_T}{\text{odds}_C} 
= \frac{p_T}{1-p_T} \cdot \frac{1-p_C}{p_C} 
= \frac{a}{b} \cdot \frac{d}{c} = \frac{ad}{bc}$$

To note, for the contingency table above, if we treat rows as Y and columns as X and recompute the odds ratio $\hat{\theta}$. We would end up getting exactly the same result. Compared to relative risks, odds ratio is superior in the aspect such that it can handle the case-control design. 

In order to conduct hypothesis testing, we now need to consider the standard error of the odds ratio. Similar to the case of relative risks, to simplify the derivation, we will consider the standard error of the **_log of odds ratio_** instead of **_odds ratio_**, i.e., $\log(\hat{\theta})$. 

Under the null hypothesis that the two ratios are identical, we have $\log(\hat{\theta}) = \log(1) = 0$. Given the CLT, we therefore have

$$z = \frac{\log(\hat{\theta})}{\text{se}(\log(\hat{\theta}))}$$
where $\hat{\theta}$ is the empirical odds ratio computed from the two samples and 

$$\text{se}(\log(\hat{\theta})) = \sqrt{ \frac{1}{a} + \frac{1}{b} + \frac{1}{c} + \frac{1}{d} }$$

`Proof - Prerequisites`: 

$$Var(f(x)) = Var(X) \cdot (f'(x))^2$$
where $f'(x)$ is the first derivative of $f(x)$ with respect to x.

In our case, for example, 

$$Var(\log(\text{odds})) = Var \left( \log(\frac{p}{1-p}) \right) 
= Var(p) \cdot \left( \frac{1}{p(1-p)} \right)^2 
= \frac{\pi(1-\pi)}{n} \cdot \frac{1}{(\pi(1-\pi))^2}$$
$$= \frac{1}{n\pi(1-\pi)}$$
where
$$\left( \log(\frac{p}{1-p}) \right)' = ( \log(p) - \log(1-p) )'
= \frac{1}{p} - \frac{-1}{1-p} = \frac{1}{p(1-p)}$$

`Proof`:
$$Var(\log(\hat{\theta})) = Var \left( \log(\frac{\text{odds}_T}{\text{odds}_C}) \right) 
= Var(\log(\text{odds}_T) - \log(\text{odds}_C))$$
$$= \frac{1}{n_Tp_T(1-p_T)} + \frac{1}{np_C(1-p_C)}
= \frac{c+d}{cd} + \frac{a+b}{ab} 
= \frac{1}{a} + \frac{1}{b} + \frac{1}{c} + \frac{1}{d}$$
where
$$p_T = \frac{d}{c+d} ~~~~ \rightarrow ~~~~ 
n_Tp_T(1-p_T) = (c+d) \cdot \frac{d}{c+d} \cdot \frac{c}{c+d} = \frac{cd}{c+d}$$
and
$$p_C = \frac{a}{a+b} ~~~~ \rightarrow ~~~~ 
n_Cp_C(1-p_C) = (a+b) \cdot \frac{b}{a+b} \cdot \frac{a}{a+b} = \frac{ab}{a+b}$$


## Categorical (X) - Dummy (Y)

Now that we can deal with a 2-by-2 contingency table. We need to generalize it to handle a n-by-n contngency table, where n is greater than 2. 

|$_X \ ^Y$| 0 | 1 |
|:---|:---|:---|
| 1 | a | b |
| 2 | c | d |
| 3 | e | f |
| 4 | g | h |

### Basics

#### Marginal, Conditional, and Joint Probabilities

page 5 in notes_20180508.pdf

#### Independence in a Contingency Table



### Chi-square Test

### G-square Test

#### Maximum Likelihood Method



## Categorical (X) - Continuous (Y)

### ANOVA




